[WARN] 2018-10-30 03:18:41,179 org.apache.spark.HeartbeatReceiver logWarning - Removing executor driver with no recent heartbeats: 12621020 ms exceeds timeout 120000 ms
[INFO] 2018-10-30 03:18:41,391 org.apache.spark.ContextCleaner logInfo - Cleaned accumulator 0
[ERROR] 2018-10-30 03:18:41,392 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost an executor driver (already removed): Executor heartbeat timed out after 12621020 ms
[INFO] 2018-10-30 03:18:41,395 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-30 03:18:41,396 org.apache.spark.SparkContext logWarning - Killing executors is not supported by current scheduler.
[INFO] 2018-10-30 03:18:41,396 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-30 03:18:41,397 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None) re-registering with master
[INFO] 2018-10-30 03:18:41,399 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,400 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,401 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-30 03:18:41,407 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-30 03:18:41,407 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None) re-registering with master
[INFO] 2018-10-30 03:18:41,407 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,408 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 03:18:41,408 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,408 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-30 03:18:41,409 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-30 03:18:41,409 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None) re-registering with master
[INFO] 2018-10-30 03:18:41,410 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,410 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,410 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-30 03:18:41,411 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-30 03:18:41,411 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None) re-registering with master
[INFO] 2018-10-30 03:18:41,411 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,411 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,412 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-30 03:18:41,412 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-30 03:18:41,413 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None) re-registering with master
[INFO] 2018-10-30 03:18:41,413 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,413 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,413 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-30 03:18:41,414 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-30 03:18:41,414 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None) re-registering with master
[INFO] 2018-10-30 03:18:41,415 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,415 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17317, None)
[INFO] 2018-10-30 03:18:41,415 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-30 03:18:41,424 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 03:18:41,435 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 03:18:41,436 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 03:18:41,437 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 03:18:41,442 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 03:18:41,447 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 03:18:41,448 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 03:18:41,448 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0f772997-a239-4068-bcee-c4d05ff076dc
[INFO] 2018-10-30 03:18:41,449 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0f772997-a239-4068-bcee-c4d05ff076dc/pyspark-06ae3e2d-efc9-491a-a0a9-e489fd34e2ae
[INFO] 2018-10-30 03:18:41,449 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-45b761c4-f95f-4d5f-b8a4-5746442d15e7
[WARN] 2018-10-30 13:44:05,950 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 13:44:06,975 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 13:44:07,006 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING_REPORT_TRUNCATE
[INFO] 2018-10-30 13:44:07,240 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 13:44:07,241 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 13:44:07,241 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 13:44:07,242 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 13:44:07,242 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 13:44:07,493 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 30731.
[INFO] 2018-10-30 13:44:07,523 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 13:44:07,547 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 13:44:07,551 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 13:44:07,552 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 13:44:07,565 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-8d2afa43-bddb-4552-a8f4-b974b328035c
[INFO] 2018-10-30 13:44:07,589 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 13:44:07,605 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 13:44:07,852 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 13:44:07,902 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 13:44:07,998 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:30731/files/etl_config.json with timestamp 1540932247997
[INFO] 2018-10-30 13:44:08,000 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-ac026256-4c81-4c95-a60c-83fcb57f98e7/userFiles-89855e95-f880-4a49-b72b-b7ebbef89874/etl_config.json
[INFO] 2018-10-30 13:44:08,014 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT_TRUNCATE.py at spark://vmwebietl02-dev:30731/files/JB_PRODUCT_BOOKING_REPORT_TRUNCATE.py with timestamp 1540932248014
[INFO] 2018-10-30 13:44:08,015 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT_TRUNCATE.py to /tmp/spark-ac026256-4c81-4c95-a60c-83fcb57f98e7/userFiles-89855e95-f880-4a49-b72b-b7ebbef89874/JB_PRODUCT_BOOKING_REPORT_TRUNCATE.py
[INFO] 2018-10-30 13:44:08,021 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:30731/files/packages.zip with timestamp 1540932248021
[INFO] 2018-10-30 13:44:08,021 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-ac026256-4c81-4c95-a60c-83fcb57f98e7/userFiles-89855e95-f880-4a49-b72b-b7ebbef89874/packages.zip
[INFO] 2018-10-30 13:44:08,127 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-30 13:44:08,202 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 42 ms (0 ms spent in bootstraps)
[INFO] 2018-10-30 13:44:08,331 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181030134407-0008
[INFO] 2018-10-30 13:44:08,335 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030134407-0008/0 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:44:08,336 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030134407-0008/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:44:08,344 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29279.
[INFO] 2018-10-30 13:44:08,345 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:29279
[INFO] 2018-10-30 13:44:08,347 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 13:44:08,364 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030134407-0008/0 is now RUNNING
[INFO] 2018-10-30 13:44:08,384 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 29279, None)
[INFO] 2018-10-30 13:44:08,393 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:29279 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 29279, None)
[INFO] 2018-10-30 13:44:08,398 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 29279, None)
[INFO] 2018-10-30 13:44:08,399 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 29279, None)
[INFO] 2018-10-30 13:44:08,587 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-30 13:44:08,864 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 13:44:08,864 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 13:44:09,300 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 13:44:09,691 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 13:44:09,703 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 13:44:09,710 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-30 13:44:09,716 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-30 13:44:09,777 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 13:44:09,794 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 13:44:09,794 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 13:44:09,806 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 13:44:09,811 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 13:44:09,846 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 13:44:09,847 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 13:44:09,849 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ac026256-4c81-4c95-a60c-83fcb57f98e7
[INFO] 2018-10-30 13:44:09,850 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ac026256-4c81-4c95-a60c-83fcb57f98e7/pyspark-0f3d2bbf-c8ff-4ae4-af32-3f894412fc63
[INFO] 2018-10-30 13:44:09,850 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1d75fe50-86dd-4a42-882e-2fa8f43f9ea5
[WARN] 2018-10-30 13:45:55,871 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 13:45:56,736 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 13:45:56,763 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING_REPORT_TRUNCATE
[INFO] 2018-10-30 13:45:56,950 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 13:45:56,951 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 13:45:56,951 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 13:45:56,951 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 13:45:56,965 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 13:45:57,170 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 35497.
[INFO] 2018-10-30 13:45:57,198 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 13:45:57,224 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 13:45:57,227 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 13:45:57,228 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 13:45:57,239 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-13f89187-f28b-47a4-b72d-19182a84122a
[INFO] 2018-10-30 13:45:57,259 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 13:45:57,277 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 13:45:57,490 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 13:45:57,549 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 13:45:57,650 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:35497/files/etl_config.json with timestamp 1540932357649
[INFO] 2018-10-30 13:45:57,653 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-ad991d63-d3f6-4417-a221-da177f271a25/userFiles-bc72b52c-21fd-401e-afa3-a06014d304e6/etl_config.json
[INFO] 2018-10-30 13:45:57,669 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT_TRUNCATE.py at spark://vmwebietl02-dev:35497/files/JB_PRODUCT_BOOKING_REPORT_TRUNCATE.py with timestamp 1540932357669
[INFO] 2018-10-30 13:45:57,670 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT_TRUNCATE.py to /tmp/spark-ad991d63-d3f6-4417-a221-da177f271a25/userFiles-bc72b52c-21fd-401e-afa3-a06014d304e6/JB_PRODUCT_BOOKING_REPORT_TRUNCATE.py
[INFO] 2018-10-30 13:45:57,675 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:35497/files/packages.zip with timestamp 1540932357675
[INFO] 2018-10-30 13:45:57,675 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-ad991d63-d3f6-4417-a221-da177f271a25/userFiles-bc72b52c-21fd-401e-afa3-a06014d304e6/packages.zip
[INFO] 2018-10-30 13:45:57,765 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-30 13:45:57,831 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 40 ms (0 ms spent in bootstraps)
[INFO] 2018-10-30 13:45:57,930 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181030134557-0009
[INFO] 2018-10-30 13:45:57,933 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030134557-0009/0 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:45:57,935 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030134557-0009/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:45:57,943 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9505.
[INFO] 2018-10-30 13:45:57,944 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:9505
[INFO] 2018-10-30 13:45:57,947 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 13:45:57,948 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030134557-0009/0 is now RUNNING
[INFO] 2018-10-30 13:45:57,981 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 9505, None)
[INFO] 2018-10-30 13:45:57,987 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:9505 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 9505, None)
[INFO] 2018-10-30 13:45:57,992 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 9505, None)
[INFO] 2018-10-30 13:45:57,992 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 9505, None)
[INFO] 2018-10-30 13:45:58,174 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-30 13:45:58,388 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 13:45:58,389 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 13:45:58,832 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 13:45:59,933 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:39737) with ID 0
[ERROR] 2018-10-30 13:45:59,994 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8131276636986639328
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 13:46:00,035 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 13:46:00,041 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 0)
[INFO] 2018-10-30 13:46:00,062 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 13:46:00,064 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-30 13:46:00,065 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 0)
[INFO] 2018-10-30 13:46:00,382 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030134557-0009/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 13:46:00,385 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030134557-0009/0 removed: Command exited with code 1
[INFO] 2018-10-30 13:46:00,385 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030134557-0009/1 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:46:00,385 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 13:46:00,387 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-30 13:46:00,387 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030134557-0009/1 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:46:00,388 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030134557-0009/1 is now RUNNING
[INFO] 2018-10-30 13:46:00,388 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-30 13:46:02,336 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 13:46:02,352 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 13:46:02,369 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-30 13:46:02,370 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-30 13:46:02,386 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 13:46:02,403 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 13:46:02,403 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 13:46:02,405 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 13:46:02,411 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 13:46:02,426 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 13:46:02,427 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 13:46:02,428 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-078e8765-4bcc-479e-8ac9-3c737cfd19dd
[INFO] 2018-10-30 13:46:02,429 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ad991d63-d3f6-4417-a221-da177f271a25
[INFO] 2018-10-30 13:46:02,429 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ad991d63-d3f6-4417-a221-da177f271a25/pyspark-eabbc163-3a52-4791-9fa0-696d141b1a91
[WARN] 2018-10-30 13:59:35,457 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 13:59:36,416 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 13:59:36,443 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING
[INFO] 2018-10-30 13:59:36,586 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 13:59:36,587 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 13:59:36,587 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 13:59:36,587 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 13:59:36,588 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 13:59:36,835 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11176.
[INFO] 2018-10-30 13:59:36,865 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 13:59:36,888 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 13:59:36,892 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 13:59:36,892 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 13:59:36,904 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1fc08e80-c5d9-4c15-a37e-60d96ba58ae0
[INFO] 2018-10-30 13:59:36,925 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 13:59:36,942 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 13:59:37,136 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 13:59:37,193 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 13:59:37,290 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:11176/files/etl_config.json with timestamp 1540933177290
[INFO] 2018-10-30 13:59:37,292 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-d8ee49f7-666b-44de-99fa-33485d2db226/userFiles-6f42d3b6-5423-4697-99bf-f5a2f7c1edf6/etl_config.json
[INFO] 2018-10-30 13:59:37,308 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py at spark://vmwebietl02-dev:11176/files/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540933177308
[INFO] 2018-10-30 13:59:37,309 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py to /tmp/spark-d8ee49f7-666b-44de-99fa-33485d2db226/userFiles-6f42d3b6-5423-4697-99bf-f5a2f7c1edf6/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 13:59:37,315 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:11176/files/packages.zip with timestamp 1540933177314
[INFO] 2018-10-30 13:59:37,315 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-d8ee49f7-666b-44de-99fa-33485d2db226/userFiles-6f42d3b6-5423-4697-99bf-f5a2f7c1edf6/packages.zip
[INFO] 2018-10-30 13:59:37,405 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-30 13:59:37,484 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 47 ms (0 ms spent in bootstraps)
[INFO] 2018-10-30 13:59:37,625 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181030135937-0010
[INFO] 2018-10-30 13:59:37,628 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030135937-0010/0 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:59:37,631 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030135937-0010/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:59:37,638 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23402.
[INFO] 2018-10-30 13:59:37,639 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:23402
[INFO] 2018-10-30 13:59:37,641 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 13:59:37,647 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/0 is now RUNNING
[INFO] 2018-10-30 13:59:37,677 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 23402, None)
[INFO] 2018-10-30 13:59:37,682 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:23402 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 23402, None)
[INFO] 2018-10-30 13:59:37,687 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 23402, None)
[INFO] 2018-10-30 13:59:37,689 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 23402, None)
[INFO] 2018-10-30 13:59:37,869 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-30 13:59:38,066 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 13:59:38,067 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 13:59:38,505 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 13:59:39,721 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:20747) with ID 0
[ERROR] 2018-10-30 13:59:39,781 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5467511088903825889
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 13:59:39,813 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 13:59:39,819 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 0)
[INFO] 2018-10-30 13:59:39,831 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:39,833 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-30 13:59:39,835 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 0)
[INFO] 2018-10-30 13:59:40,158 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 13:59:40,161 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030135937-0010/0 removed: Command exited with code 1
[INFO] 2018-10-30 13:59:40,162 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:40,163 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030135937-0010/1 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:59:40,163 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-30 13:59:40,164 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-30 13:59:40,166 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030135937-0010/1 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:59:40,167 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/1 is now RUNNING
[INFO] 2018-10-30 13:59:42,034 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 239.321448 ms
[INFO] 2018-10-30 13:59:42,202 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 13:59:42,236 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 13:59:42,237 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 13:59:42,239 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 13:59:42,243 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 13:59:42,254 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 13:59:42,255 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:20753) with ID 1
[ERROR] 2018-10-30 13:59:42,305 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4785381728592320191
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 13:59:42,321 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 13:59:42,342 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 13:59:42,376 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 13:59:42,379 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:23402 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 13:59:42,382 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 13:59:42,398 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 13:59:42,399 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 13:59:42,421 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 1)
[INFO] 2018-10-30 13:59:42,421 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:42,422 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-30 13:59:42,422 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 1)
[INFO] 2018-10-30 13:59:42,677 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 13:59:42,678 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030135937-0010/1 removed: Command exited with code 1
[INFO] 2018-10-30 13:59:42,678 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:42,678 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-30 13:59:42,679 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-30 13:59:42,679 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030135937-0010/2 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:59:42,680 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030135937-0010/2 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:59:42,681 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/2 is now RUNNING
[INFO] 2018-10-30 13:59:44,644 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:20759) with ID 2
[INFO] 2018-10-30 13:59:44,665 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 13:59:44,691 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8303879148439662913
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 13:59:44,703 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 13:59:44,717 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 13:59:44,726 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 2)
[INFO] 2018-10-30 13:59:44,727 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:44,728 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-30 13:59:44,729 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 2)
[INFO] 2018-10-30 13:59:45,050 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 13:59:45,051 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030135937-0010/2 removed: Command exited with code 1
[INFO] 2018-10-30 13:59:45,051 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030135937-0010/3 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:59:45,051 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:45,051 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-30 13:59:45,052 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030135937-0010/3 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:59:45,052 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-30 13:59:45,053 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/3 is now RUNNING
[INFO] 2018-10-30 13:59:47,017 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:20765) with ID 3
[INFO] 2018-10-30 13:59:47,020 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 13:59:47,061 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8837502847135721294
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 13:59:47,076 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 13:59:47,077 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 13:59:47,078 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 3)
[INFO] 2018-10-30 13:59:47,078 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:47,079 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-30 13:59:47,080 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 3)
[INFO] 2018-10-30 13:59:47,422 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/3 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 13:59:47,423 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030135937-0010/3 removed: Command exited with code 1
[INFO] 2018-10-30 13:59:47,423 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030135937-0010/4 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:59:47,423 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-30 13:59:47,423 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:47,424 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-30 13:59:47,424 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030135937-0010/4 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:59:47,427 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/4 is now RUNNING
[INFO] 2018-10-30 13:59:49,281 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:20771) with ID 4
[INFO] 2018-10-30 13:59:49,283 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 13:59:49,320 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7963551604905895327
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 13:59:49,333 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 13:59:49,333 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 13:59:49,335 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 4)
[INFO] 2018-10-30 13:59:49,335 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:49,336 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-30 13:59:49,336 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 4)
[INFO] 2018-10-30 13:59:49,697 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 13:59:49,698 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030135937-0010/4 removed: Command exited with code 1
[INFO] 2018-10-30 13:59:49,698 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030135937-0010/5 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 13:59:49,698 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-30 13:59:49,698 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:49,699 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030135937-0010/5 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 13:59:49,699 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-30 13:59:49,701 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030135937-0010/5 is now RUNNING
[INFO] 2018-10-30 13:59:51,578 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:20777) with ID 5
[INFO] 2018-10-30 13:59:51,581 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 13:59:51,625 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8451607028252720688
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 13:59:51,642 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 13:59:51,642 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-30 13:59:51,646 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-30 13:59:51,650 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 13:59:51,655 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-30 13:59:51,658 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 9.383 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-30 13:59:51,663 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 5)
[INFO] 2018-10-30 13:59:51,663 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-30 13:59:51,664 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-30 13:59:51,665 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 5)
[INFO] 2018-10-30 13:59:51,665 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 9.461391 s
[INFO] 2018-10-30 13:59:51,740 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 13:59:51,752 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 13:59:51,759 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-30 13:59:51,760 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-30 13:59:51,777 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 13:59:51,803 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 13:59:51,803 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 13:59:51,805 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 13:59:51,811 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 13:59:51,821 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 13:59:51,822 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 13:59:51,823 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c6e840c0-66d6-440b-bc32-2167858e50a4
[INFO] 2018-10-30 13:59:51,824 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d8ee49f7-666b-44de-99fa-33485d2db226/pyspark-83df0964-4266-4ff9-9418-ef85c9a35821
[INFO] 2018-10-30 13:59:51,824 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d8ee49f7-666b-44de-99fa-33485d2db226
[WARN] 2018-10-30 14:50:01,852 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 14:50:02,298 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 14:50:02,300 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-dcab683b-5575-407b-a7ca-8fadc19cd9dd
[WARN] 2018-10-30 14:50:15,137 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 14:50:15,942 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 14:50:15,970 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING
[INFO] 2018-10-30 14:50:16,218 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 14:50:16,218 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 14:50:16,219 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 14:50:16,219 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 14:50:16,220 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 14:50:16,455 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 18309.
[INFO] 2018-10-30 14:50:16,482 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 14:50:16,501 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 14:50:16,504 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 14:50:16,505 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 14:50:16,514 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-e8cd6dfa-ff1e-4746-9680-0f6db1bd1750
[INFO] 2018-10-30 14:50:16,532 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 14:50:16,547 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 14:50:16,768 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 14:50:16,846 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 14:50:16,953 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:18309/files/etl_config.json with timestamp 1540936216953
[INFO] 2018-10-30 14:50:16,955 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-a9664372-a027-41bb-8096-46d5f094a7ea/userFiles-b0f78d3e-5c4c-46e0-8d8a-73cca277981a/etl_config.json
[INFO] 2018-10-30 14:50:16,969 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py at spark://vmwebietl02-dev:18309/files/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540936216969
[INFO] 2018-10-30 14:50:16,970 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py to /tmp/spark-a9664372-a027-41bb-8096-46d5f094a7ea/userFiles-b0f78d3e-5c4c-46e0-8d8a-73cca277981a/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 14:50:16,974 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:18309/files/packages.zip with timestamp 1540936216974
[INFO] 2018-10-30 14:50:16,975 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-a9664372-a027-41bb-8096-46d5f094a7ea/userFiles-b0f78d3e-5c4c-46e0-8d8a-73cca277981a/packages.zip
[INFO] 2018-10-30 14:50:17,068 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-30 14:50:17,133 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 39 ms (0 ms spent in bootstraps)
[INFO] 2018-10-30 14:50:17,239 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181030145016-0011
[INFO] 2018-10-30 14:50:17,241 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145016-0011/0 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:17,242 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145016-0011/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:17,250 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9581.
[INFO] 2018-10-30 14:50:17,251 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:9581
[INFO] 2018-10-30 14:50:17,253 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 14:50:17,260 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/0 is now RUNNING
[INFO] 2018-10-30 14:50:17,288 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 9581, None)
[INFO] 2018-10-30 14:50:17,293 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:9581 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 9581, None)
[INFO] 2018-10-30 14:50:17,299 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 9581, None)
[INFO] 2018-10-30 14:50:17,300 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 9581, None)
[INFO] 2018-10-30 14:50:17,476 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-30 14:50:17,693 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 14:50:17,695 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 14:50:18,206 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 14:50:19,288 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15970) with ID 0
[ERROR] 2018-10-30 14:50:19,345 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6159225142133802171
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:50:19,381 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:50:19,391 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 0)
[INFO] 2018-10-30 14:50:19,399 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:19,401 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-30 14:50:19,402 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 0)
[INFO] 2018-10-30 14:50:19,728 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:50:19,730 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145016-0011/0 removed: Command exited with code 1
[INFO] 2018-10-30 14:50:19,731 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145016-0011/1 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:19,731 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:19,731 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145016-0011/1 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:19,731 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-30 14:50:19,732 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/1 is now RUNNING
[INFO] 2018-10-30 14:50:19,732 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-30 14:50:21,453 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 222.031269 ms
[INFO] 2018-10-30 14:50:21,565 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15976) with ID 1
[INFO] 2018-10-30 14:50:21,590 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 14:50:21,613 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 14:50:21,614 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 14:50:21,614 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 14:50:21,616 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[ERROR] 2018-10-30 14:50:21,618 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7957456896923298586
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-30 14:50:21,623 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[ERROR] 2018-10-30 14:50:21,633 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:50:21,699 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 14:50:21,729 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 14:50:21,732 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:9581 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 14:50:21,735 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 14:50:21,749 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 14:50:21,750 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 14:50:21,767 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 1)
[INFO] 2018-10-30 14:50:21,767 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:21,768 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-30 14:50:21,768 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 1)
[INFO] 2018-10-30 14:50:21,982 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:50:21,983 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145016-0011/1 removed: Command exited with code 1
[INFO] 2018-10-30 14:50:21,984 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145016-0011/2 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:21,984 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:21,984 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-30 14:50:21,985 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145016-0011/2 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:21,985 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-30 14:50:21,988 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/2 is now RUNNING
[INFO] 2018-10-30 14:50:23,954 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15982) with ID 2
[INFO] 2018-10-30 14:50:23,971 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 14:50:24,006 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5013657700875792592
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:50:24,024 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 14:50:24,038 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:50:24,047 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 2)
[INFO] 2018-10-30 14:50:24,048 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:24,048 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-30 14:50:24,048 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 2)
[INFO] 2018-10-30 14:50:24,377 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:50:24,378 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145016-0011/2 removed: Command exited with code 1
[INFO] 2018-10-30 14:50:24,378 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145016-0011/3 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:24,378 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-30 14:50:24,378 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:24,379 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145016-0011/3 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:24,379 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-30 14:50:24,380 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/3 is now RUNNING
[INFO] 2018-10-30 14:50:26,303 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15988) with ID 3
[INFO] 2018-10-30 14:50:26,306 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 14:50:26,352 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6755951353018219534
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:50:26,367 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 14:50:26,367 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:50:26,368 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 3)
[INFO] 2018-10-30 14:50:26,369 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:26,370 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-30 14:50:26,370 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 3)
[INFO] 2018-10-30 14:50:26,716 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/3 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:50:26,716 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145016-0011/3 removed: Command exited with code 1
[INFO] 2018-10-30 14:50:26,717 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145016-0011/4 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:26,717 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-30 14:50:26,717 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:26,717 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-30 14:50:26,718 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145016-0011/4 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:26,719 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/4 is now RUNNING
[INFO] 2018-10-30 14:50:28,538 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15994) with ID 4
[INFO] 2018-10-30 14:50:28,541 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 14:50:28,582 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5865379839212593656
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:50:28,595 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 14:50:28,595 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:50:28,596 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 4)
[INFO] 2018-10-30 14:50:28,597 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:28,597 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-30 14:50:28,597 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 4)
[INFO] 2018-10-30 14:50:28,947 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:50:28,948 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145016-0011/4 removed: Command exited with code 1
[INFO] 2018-10-30 14:50:28,948 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145016-0011/5 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:28,949 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:28,949 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-30 14:50:28,949 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145016-0011/5 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:28,949 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-30 14:50:28,950 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145016-0011/5 is now RUNNING
[INFO] 2018-10-30 14:50:30,795 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:16000) with ID 5
[INFO] 2018-10-30 14:50:30,799 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 14:50:30,845 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7249571048190757516
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:50:30,859 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 14:50:30,859 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-30 14:50:30,862 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-30 14:50:30,866 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 14:50:30,871 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-30 14:50:30,874 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 9.231 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-30 14:50:30,881 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 5)
[INFO] 2018-10-30 14:50:30,882 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:30,883 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-30 14:50:30,883 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 5)
[INFO] 2018-10-30 14:50:30,884 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 9.291646 s
[INFO] 2018-10-30 14:50:30,951 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 14:50:30,971 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 14:50:30,986 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-30 14:50:30,987 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-30 14:50:31,005 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 14:50:31,032 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 14:50:31,032 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 14:50:31,034 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 14:50:31,037 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 14:50:31,054 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 14:50:31,055 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 14:50:31,057 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a9664372-a027-41bb-8096-46d5f094a7ea
[INFO] 2018-10-30 14:50:31,058 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f28bbdc6-e08c-4e59-8f88-1cd854f24967
[INFO] 2018-10-30 14:50:31,058 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a9664372-a027-41bb-8096-46d5f094a7ea/pyspark-3a87dd97-7af5-46d4-b02e-c516668dd01c
[WARN] 2018-10-30 14:50:51,235 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 14:50:52,056 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 14:50:52,083 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING
[INFO] 2018-10-30 14:50:52,245 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 14:50:52,246 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 14:50:52,246 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 14:50:52,246 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 14:50:52,259 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 14:50:52,462 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11370.
[INFO] 2018-10-30 14:50:52,489 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 14:50:52,509 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 14:50:52,512 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 14:50:52,512 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 14:50:52,522 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-d1c236fd-cf47-400e-bcc6-0dbf85831d65
[INFO] 2018-10-30 14:50:52,541 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 14:50:52,555 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 14:50:52,747 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 14:50:52,800 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 14:50:52,911 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:11370/files/etl_config.json with timestamp 1540936252910
[INFO] 2018-10-30 14:50:52,913 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-cbd331c0-4ed0-4152-9759-29e486b770d0/userFiles-b6891d78-eff7-4c47-82f9-b9f6d6f877fa/etl_config.json
[INFO] 2018-10-30 14:50:52,942 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py at spark://vmwebietl02-dev:11370/files/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540936252942
[INFO] 2018-10-30 14:50:52,943 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py to /tmp/spark-cbd331c0-4ed0-4152-9759-29e486b770d0/userFiles-b6891d78-eff7-4c47-82f9-b9f6d6f877fa/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 14:50:52,949 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:11370/files/packages.zip with timestamp 1540936252949
[INFO] 2018-10-30 14:50:52,949 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-cbd331c0-4ed0-4152-9759-29e486b770d0/userFiles-b6891d78-eff7-4c47-82f9-b9f6d6f877fa/packages.zip
[INFO] 2018-10-30 14:50:53,039 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-30 14:50:53,111 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 40 ms (0 ms spent in bootstraps)
[INFO] 2018-10-30 14:50:53,220 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181030145052-0012
[INFO] 2018-10-30 14:50:53,224 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145052-0012/0 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:53,226 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145052-0012/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:53,232 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29365.
[INFO] 2018-10-30 14:50:53,233 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:29365
[INFO] 2018-10-30 14:50:53,234 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 14:50:53,235 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/0 is now RUNNING
[INFO] 2018-10-30 14:50:53,272 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 29365, None)
[INFO] 2018-10-30 14:50:53,277 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:29365 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 29365, None)
[INFO] 2018-10-30 14:50:53,283 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 29365, None)
[INFO] 2018-10-30 14:50:53,284 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 29365, None)
[INFO] 2018-10-30 14:50:53,437 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-30 14:50:53,628 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 14:50:53,628 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 14:50:54,069 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 14:50:55,195 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:63357) with ID 0
[ERROR] 2018-10-30 14:50:55,251 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9085835856188152372
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:50:55,289 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:50:55,296 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 0)
[INFO] 2018-10-30 14:50:55,309 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:55,310 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-30 14:50:55,312 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 0)
[INFO] 2018-10-30 14:50:55,634 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:50:55,638 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145052-0012/0 removed: Command exited with code 1
[INFO] 2018-10-30 14:50:55,638 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145052-0012/1 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:55,638 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:55,639 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-30 14:50:55,639 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145052-0012/1 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:55,640 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/1 is now RUNNING
[INFO] 2018-10-30 14:50:55,640 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-30 14:50:57,382 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 208.541968 ms
[INFO] 2018-10-30 14:50:57,539 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 14:50:57,558 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 14:50:57,559 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 14:50:57,560 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 14:50:57,562 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 14:50:57,569 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 14:50:57,574 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:63363) with ID 1
[ERROR] 2018-10-30 14:50:57,640 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8795724435224552111
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-30 14:50:57,647 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[ERROR] 2018-10-30 14:50:57,656 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:50:57,694 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 14:50:57,698 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:29365 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 14:50:57,701 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 14:50:57,717 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 14:50:57,718 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 14:50:57,737 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 1)
[INFO] 2018-10-30 14:50:57,738 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:57,738 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-30 14:50:57,738 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 1)
[INFO] 2018-10-30 14:50:58,006 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:50:58,007 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145052-0012/1 removed: Command exited with code 1
[INFO] 2018-10-30 14:50:58,008 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145052-0012/2 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:50:58,008 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-30 14:50:58,008 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:58,008 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145052-0012/2 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:50:58,008 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-30 14:50:58,011 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/2 is now RUNNING
[INFO] 2018-10-30 14:50:59,872 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:63369) with ID 2
[INFO] 2018-10-30 14:50:59,889 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 14:50:59,923 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6841957615624042696
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:50:59,945 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 14:50:59,960 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:50:59,969 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 2)
[INFO] 2018-10-30 14:50:59,969 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-30 14:50:59,970 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-30 14:50:59,970 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 2)
[INFO] 2018-10-30 14:51:00,293 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:51:00,294 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145052-0012/2 removed: Command exited with code 1
[INFO] 2018-10-30 14:51:00,296 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145052-0012/3 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:51:00,296 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-30 14:51:00,296 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-30 14:51:00,297 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145052-0012/3 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:51:00,297 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-30 14:51:00,297 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/3 is now RUNNING
[INFO] 2018-10-30 14:51:02,162 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:63375) with ID 3
[INFO] 2018-10-30 14:51:02,165 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 14:51:02,207 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7727193065843683323
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:51:02,220 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 14:51:02,221 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:51:02,221 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 3)
[INFO] 2018-10-30 14:51:02,222 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-30 14:51:02,223 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-30 14:51:02,224 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 3)
[INFO] 2018-10-30 14:51:02,572 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/3 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:51:02,572 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145052-0012/3 removed: Command exited with code 1
[INFO] 2018-10-30 14:51:02,573 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145052-0012/4 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:51:02,573 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-30 14:51:02,573 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-30 14:51:02,574 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145052-0012/4 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:51:02,574 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-30 14:51:02,576 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/4 is now RUNNING
[INFO] 2018-10-30 14:51:04,436 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:63381) with ID 4
[INFO] 2018-10-30 14:51:04,439 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 14:51:04,480 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7182326739353321593
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:51:04,494 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 14:51:04,495 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 14:51:04,496 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 4)
[INFO] 2018-10-30 14:51:04,496 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-30 14:51:04,498 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-30 14:51:04,498 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 4)
[INFO] 2018-10-30 14:51:04,844 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 14:51:04,845 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030145052-0012/4 removed: Command exited with code 1
[INFO] 2018-10-30 14:51:04,845 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030145052-0012/5 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 14:51:04,845 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-30 14:51:04,845 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-30 14:51:04,846 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030145052-0012/5 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 14:51:04,846 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-30 14:51:04,847 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030145052-0012/5 is now RUNNING
[INFO] 2018-10-30 14:51:06,745 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:63387) with ID 5
[INFO] 2018-10-30 14:51:06,749 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 14:51:06,795 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6227271642077464398
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 14:51:06,808 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 14:51:06,810 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-30 14:51:06,814 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-30 14:51:06,819 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 14:51:06,825 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-30 14:51:06,829 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 9.239 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-30 14:51:06,837 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 5)
[INFO] 2018-10-30 14:51:06,837 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-30 14:51:06,837 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 9.297355 s
[INFO] 2018-10-30 14:51:06,839 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-30 14:51:06,839 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 5)
[INFO] 2018-10-30 14:51:06,923 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 14:51:06,943 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 14:51:06,956 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-30 14:51:06,957 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-30 14:51:06,980 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 14:51:07,006 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 14:51:07,008 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 14:51:07,011 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 14:51:07,018 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 14:51:07,034 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 14:51:07,036 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 14:51:07,038 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f38aca19-dc4e-40e1-a27f-b978f8dec203
[INFO] 2018-10-30 14:51:07,039 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-cbd331c0-4ed0-4152-9759-29e486b770d0
[INFO] 2018-10-30 14:51:07,039 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-cbd331c0-4ed0-4152-9759-29e486b770d0/pyspark-ee9a9dac-6f7d-4337-a103-a4427fec38d9
[WARN] 2018-10-30 21:36:41,491 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 21:36:42,427 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 21:36:42,454 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-30 21:36:42,622 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 21:36:42,622 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 21:36:42,623 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 21:36:42,623 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 21:36:42,635 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 21:36:42,866 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 22059.
[INFO] 2018-10-30 21:36:42,897 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 21:36:42,920 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 21:36:42,924 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 21:36:42,924 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 21:36:42,936 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-d34d32b2-30ee-47cc-ab73-42721177628a
[INFO] 2018-10-30 21:36:42,957 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 21:36:42,974 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 21:36:43,169 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 21:36:43,226 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 21:36:43,322 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540960603322
[INFO] 2018-10-30 21:36:43,324 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-99b27f18-d8d0-4ac9-9bb6-bb00cfcf7745/userFiles-8fdf544c-7ab2-4c6e-a757-2aa07e430a9d/etl_config.json
[INFO] 2018-10-30 21:36:43,337 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540960603337
[INFO] 2018-10-30 21:36:43,338 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-99b27f18-d8d0-4ac9-9bb6-bb00cfcf7745/userFiles-8fdf544c-7ab2-4c6e-a757-2aa07e430a9d/JB_STG_BOOKINGS.py
[INFO] 2018-10-30 21:36:43,341 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540960603341
[INFO] 2018-10-30 21:36:43,341 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-99b27f18-d8d0-4ac9-9bb6-bb00cfcf7745/userFiles-8fdf544c-7ab2-4c6e-a757-2aa07e430a9d/packages.zip
[INFO] 2018-10-30 21:36:43,410 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 21:36:43,435 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36803.
[INFO] 2018-10-30 21:36:43,436 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:36803
[INFO] 2018-10-30 21:36:43,438 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 21:36:43,472 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 36803, None)
[INFO] 2018-10-30 21:36:43,480 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:36803 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 36803, None)
[INFO] 2018-10-30 21:36:43,484 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 36803, None)
[INFO] 2018-10-30 21:36:43,485 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 36803, None)
[INFO] 2018-10-30 21:36:43,795 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 21:36:43,795 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 21:36:44,413 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 21:39:18,165 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 21:39:18,178 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 21:39:18,195 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 21:39:18,210 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 21:39:18,211 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 21:39:18,212 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 21:39:18,219 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 21:39:18,229 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 21:39:18,229 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 21:39:18,231 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-99b27f18-d8d0-4ac9-9bb6-bb00cfcf7745/pyspark-a44f488d-9bda-4b15-b692-bc263e093c06
[INFO] 2018-10-30 21:39:18,232 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-99b27f18-d8d0-4ac9-9bb6-bb00cfcf7745
[INFO] 2018-10-30 21:39:18,233 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0044dbee-43d5-4725-a398-931a0280e2f4
[WARN] 2018-10-30 21:43:38,151 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 21:43:38,940 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 21:43:38,968 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-30 21:43:39,194 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 21:43:39,195 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 21:43:39,195 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 21:43:39,195 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 21:43:39,203 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 21:43:39,405 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 20923.
[INFO] 2018-10-30 21:43:39,431 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 21:43:39,452 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 21:43:39,455 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 21:43:39,455 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 21:43:39,465 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-fb331ec0-1835-426e-aefd-31f36be78d0b
[INFO] 2018-10-30 21:43:39,483 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 21:43:39,497 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 21:43:39,678 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 21:43:39,732 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 21:43:39,828 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540961019827
[INFO] 2018-10-30 21:43:39,830 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-7d052dee-09ad-42ed-a3b2-12c7b1503221/userFiles-f2b94bba-13f4-4752-9b9f-b1b3365735e8/etl_config.json
[INFO] 2018-10-30 21:43:39,842 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540961019842
[INFO] 2018-10-30 21:43:39,843 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-7d052dee-09ad-42ed-a3b2-12c7b1503221/userFiles-f2b94bba-13f4-4752-9b9f-b1b3365735e8/JB_STG_BOOKINGS.py
[INFO] 2018-10-30 21:43:39,848 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540961019848
[INFO] 2018-10-30 21:43:39,849 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-7d052dee-09ad-42ed-a3b2-12c7b1503221/userFiles-f2b94bba-13f4-4752-9b9f-b1b3365735e8/packages.zip
[INFO] 2018-10-30 21:43:39,926 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 21:43:39,957 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36825.
[INFO] 2018-10-30 21:43:39,958 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:36825
[INFO] 2018-10-30 21:43:39,960 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 21:43:39,996 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 36825, None)
[INFO] 2018-10-30 21:43:40,002 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:36825 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 36825, None)
[INFO] 2018-10-30 21:43:40,006 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 36825, None)
[INFO] 2018-10-30 21:43:40,008 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 36825, None)
[INFO] 2018-10-30 21:43:40,324 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 21:43:40,325 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 21:43:40,882 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 21:44:22,106 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 21:44:22,121 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 21:44:22,138 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 21:44:22,156 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 21:44:22,156 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 21:44:22,157 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 21:44:22,164 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 21:44:22,174 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 21:44:22,175 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 21:44:22,177 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7d052dee-09ad-42ed-a3b2-12c7b1503221/pyspark-edd5e269-f497-4d4c-bc71-ecf5adb8e653
[INFO] 2018-10-30 21:44:22,177 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-68eb0193-4987-4ac2-8a93-84071e1b3ab5
[INFO] 2018-10-30 21:44:22,178 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7d052dee-09ad-42ed-a3b2-12c7b1503221
[WARN] 2018-10-30 22:13:49,848 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:13:50,313 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:13:50,316 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e9a91ef7-47fb-48c3-843c-2c1dcba1cb36
[WARN] 2018-10-30 22:14:18,267 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:14:19,108 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:14:19,135 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING_REPORT
[INFO] 2018-10-30 22:14:19,346 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:14:19,346 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:14:19,347 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:14:19,347 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:14:19,347 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:14:19,580 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 12778.
[INFO] 2018-10-30 22:14:19,609 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:14:19,629 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:14:19,632 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:14:19,633 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:14:19,643 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-38f94e3f-12e2-482b-b8ea-c072d51d2d5b
[INFO] 2018-10-30 22:14:19,662 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:14:19,677 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 22:14:19,862 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 22:14:19,917 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:14:20,022 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:12778/files/etl_config.json with timestamp 1540962860022
[INFO] 2018-10-30 22:14:20,025 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-bf438822-7706-4242-9c21-1615c465c268/userFiles-fdd64e64-1694-43d3-b799-74c26c0c4f77/etl_config.json
[INFO] 2018-10-30 22:14:20,039 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py at spark://vmwebietl02-dev:12778/files/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540962860039
[INFO] 2018-10-30 22:14:20,040 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py to /tmp/spark-bf438822-7706-4242-9c21-1615c465c268/userFiles-fdd64e64-1694-43d3-b799-74c26c0c4f77/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 22:14:20,045 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:12778/files/packages.zip with timestamp 1540962860045
[INFO] 2018-10-30 22:14:20,046 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-bf438822-7706-4242-9c21-1615c465c268/userFiles-fdd64e64-1694-43d3-b799-74c26c0c4f77/packages.zip
[INFO] 2018-10-30 22:14:20,157 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-30 22:14:20,233 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 42 ms (0 ms spent in bootstraps)
[INFO] 2018-10-30 22:14:20,342 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181030221420-0013
[INFO] 2018-10-30 22:14:20,345 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030221420-0013/0 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 22:14:20,347 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030221420-0013/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 22:14:20,354 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25555.
[INFO] 2018-10-30 22:14:20,355 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:25555
[INFO] 2018-10-30 22:14:20,357 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:14:20,358 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/0 is now RUNNING
[INFO] 2018-10-30 22:14:20,391 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25555, None)
[INFO] 2018-10-30 22:14:20,395 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:25555 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 25555, None)
[INFO] 2018-10-30 22:14:20,398 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25555, None)
[INFO] 2018-10-30 22:14:20,399 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 25555, None)
[INFO] 2018-10-30 22:14:20,578 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-30 22:14:20,796 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:14:20,797 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:14:21,315 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:14:22,375 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15401) with ID 0
[ERROR] 2018-10-30 22:14:22,426 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5554943098661947037
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 22:14:22,462 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 22:14:22,469 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 0)
[INFO] 2018-10-30 22:14:22,481 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:22,483 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-30 22:14:22,483 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 0)
[INFO] 2018-10-30 22:14:22,807 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 22:14:22,809 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030221420-0013/0 removed: Command exited with code 1
[INFO] 2018-10-30 22:14:22,810 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030221420-0013/1 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 22:14:22,811 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:22,812 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-30 22:14:22,813 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030221420-0013/1 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 22:14:22,813 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/1 is now RUNNING
[INFO] 2018-10-30 22:14:22,812 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-30 22:14:24,738 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15407) with ID 1
[ERROR] 2018-10-30 22:14:25,026 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7410646871037326307
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 22:14:25,055 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 22:14:25,057 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 1)
[INFO] 2018-10-30 22:14:25,058 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:25,058 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-30 22:14:25,059 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 1)
[INFO] 2018-10-30 22:14:25,407 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 22:14:25,408 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030221420-0013/1 removed: Command exited with code 1
[INFO] 2018-10-30 22:14:25,408 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030221420-0013/2 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 22:14:25,408 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-30 22:14:25,409 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030221420-0013/2 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 22:14:25,409 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-30 22:14:25,408 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:25,414 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/2 is now RUNNING
[INFO] 2018-10-30 22:14:26,156 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 209.24778 ms
[INFO] 2018-10-30 22:14:26,288 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 22:14:26,309 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 22:14:26,309 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 22:14:26,310 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:14:26,312 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:14:26,319 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 22:14:26,392 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:14:26,428 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:14:26,432 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:25555 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:14:26,435 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:14:26,459 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:14:26,460 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 22:14:27,296 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15415) with ID 2
[INFO] 2018-10-30 22:14:27,317 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 22:14:27,346 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7564519136761121027
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 22:14:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 22:14:27,379 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 22:14:27,391 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 2)
[INFO] 2018-10-30 22:14:27,391 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:27,392 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-30 22:14:27,392 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 2)
[INFO] 2018-10-30 22:14:27,711 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 22:14:27,712 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030221420-0013/2 removed: Command exited with code 1
[INFO] 2018-10-30 22:14:27,712 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:27,712 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-30 22:14:27,713 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030221420-0013/3 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 22:14:27,713 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-30 22:14:27,714 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030221420-0013/3 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 22:14:27,716 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/3 is now RUNNING
[INFO] 2018-10-30 22:14:29,531 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15421) with ID 3
[INFO] 2018-10-30 22:14:29,535 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 22:14:29,576 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6312205148240261881
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 22:14:29,589 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 22:14:29,590 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 22:14:29,591 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 3)
[INFO] 2018-10-30 22:14:29,591 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:29,592 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-30 22:14:29,592 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 3)
[INFO] 2018-10-30 22:14:29,938 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/3 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 22:14:29,939 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030221420-0013/3 removed: Command exited with code 1
[INFO] 2018-10-30 22:14:29,939 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030221420-0013/4 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 22:14:29,939 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-30 22:14:29,939 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:29,940 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030221420-0013/4 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 22:14:29,940 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-30 22:14:29,941 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/4 is now RUNNING
[INFO] 2018-10-30 22:14:31,743 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15427) with ID 4
[INFO] 2018-10-30 22:14:31,746 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 22:14:31,802 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5920494881979016773
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 22:14:31,817 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 22:14:31,817 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-30 22:14:31,818 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 4)
[INFO] 2018-10-30 22:14:31,819 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:31,819 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-30 22:14:31,820 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 4)
[INFO] 2018-10-30 22:14:32,163 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-30 22:14:32,163 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181030221420-0013/4 removed: Command exited with code 1
[INFO] 2018-10-30 22:14:32,164 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181030221420-0013/5 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-30 22:14:32,164 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:32,164 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-30 22:14:32,165 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181030221420-0013/5 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-30 22:14:32,165 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-30 22:14:32,167 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181030221420-0013/5 is now RUNNING
[INFO] 2018-10-30 22:14:34,032 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:15433) with ID 5
[INFO] 2018-10-30 22:14:34,035 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-30 22:14:34,080 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5596893709802495036
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-30 22:14:34,094 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-30 22:14:34,095 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-30 22:14:34,098 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-30 22:14:34,102 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:14:34,106 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-30 22:14:34,110 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 7.770 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-30 22:14:34,115 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 5)
[INFO] 2018-10-30 22:14:34,116 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-30 22:14:34,116 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 7.826768 s
[INFO] 2018-10-30 22:14:34,117 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-30 22:14:34,118 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 5)
[INFO] 2018-10-30 22:14:34,178 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:14:34,189 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:14:34,196 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-30 22:14:34,197 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-30 22:14:34,210 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:14:34,220 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:14:34,221 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:14:34,222 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:14:34,231 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:14:34,238 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:14:34,239 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:14:34,240 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bf438822-7706-4242-9c21-1615c465c268/pyspark-9af969c4-5218-4845-b420-5b2b521084df
[INFO] 2018-10-30 22:14:34,240 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bf438822-7706-4242-9c21-1615c465c268
[INFO] 2018-10-30 22:14:34,241 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-861926a4-dd4c-49c1-bcd6-32f2cc106a8b
[WARN] 2018-10-30 22:15:20,553 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:15:21,393 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:15:21,419 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING_REPORT
[INFO] 2018-10-30 22:15:21,563 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:15:21,563 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:15:21,564 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:15:21,564 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:15:21,565 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:15:21,809 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 24934.
[INFO] 2018-10-30 22:15:21,840 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:15:21,864 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:15:21,867 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:15:21,868 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:15:21,879 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-48721586-8433-48e7-a49f-671ff63ddef3
[INFO] 2018-10-30 22:15:21,901 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:15:21,919 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 22:15:22,117 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 22:15:22,173 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:15:22,272 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540962922272
[INFO] 2018-10-30 22:15:22,274 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-35493536-0ce1-4fec-9d0e-fb1faf680203/userFiles-1ae60d29-3c7a-4314-af70-a2227a5fc93c/etl_config.json
[INFO] 2018-10-30 22:15:22,287 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py at file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540962922287
[INFO] 2018-10-30 22:15:22,288 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py to /tmp/spark-35493536-0ce1-4fec-9d0e-fb1faf680203/userFiles-1ae60d29-3c7a-4314-af70-a2227a5fc93c/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 22:15:22,293 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540962922293
[INFO] 2018-10-30 22:15:22,294 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-35493536-0ce1-4fec-9d0e-fb1faf680203/userFiles-1ae60d29-3c7a-4314-af70-a2227a5fc93c/packages.zip
[INFO] 2018-10-30 22:15:22,369 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:15:22,395 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35733.
[INFO] 2018-10-30 22:15:22,396 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:35733
[INFO] 2018-10-30 22:15:22,397 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:15:22,431 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35733, None)
[INFO] 2018-10-30 22:15:22,437 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:35733 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 35733, None)
[INFO] 2018-10-30 22:15:22,441 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35733, None)
[INFO] 2018-10-30 22:15:22,442 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 35733, None)
[INFO] 2018-10-30 22:15:22,723 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:15:22,724 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:15:23,257 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:15:26,589 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 213.639387 ms
[INFO] 2018-10-30 22:15:26,762 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 22:15:26,789 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 22:15:26,789 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 22:15:26,790 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:15:26,792 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:15:26,801 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 22:15:26,878 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:15:26,914 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:15:26,918 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:35733 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:15:26,921 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:15:26,936 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:15:26,938 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 22:15:27,002 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:15:27,018 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-30 22:15:27,024 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540962922272
[INFO] 2018-10-30 22:15:27,062 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-35493536-0ce1-4fec-9d0e-fb1faf680203/userFiles-1ae60d29-3c7a-4314-af70-a2227a5fc93c/etl_config.json
[INFO] 2018-10-30 22:15:27,069 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540962922287
[INFO] 2018-10-30 22:15:27,069 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py has been previously copied to /tmp/spark-35493536-0ce1-4fec-9d0e-fb1faf680203/userFiles-1ae60d29-3c7a-4314-af70-a2227a5fc93c/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 22:15:27,076 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540962922293
[INFO] 2018-10-30 22:15:27,077 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-35493536-0ce1-4fec-9d0e-fb1faf680203/userFiles-1ae60d29-3c7a-4314-af70-a2227a5fc93c/packages.zip
[INFO] 2018-10-30 22:15:28,755 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:15:28,799 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 534, boot = 440, init = 94, finish = 0
[INFO] 2018-10-30 22:15:28,819 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:15:28,836 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1854 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:15:28,843 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:15:28,849 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 2.026 s
[INFO] 2018-10-30 22:15:28,856 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 2.092770 s
[INFO] 2018-10-30 22:15:29,260 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340
[INFO] 2018-10-30 22:15:29,262 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) with 1 output partitions
[INFO] 2018-10-30 22:15:29,262 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340)
[INFO] 2018-10-30 22:15:29,262 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:15:29,263 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:15:29,263 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340), which has no missing parents
[INFO] 2018-10-30 22:15:29,267 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:15:29,269 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:15:29,270 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:35733 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:15:29,272 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:15:29,273 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:15:29,273 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-30 22:15:29,275 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:15:29,276 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-30 22:15:29,667 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:15:29,684 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -830, init = 872, finish = 0
[INFO] 2018-10-30 22:15:29,687 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-30 22:15:29,691 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 417 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:15:29,692 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:15:29,696 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) finished in 0.430 s
[INFO] 2018-10-30 22:15:29,697 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340, took 0.436613 s
[INFO] 2018-10-30 22:15:30,083 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:15:30,094 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:15:30,117 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:15:30,129 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:15:30,129 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:15:30,143 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:15:30,147 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:15:30,156 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:15:30,157 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:15:30,158 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4d8baa09-2c16-4196-9fbb-f91eb0476640
[INFO] 2018-10-30 22:15:30,159 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-35493536-0ce1-4fec-9d0e-fb1faf680203
[INFO] 2018-10-30 22:15:30,159 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-35493536-0ce1-4fec-9d0e-fb1faf680203/pyspark-e364c27f-9beb-4665-9c68-8a080354ae04
[WARN] 2018-10-30 22:19:25,855 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:19:26,715 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:19:26,742 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING_REPORT
[INFO] 2018-10-30 22:19:26,956 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:19:26,956 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:19:26,957 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:19:26,957 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:19:26,958 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:19:27,179 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 18177.
[INFO] 2018-10-30 22:19:27,209 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:19:27,231 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:19:27,234 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:19:27,235 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:19:27,245 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-b02a3e60-44eb-4635-ac57-1cc441eb9a5f
[INFO] 2018-10-30 22:19:27,264 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:19:27,284 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 22:19:27,488 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 22:19:27,542 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:19:27,649 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963167648
[INFO] 2018-10-30 22:19:27,651 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-2ecec277-15d3-4243-ab6d-a1f070464c84/userFiles-b79bca27-d173-44ae-bf4e-cd4db69e5d1f/etl_config.json
[INFO] 2018-10-30 22:19:27,667 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py at file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540963167667
[INFO] 2018-10-30 22:19:27,668 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py to /tmp/spark-2ecec277-15d3-4243-ab6d-a1f070464c84/userFiles-b79bca27-d173-44ae-bf4e-cd4db69e5d1f/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 22:19:27,673 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540963167673
[INFO] 2018-10-30 22:19:27,674 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-2ecec277-15d3-4243-ab6d-a1f070464c84/userFiles-b79bca27-d173-44ae-bf4e-cd4db69e5d1f/packages.zip
[INFO] 2018-10-30 22:19:27,744 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:19:27,769 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23278.
[INFO] 2018-10-30 22:19:27,770 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:23278
[INFO] 2018-10-30 22:19:27,772 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:19:27,805 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 23278, None)
[INFO] 2018-10-30 22:19:27,812 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:23278 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 23278, None)
[INFO] 2018-10-30 22:19:27,816 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 23278, None)
[INFO] 2018-10-30 22:19:27,816 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 23278, None)
[INFO] 2018-10-30 22:19:28,091 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:19:28,091 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:19:28,630 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:19:28,676 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:19:28,689 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:19:28,703 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:19:28,713 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:19:28,714 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:19:28,726 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:19:28,734 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:19:28,747 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:19:28,748 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:19:28,750 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2ecec277-15d3-4243-ab6d-a1f070464c84
[INFO] 2018-10-30 22:19:28,752 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2ecec277-15d3-4243-ab6d-a1f070464c84/pyspark-dd7115a7-b4e7-4b9a-b190-0c0f96353883
[INFO] 2018-10-30 22:19:28,752 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7d03892d-16ac-4607-b6ec-ade00ef10b4c
[WARN] 2018-10-30 22:23:59,613 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:24:00,459 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:24:00,486 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING_REPORT
[INFO] 2018-10-30 22:24:00,702 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:24:00,703 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:24:00,703 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:24:00,703 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:24:00,704 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:24:00,923 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11661.
[INFO] 2018-10-30 22:24:00,950 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:24:00,971 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:24:00,974 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:24:00,975 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:24:00,984 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-dcad3ace-f80e-4cf9-a7f6-cc21efce37f6
[INFO] 2018-10-30 22:24:01,003 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:24:01,018 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 22:24:01,207 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 22:24:01,259 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:24:01,358 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963441357
[INFO] 2018-10-30 22:24:01,360 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-ad25180e-d7de-455d-b646-1a163d842376/userFiles-20d22269-a199-48ca-a26b-a6562d8df360/etl_config.json
[INFO] 2018-10-30 22:24:01,374 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py at file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540963441374
[INFO] 2018-10-30 22:24:01,375 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py to /tmp/spark-ad25180e-d7de-455d-b646-1a163d842376/userFiles-20d22269-a199-48ca-a26b-a6562d8df360/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 22:24:01,380 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540963441380
[INFO] 2018-10-30 22:24:01,380 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-ad25180e-d7de-455d-b646-1a163d842376/userFiles-20d22269-a199-48ca-a26b-a6562d8df360/packages.zip
[INFO] 2018-10-30 22:24:01,455 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:24:01,481 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 17129.
[INFO] 2018-10-30 22:24:01,482 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:17129
[INFO] 2018-10-30 22:24:01,484 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:24:01,519 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17129, None)
[INFO] 2018-10-30 22:24:01,526 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:17129 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 17129, None)
[INFO] 2018-10-30 22:24:01,530 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17129, None)
[INFO] 2018-10-30 22:24:01,531 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 17129, None)
[INFO] 2018-10-30 22:24:01,807 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:24:01,808 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:24:02,336 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:24:02,384 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:24:02,400 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:24:02,414 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:24:02,424 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:24:02,424 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:24:02,434 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:24:02,442 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:24:02,451 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:24:02,452 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:24:02,453 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a39367a4-48f6-4615-94d2-7056243c8a9b
[INFO] 2018-10-30 22:24:02,453 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ad25180e-d7de-455d-b646-1a163d842376
[INFO] 2018-10-30 22:24:02,454 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ad25180e-d7de-455d-b646-1a163d842376/pyspark-fdea4db9-d00e-4ab6-916f-0af16edff709
[WARN] 2018-10-30 22:25:40,846 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:25:41,635 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:25:41,662 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING_REPORT
[INFO] 2018-10-30 22:25:41,850 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:25:41,850 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:25:41,851 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:25:41,851 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:25:41,865 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:25:42,102 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 34512.
[INFO] 2018-10-30 22:25:42,135 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:25:42,159 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:25:42,163 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:25:42,164 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:25:42,174 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-21c92836-afbb-4f48-971f-6750989b314d
[INFO] 2018-10-30 22:25:42,196 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:25:42,213 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 22:25:42,435 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 22:25:42,495 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:25:42,591 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963542591
[INFO] 2018-10-30 22:25:42,593 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-d19cc1c7-1ff0-4e7b-b153-b6fc7906256d/userFiles-17de4554-417c-4a3e-b1ef-70a2002a5669/etl_config.json
[INFO] 2018-10-30 22:25:42,607 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py at file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py with timestamp 1540963542607
[INFO] 2018-10-30 22:25:42,608 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING_REPORT.py to /tmp/spark-d19cc1c7-1ff0-4e7b-b153-b6fc7906256d/userFiles-17de4554-417c-4a3e-b1ef-70a2002a5669/JB_PRODUCT_BOOKING_REPORT.py
[INFO] 2018-10-30 22:25:42,613 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540963542613
[INFO] 2018-10-30 22:25:42,614 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-d19cc1c7-1ff0-4e7b-b153-b6fc7906256d/userFiles-17de4554-417c-4a3e-b1ef-70a2002a5669/packages.zip
[INFO] 2018-10-30 22:25:42,688 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:25:42,713 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10676.
[INFO] 2018-10-30 22:25:42,714 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:10676
[INFO] 2018-10-30 22:25:42,716 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:25:42,750 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 10676, None)
[INFO] 2018-10-30 22:25:42,754 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:10676 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 10676, None)
[INFO] 2018-10-30 22:25:42,757 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 10676, None)
[INFO] 2018-10-30 22:25:42,758 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 10676, None)
[INFO] 2018-10-30 22:25:43,042 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:25:43,043 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:25:43,592 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[WARN] 2018-10-30 22:28:26,900 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:28:27,776 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:28:27,803 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_DEBOOKING_POS
[INFO] 2018-10-30 22:28:27,975 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:28:27,976 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:28:27,976 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:28:27,976 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:28:27,977 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:28:28,210 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 34829.
[INFO] 2018-10-30 22:28:28,241 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:28:28,265 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:28:28,269 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:28:28,270 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:28:28,280 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-40f0cc89-49a3-4dbc-b890-bda781cd1d7a
[INFO] 2018-10-30 22:28:28,302 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:28:28,318 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-30 22:28:28,507 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-30 22:28:28,513 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-30 22:28:28,566 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-30 22:28:28,676 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963708676
[INFO] 2018-10-30 22:28:28,679 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-9db5aee6-57e8-4e97-bdd3-ae9abcd84fd0/userFiles-bb18d5b7-b713-4f3f-be28-6ff0c798c16f/etl_config.json
[INFO] 2018-10-30 22:28:28,694 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py with timestamp 1540963708694
[INFO] 2018-10-30 22:28:28,695 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py to /tmp/spark-9db5aee6-57e8-4e97-bdd3-ae9abcd84fd0/userFiles-bb18d5b7-b713-4f3f-be28-6ff0c798c16f/JB_STG_DEBOOKING_POS.py
[INFO] 2018-10-30 22:28:28,698 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540963708698
[INFO] 2018-10-30 22:28:28,699 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-9db5aee6-57e8-4e97-bdd3-ae9abcd84fd0/userFiles-bb18d5b7-b713-4f3f-be28-6ff0c798c16f/packages.zip
[INFO] 2018-10-30 22:28:28,766 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:28:28,793 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35071.
[INFO] 2018-10-30 22:28:28,794 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:35071
[INFO] 2018-10-30 22:28:28,796 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:28:28,832 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35071, None)
[INFO] 2018-10-30 22:28:28,838 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:35071 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 35071, None)
[INFO] 2018-10-30 22:28:28,842 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35071, None)
[INFO] 2018-10-30 22:28:28,843 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 35071, None)
[INFO] 2018-10-30 22:28:29,142 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:28:29,143 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:28:29,701 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:28:33,085 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 252.241178 ms
[INFO] 2018-10-30 22:28:33,239 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 22:28:33,266 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 22:28:33,267 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 22:28:33,268 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:28:33,270 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:28:33,280 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 22:28:33,356 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:28:33,390 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:28:33,393 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:35071 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:28:33,396 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:28:33,413 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:28:33,414 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 22:28:33,470 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:28:33,485 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-30 22:28:33,490 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963708676
[INFO] 2018-10-30 22:28:33,525 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-9db5aee6-57e8-4e97-bdd3-ae9abcd84fd0/userFiles-bb18d5b7-b713-4f3f-be28-6ff0c798c16f/etl_config.json
[INFO] 2018-10-30 22:28:33,531 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py with timestamp 1540963708694
[INFO] 2018-10-30 22:28:33,533 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py has been previously copied to /tmp/spark-9db5aee6-57e8-4e97-bdd3-ae9abcd84fd0/userFiles-bb18d5b7-b713-4f3f-be28-6ff0c798c16f/JB_STG_DEBOOKING_POS.py
[INFO] 2018-10-30 22:28:33,543 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540963708698
[INFO] 2018-10-30 22:28:33,544 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-9db5aee6-57e8-4e97-bdd3-ae9abcd84fd0/userFiles-bb18d5b7-b713-4f3f-be28-6ff0c798c16f/packages.zip
[INFO] 2018-10-30 22:28:34,462 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:28:34,501 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 589, boot = 477, init = 112, finish = 0
[INFO] 2018-10-30 22:28:34,521 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:28:34,537 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1079 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:28:34,541 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:28:34,554 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.250 s
[INFO] 2018-10-30 22:28:34,560 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.320707 s
[INFO] 2018-10-30 22:28:34,978 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409
[INFO] 2018-10-30 22:28:34,980 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) with 1 output partitions
[INFO] 2018-10-30 22:28:34,980 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409)
[INFO] 2018-10-30 22:28:34,980 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:28:34,981 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:28:34,982 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409), which has no missing parents
[INFO] 2018-10-30 22:28:34,989 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-30 22:28:34,991 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-30 22:28:34,992 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:35071 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:28:34,994 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:28:34,995 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:28:34,995 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-30 22:28:34,997 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:28:34,998 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-30 22:28:35,330 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:28:35,334 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 53, boot = -774, init = 827, finish = 0
[INFO] 2018-10-30 22:28:35,337 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:28:35,341 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 344 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:28:35,341 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:28:35,344 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) finished in 0.359 s
[INFO] 2018-10-30 22:28:35,345 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409, took 0.367351 s
[INFO] 2018-10-30 22:28:35,380 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:28:35,391 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-30 22:28:35,407 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:28:35,437 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:28:35,437 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:28:35,446 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:28:35,449 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:28:35,476 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:28:35,476 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:28:35,477 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-9db5aee6-57e8-4e97-bdd3-ae9abcd84fd0
[INFO] 2018-10-30 22:28:35,478 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-9db5aee6-57e8-4e97-bdd3-ae9abcd84fd0/pyspark-f756d8ea-0214-4bb0-9a04-cc699b60e099
[INFO] 2018-10-30 22:28:35,478 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-43346a89-80eb-4446-89cf-622cd48d81d1
[WARN] 2018-10-30 22:29:35,718 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:29:36,512 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:29:36,538 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_DEBOOKING_POS
[INFO] 2018-10-30 22:29:36,719 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:29:36,720 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:29:36,720 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:29:36,720 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:29:36,721 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:29:36,936 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 31126.
[INFO] 2018-10-30 22:29:36,963 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:29:36,983 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:29:36,986 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:29:36,986 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:29:36,995 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1fb8413d-0499-4bee-aa23-72da0528aa5b
[INFO] 2018-10-30 22:29:37,013 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:29:37,027 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-30 22:29:37,211 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-30 22:29:37,217 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-30 22:29:37,280 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-30 22:29:37,378 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963777377
[INFO] 2018-10-30 22:29:37,380 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-afe99d19-48f2-462d-b16b-7068afbef8dd/userFiles-b1c8484d-e794-4b47-8bbe-bef0736f8cc2/etl_config.json
[INFO] 2018-10-30 22:29:37,394 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py with timestamp 1540963777394
[INFO] 2018-10-30 22:29:37,394 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py to /tmp/spark-afe99d19-48f2-462d-b16b-7068afbef8dd/userFiles-b1c8484d-e794-4b47-8bbe-bef0736f8cc2/JB_STG_DEBOOKING_POS.py
[INFO] 2018-10-30 22:29:37,399 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540963777399
[INFO] 2018-10-30 22:29:37,399 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-afe99d19-48f2-462d-b16b-7068afbef8dd/userFiles-b1c8484d-e794-4b47-8bbe-bef0736f8cc2/packages.zip
[INFO] 2018-10-30 22:29:37,468 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:29:37,495 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24760.
[INFO] 2018-10-30 22:29:37,496 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:24760
[INFO] 2018-10-30 22:29:37,497 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:29:37,530 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 24760, None)
[INFO] 2018-10-30 22:29:37,536 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:24760 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 24760, None)
[INFO] 2018-10-30 22:29:37,540 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 24760, None)
[INFO] 2018-10-30 22:29:37,541 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 24760, None)
[INFO] 2018-10-30 22:29:37,824 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:29:37,824 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:29:38,404 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:29:41,700 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 204.91274 ms
[INFO] 2018-10-30 22:29:41,850 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 22:29:41,875 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 22:29:41,875 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 22:29:41,876 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:29:41,878 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:29:41,887 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 22:29:41,962 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:29:42,001 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:29:42,004 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:24760 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:29:42,007 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:29:42,021 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:29:42,022 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 22:29:42,069 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:29:42,083 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-30 22:29:42,091 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963777377
[INFO] 2018-10-30 22:29:42,123 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-afe99d19-48f2-462d-b16b-7068afbef8dd/userFiles-b1c8484d-e794-4b47-8bbe-bef0736f8cc2/etl_config.json
[INFO] 2018-10-30 22:29:42,128 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py with timestamp 1540963777394
[INFO] 2018-10-30 22:29:42,129 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py has been previously copied to /tmp/spark-afe99d19-48f2-462d-b16b-7068afbef8dd/userFiles-b1c8484d-e794-4b47-8bbe-bef0736f8cc2/JB_STG_DEBOOKING_POS.py
[INFO] 2018-10-30 22:29:42,134 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540963777399
[INFO] 2018-10-30 22:29:42,135 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-afe99d19-48f2-462d-b16b-7068afbef8dd/userFiles-b1c8484d-e794-4b47-8bbe-bef0736f8cc2/packages.zip
[INFO] 2018-10-30 22:29:42,927 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:29:42,969 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 474, boot = 385, init = 88, finish = 1
[INFO] 2018-10-30 22:29:42,988 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:29:43,003 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 946 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:29:43,008 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:29:43,022 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.111 s
[INFO] 2018-10-30 22:29:43,027 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.176702 s
[INFO] 2018-10-30 22:29:43,401 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409
[INFO] 2018-10-30 22:29:43,405 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) with 1 output partitions
[INFO] 2018-10-30 22:29:43,405 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409)
[INFO] 2018-10-30 22:29:43,405 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:29:43,406 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:29:43,407 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409), which has no missing parents
[INFO] 2018-10-30 22:29:43,411 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-30 22:29:43,413 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-30 22:29:43,415 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:24760 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:29:43,417 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:29:43,417 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:29:43,418 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-30 22:29:43,419 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:29:43,420 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-30 22:29:43,742 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:29:43,749 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -728, init = 770, finish = 0
[INFO] 2018-10-30 22:29:43,751 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:29:43,754 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 335 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:29:43,755 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:29:43,757 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) finished in 0.348 s
[INFO] 2018-10-30 22:29:43,758 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409, took 0.356242 s
[INFO] 2018-10-30 22:29:44,094 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:29:44,107 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-30 22:29:44,126 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:29:44,150 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:29:44,151 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:29:44,159 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:29:44,171 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:29:44,193 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:29:44,194 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:29:44,196 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-afe99d19-48f2-462d-b16b-7068afbef8dd
[INFO] 2018-10-30 22:29:44,197 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fb4680ff-cd69-40ba-b46f-e7a7edf76435
[INFO] 2018-10-30 22:29:44,198 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-afe99d19-48f2-462d-b16b-7068afbef8dd/pyspark-0d1d667f-b9ea-4fbd-b310-556cf75bfcd7
[WARN] 2018-10-30 22:31:13,657 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:31:14,456 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:31:14,483 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_DEBOOKING_POS
[INFO] 2018-10-30 22:31:14,618 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:31:14,618 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:31:14,619 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:31:14,619 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:31:14,619 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:31:14,828 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 33147.
[INFO] 2018-10-30 22:31:14,852 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:31:14,872 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:31:14,875 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:31:14,876 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:31:14,885 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-6b01430b-93d4-4fd8-b5a0-4ddaf7937aee
[INFO] 2018-10-30 22:31:14,903 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:31:14,917 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-30 22:31:15,091 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-30 22:31:15,097 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-30 22:31:15,151 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-30 22:31:15,248 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963875247
[INFO] 2018-10-30 22:31:15,250 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-bab237c0-1bca-419e-839b-d335d97b58ed/userFiles-7c4614da-e279-4939-8321-9b362c752689/etl_config.json
[INFO] 2018-10-30 22:31:15,264 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py with timestamp 1540963875264
[INFO] 2018-10-30 22:31:15,264 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py to /tmp/spark-bab237c0-1bca-419e-839b-d335d97b58ed/userFiles-7c4614da-e279-4939-8321-9b362c752689/JB_STG_DEBOOKING_POS.py
[INFO] 2018-10-30 22:31:15,269 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540963875269
[INFO] 2018-10-30 22:31:15,269 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-bab237c0-1bca-419e-839b-d335d97b58ed/userFiles-7c4614da-e279-4939-8321-9b362c752689/packages.zip
[INFO] 2018-10-30 22:31:15,339 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:31:15,368 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15402.
[INFO] 2018-10-30 22:31:15,369 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:15402
[INFO] 2018-10-30 22:31:15,371 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:31:15,406 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 15402, None)
[INFO] 2018-10-30 22:31:15,413 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:15402 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 15402, None)
[INFO] 2018-10-30 22:31:15,417 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 15402, None)
[INFO] 2018-10-30 22:31:15,418 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 15402, None)
[INFO] 2018-10-30 22:31:15,737 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:31:15,738 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:31:16,248 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:31:19,841 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 210.111131 ms
[INFO] 2018-10-30 22:31:20,026 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 22:31:20,049 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 22:31:20,049 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 22:31:20,050 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:31:20,052 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:31:20,061 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 22:31:20,144 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:31:20,183 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:31:20,186 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:15402 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:31:20,189 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:31:20,202 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:31:20,203 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 22:31:20,254 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:31:20,268 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-30 22:31:20,273 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540963875247
[INFO] 2018-10-30 22:31:20,302 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-bab237c0-1bca-419e-839b-d335d97b58ed/userFiles-7c4614da-e279-4939-8321-9b362c752689/etl_config.json
[INFO] 2018-10-30 22:31:20,307 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py with timestamp 1540963875264
[INFO] 2018-10-30 22:31:20,308 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_STG_DEBOOKING_POS.py has been previously copied to /tmp/spark-bab237c0-1bca-419e-839b-d335d97b58ed/userFiles-7c4614da-e279-4939-8321-9b362c752689/JB_STG_DEBOOKING_POS.py
[INFO] 2018-10-30 22:31:20,311 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540963875269
[INFO] 2018-10-30 22:31:20,313 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-bab237c0-1bca-419e-839b-d335d97b58ed/userFiles-7c4614da-e279-4939-8321-9b362c752689/packages.zip
[INFO] 2018-10-30 22:31:21,103 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:31:21,143 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 474, boot = 396, init = 77, finish = 1
[INFO] 2018-10-30 22:31:21,162 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:31:21,182 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 943 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:31:21,186 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:31:21,200 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.109 s
[INFO] 2018-10-30 22:31:21,206 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.180004 s
[INFO] 2018-10-30 22:31:21,585 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409
[INFO] 2018-10-30 22:31:21,587 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) with 1 output partitions
[INFO] 2018-10-30 22:31:21,587 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409)
[INFO] 2018-10-30 22:31:21,588 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:31:21,588 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:31:21,589 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409), which has no missing parents
[INFO] 2018-10-30 22:31:21,596 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-30 22:31:21,599 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-30 22:31:21,600 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:15402 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:31:21,602 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:31:21,604 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:31:21,605 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-30 22:31:21,607 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:31:21,608 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-30 22:31:21,925 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:31:21,944 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -746, init = 788, finish = 0
[INFO] 2018-10-30 22:31:21,946 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:31:21,949 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 343 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:31:21,950 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:31:21,953 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409) finished in 0.361 s
[INFO] 2018-10-30 22:31:21,954 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:409, took 0.368501 s
[WARN] 2018-10-30 22:46:26,317 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:46:27,235 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:46:27,262 org.apache.spark.SparkContext logInfo - Submitted application: jb_installed
[INFO] 2018-10-30 22:46:27,435 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:46:27,436 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:46:27,436 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:46:27,436 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:46:27,444 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:46:27,678 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 10397.
[INFO] 2018-10-30 22:46:27,705 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:46:27,726 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:46:27,729 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:46:27,729 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:46:27,738 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-974471c2-4243-467b-b4f1-e075ca50942a
[INFO] 2018-10-30 22:46:27,757 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:46:27,772 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-30 22:46:27,967 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-30 22:46:27,968 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-30 22:46:27,975 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-30 22:46:28,036 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-30 22:46:28,154 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540964788154
[INFO] 2018-10-30 22:46:28,156 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-0d630cee-4a05-46c4-af5a-5f31a61000fc/userFiles-d26840ef-8e23-42a0-9ac7-c225b9cca747/etl_config.json
[INFO] 2018-10-30 22:46:28,171 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py at file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py with timestamp 1540964788171
[INFO] 2018-10-30 22:46:28,171 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/jb_installed.py to /tmp/spark-0d630cee-4a05-46c4-af5a-5f31a61000fc/userFiles-d26840ef-8e23-42a0-9ac7-c225b9cca747/jb_installed.py
[INFO] 2018-10-30 22:46:28,176 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540964788176
[INFO] 2018-10-30 22:46:28,177 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-0d630cee-4a05-46c4-af5a-5f31a61000fc/userFiles-d26840ef-8e23-42a0-9ac7-c225b9cca747/packages.zip
[INFO] 2018-10-30 22:46:28,250 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:46:28,279 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33585.
[INFO] 2018-10-30 22:46:28,280 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:33585
[INFO] 2018-10-30 22:46:28,283 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:46:28,316 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 33585, None)
[INFO] 2018-10-30 22:46:28,327 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:33585 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 33585, None)
[INFO] 2018-10-30 22:46:28,330 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 33585, None)
[INFO] 2018-10-30 22:46:28,331 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 33585, None)
[INFO] 2018-10-30 22:46:28,646 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:46:28,647 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:46:28,661 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:46:28,677 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:46:28,700 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:46:28,743 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:46:28,744 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:46:28,745 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:46:28,753 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:46:28,777 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:46:28,778 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:46:28,779 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d19cc1c7-1ff0-4e7b-b153-b6fc7906256d/pyspark-807f9878-f7d7-4a42-93a4-551c41ac1512
[INFO] 2018-10-30 22:46:28,779 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-22ed7b83-2203-44a2-b6e8-34fc4d151d1e
[INFO] 2018-10-30 22:46:28,780 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d19cc1c7-1ff0-4e7b-b153-b6fc7906256d
[INFO] 2018-10-30 22:46:29,177 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:46:32,224 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 214.031135 ms
[INFO] 2018-10-30 22:46:32,375 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 22:46:32,401 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 22:46:32,402 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 22:46:32,403 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:46:32,404 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:46:32,414 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 22:46:32,487 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:46:32,520 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:46:32,524 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:33585 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:46:32,527 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:46:32,541 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:46:32,543 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 22:46:32,595 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:46:32,606 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-30 22:46:32,611 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540964788154
[INFO] 2018-10-30 22:46:32,640 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-0d630cee-4a05-46c4-af5a-5f31a61000fc/userFiles-d26840ef-8e23-42a0-9ac7-c225b9cca747/etl_config.json
[INFO] 2018-10-30 22:46:32,644 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540964788176
[INFO] 2018-10-30 22:46:32,646 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-0d630cee-4a05-46c4-af5a-5f31a61000fc/userFiles-d26840ef-8e23-42a0-9ac7-c225b9cca747/packages.zip
[INFO] 2018-10-30 22:46:32,649 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py with timestamp 1540964788171
[INFO] 2018-10-30 22:46:32,651 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/jb_installed.py has been previously copied to /tmp/spark-0d630cee-4a05-46c4-af5a-5f31a61000fc/userFiles-d26840ef-8e23-42a0-9ac7-c225b9cca747/jb_installed.py
[INFO] 2018-10-30 22:46:33,500 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:46:33,538 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 523, boot = 422, init = 100, finish = 1
[INFO] 2018-10-30 22:46:33,557 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:46:33,571 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 987 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:46:33,576 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:46:33,590 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.153 s
[INFO] 2018-10-30 22:46:33,596 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.220249 s
[INFO] 2018-10-30 22:46:33,636 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:46:33,645 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-30 22:46:33,660 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:46:33,673 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:46:33,673 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:46:33,688 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:46:33,691 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:46:33,702 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:46:33,702 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:46:33,703 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bb9dfc65-3f95-4e65-a8a1-2b4d6646848e
[INFO] 2018-10-30 22:46:33,704 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0d630cee-4a05-46c4-af5a-5f31a61000fc
[INFO] 2018-10-30 22:46:33,704 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0d630cee-4a05-46c4-af5a-5f31a61000fc/pyspark-44a235aa-3205-4980-90b8-2d6ee57136c4
[WARN] 2018-10-30 22:47:58,242 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:47:59,178 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:47:59,205 org.apache.spark.SparkContext logInfo - Submitted application: jb_installed
[INFO] 2018-10-30 22:47:59,399 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:47:59,400 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:47:59,400 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:47:59,401 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:47:59,401 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:47:59,635 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 13573.
[INFO] 2018-10-30 22:47:59,663 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:47:59,683 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:47:59,686 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:47:59,687 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:47:59,696 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-db4cfc7c-f0e7-4ec2-afb3-ca1d7de6ab1e
[INFO] 2018-10-30 22:47:59,715 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:47:59,729 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 22:47:59,934 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 22:47:59,994 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:48:00,109 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540964880109
[INFO] 2018-10-30 22:48:00,112 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-75a5db59-8dd9-40d3-a032-c328fcef9f1f/userFiles-50221792-2f7e-4697-b3be-a026b21857d4/etl_config.json
[INFO] 2018-10-30 22:48:00,128 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py at file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py with timestamp 1540964880128
[INFO] 2018-10-30 22:48:00,129 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/jb_installed.py to /tmp/spark-75a5db59-8dd9-40d3-a032-c328fcef9f1f/userFiles-50221792-2f7e-4697-b3be-a026b21857d4/jb_installed.py
[INFO] 2018-10-30 22:48:00,134 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540964880134
[INFO] 2018-10-30 22:48:00,135 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-75a5db59-8dd9-40d3-a032-c328fcef9f1f/userFiles-50221792-2f7e-4697-b3be-a026b21857d4/packages.zip
[INFO] 2018-10-30 22:48:00,211 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:48:00,237 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9022.
[INFO] 2018-10-30 22:48:00,238 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:9022
[INFO] 2018-10-30 22:48:00,240 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:48:00,275 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 9022, None)
[INFO] 2018-10-30 22:48:00,282 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:9022 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 9022, None)
[INFO] 2018-10-30 22:48:00,286 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 9022, None)
[INFO] 2018-10-30 22:48:00,287 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 9022, None)
[INFO] 2018-10-30 22:48:00,595 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:48:00,596 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:48:01,159 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:48:04,658 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 238.54855 ms
[INFO] 2018-10-30 22:48:04,810 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 22:48:04,836 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 22:48:04,837 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 22:48:04,837 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:48:04,839 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:48:04,849 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 22:48:04,930 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:48:04,973 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:48:04,978 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:9022 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:48:04,981 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:48:04,997 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:48:04,998 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 22:48:05,055 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:48:05,071 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-30 22:48:05,079 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540964880109
[INFO] 2018-10-30 22:48:05,123 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-75a5db59-8dd9-40d3-a032-c328fcef9f1f/userFiles-50221792-2f7e-4697-b3be-a026b21857d4/etl_config.json
[INFO] 2018-10-30 22:48:05,130 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540964880134
[INFO] 2018-10-30 22:48:05,131 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-75a5db59-8dd9-40d3-a032-c328fcef9f1f/userFiles-50221792-2f7e-4697-b3be-a026b21857d4/packages.zip
[INFO] 2018-10-30 22:48:05,142 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py with timestamp 1540964880128
[INFO] 2018-10-30 22:48:05,143 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/jb_installed.py has been previously copied to /tmp/spark-75a5db59-8dd9-40d3-a032-c328fcef9f1f/userFiles-50221792-2f7e-4697-b3be-a026b21857d4/jb_installed.py
[INFO] 2018-10-30 22:48:06,017 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:48:06,057 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 526, boot = 421, init = 105, finish = 0
[INFO] 2018-10-30 22:48:06,083 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1706 bytes result sent to driver
[INFO] 2018-10-30 22:48:06,100 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1059 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:48:06,107 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:48:06,113 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.235 s
[INFO] 2018-10-30 22:48:06,119 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.307964 s
[INFO] 2018-10-30 22:48:06,157 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:48:06,171 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:48:06,187 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:48:06,203 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:48:06,203 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:48:06,216 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:48:06,221 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:48:06,245 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:48:06,246 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:48:06,249 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-75a5db59-8dd9-40d3-a032-c328fcef9f1f
[INFO] 2018-10-30 22:48:06,250 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-75a5db59-8dd9-40d3-a032-c328fcef9f1f/pyspark-ac26fae6-e2ca-4b64-8c6b-463c7dc03f90
[INFO] 2018-10-30 22:48:06,250 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-20c258a2-6062-40ce-8d55-2e7a7fe54c4d
[WARN] 2018-10-30 22:48:32,117 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 22:48:32,918 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 22:48:32,945 org.apache.spark.SparkContext logInfo - Submitted application: jb_installed
[INFO] 2018-10-30 22:48:33,157 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 22:48:33,157 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 22:48:33,157 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 22:48:33,158 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 22:48:33,158 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 22:48:33,368 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 15972.
[INFO] 2018-10-30 22:48:33,394 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 22:48:33,414 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 22:48:33,417 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 22:48:33,417 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 22:48:33,427 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-91fb61d3-26c5-4c36-8ac2-9a7e57db1ed3
[INFO] 2018-10-30 22:48:33,444 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 22:48:33,458 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 22:48:33,641 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 22:48:33,694 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:48:33,793 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540964913792
[INFO] 2018-10-30 22:48:33,794 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-cc2ef18e-8924-49c7-aa86-b1b07f60af2e/userFiles-58befb14-5188-47af-96f4-47a901a5e648/etl_config.json
[INFO] 2018-10-30 22:48:33,808 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py at file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py with timestamp 1540964913808
[INFO] 2018-10-30 22:48:33,809 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/jb_installed.py to /tmp/spark-cc2ef18e-8924-49c7-aa86-b1b07f60af2e/userFiles-58befb14-5188-47af-96f4-47a901a5e648/jb_installed.py
[INFO] 2018-10-30 22:48:33,813 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540964913813
[INFO] 2018-10-30 22:48:33,814 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-cc2ef18e-8924-49c7-aa86-b1b07f60af2e/userFiles-58befb14-5188-47af-96f4-47a901a5e648/packages.zip
[INFO] 2018-10-30 22:48:33,884 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 22:48:33,908 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25841.
[INFO] 2018-10-30 22:48:33,909 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:25841
[INFO] 2018-10-30 22:48:33,911 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 22:48:33,946 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25841, None)
[INFO] 2018-10-30 22:48:33,954 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:25841 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 25841, None)
[INFO] 2018-10-30 22:48:33,959 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25841, None)
[INFO] 2018-10-30 22:48:33,960 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 25841, None)
[INFO] 2018-10-30 22:48:34,258 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 22:48:34,259 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 22:48:34,793 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 22:48:38,067 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 249.485045 ms
[INFO] 2018-10-30 22:48:38,235 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 22:48:38,263 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 22:48:38,263 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 22:48:38,264 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 22:48:38,266 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 22:48:38,282 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 22:48:38,360 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 22:48:38,395 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 22:48:38,400 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:25841 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 22:48:38,403 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 22:48:38,427 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 22:48:38,429 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 22:48:38,476 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 22:48:38,491 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-30 22:48:38,499 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540964913792
[INFO] 2018-10-30 22:48:38,532 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-cc2ef18e-8924-49c7-aa86-b1b07f60af2e/userFiles-58befb14-5188-47af-96f4-47a901a5e648/etl_config.json
[INFO] 2018-10-30 22:48:38,539 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540964913813
[INFO] 2018-10-30 22:48:38,539 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-cc2ef18e-8924-49c7-aa86-b1b07f60af2e/userFiles-58befb14-5188-47af-96f4-47a901a5e648/packages.zip
[INFO] 2018-10-30 22:48:38,546 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/jb_installed.py with timestamp 1540964913808
[INFO] 2018-10-30 22:48:38,548 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/jb_installed.py has been previously copied to /tmp/spark-cc2ef18e-8924-49c7-aa86-b1b07f60af2e/userFiles-58befb14-5188-47af-96f4-47a901a5e648/jb_installed.py
[INFO] 2018-10-30 22:48:39,314 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 22:48:39,351 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 478, boot = 380, init = 97, finish = 1
[INFO] 2018-10-30 22:48:39,371 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-30 22:48:39,389 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 925 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 22:48:39,394 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 22:48:39,409 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.100 s
[INFO] 2018-10-30 22:48:39,415 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.180000 s
[INFO] 2018-10-30 22:48:39,767 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:48:39,779 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 22:48:39,799 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:48:39,811 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:48:39,812 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:48:39,822 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:48:39,827 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:48:39,842 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:48:39,843 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:48:39,845 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-cc2ef18e-8924-49c7-aa86-b1b07f60af2e/pyspark-3f9d5f14-c824-4cd9-a3b2-03bc13b06ea4
[INFO] 2018-10-30 22:48:39,846 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7a1602a5-2b1f-49ff-af3f-231ea88b7dc9
[INFO] 2018-10-30 22:48:39,847 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-cc2ef18e-8924-49c7-aa86-b1b07f60af2e
[INFO] 2018-10-30 22:58:43,533 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 22:58:43,548 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-30 22:58:43,571 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 22:58:43,590 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 22:58:43,590 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 22:58:43,591 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 22:58:43,596 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 22:58:43,605 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 22:58:43,606 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 22:58:43,608 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bab237c0-1bca-419e-839b-d335d97b58ed/pyspark-4bd8383c-9005-47bc-b472-4f756cb00b84
[INFO] 2018-10-30 22:58:43,608 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bab237c0-1bca-419e-839b-d335d97b58ed
[INFO] 2018-10-30 22:58:43,609 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-319d8626-ebf2-45b2-b2fe-05de40a6bb2f
[WARN] 2018-10-30 23:02:57,680 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 23:02:58,474 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 23:02:58,502 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_DEBOOKING_TRUNCATE
[INFO] 2018-10-30 23:02:58,725 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 23:02:58,726 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 23:02:58,726 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 23:02:58,726 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 23:02:58,727 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 23:02:58,940 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 20673.
[INFO] 2018-10-30 23:02:58,966 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 23:02:58,986 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 23:02:58,989 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 23:02:58,990 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 23:02:58,999 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-bd432913-0609-4e01-9eaf-282583fa3b60
[INFO] 2018-10-30 23:02:59,018 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 23:02:59,032 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 23:02:59,219 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 23:02:59,273 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:02:59,370 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540965779369
[INFO] 2018-10-30 23:02:59,372 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-19879042-047e-450e-bb89-567654306241/userFiles-fea4f754-789f-4808-bba8-0e76cab8d8bd/etl_config.json
[INFO] 2018-10-30 23:02:59,387 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_DEBOOKING_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_DEBOOKING_TRUNCATE.py with timestamp 1540965779387
[INFO] 2018-10-30 23:02:59,388 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_DEBOOKING_TRUNCATE.py to /tmp/spark-19879042-047e-450e-bb89-567654306241/userFiles-fea4f754-789f-4808-bba8-0e76cab8d8bd/JB_WORK_DEBOOKING_TRUNCATE.py
[INFO] 2018-10-30 23:02:59,391 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540965779391
[INFO] 2018-10-30 23:02:59,392 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-19879042-047e-450e-bb89-567654306241/userFiles-fea4f754-789f-4808-bba8-0e76cab8d8bd/packages.zip
[INFO] 2018-10-30 23:02:59,459 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 23:02:59,484 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23478.
[INFO] 2018-10-30 23:02:59,486 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:23478
[INFO] 2018-10-30 23:02:59,488 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 23:02:59,523 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 23478, None)
[INFO] 2018-10-30 23:02:59,530 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:23478 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 23478, None)
[INFO] 2018-10-30 23:02:59,535 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 23478, None)
[INFO] 2018-10-30 23:02:59,536 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 23478, None)
[INFO] 2018-10-30 23:02:59,847 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 23:02:59,848 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 23:03:00,468 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 23:03:01,865 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 23:03:01,878 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:03:01,890 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 23:03:01,905 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 23:03:01,906 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 23:03:01,918 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 23:03:01,925 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 23:03:01,936 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 23:03:01,937 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 23:03:01,938 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-19879042-047e-450e-bb89-567654306241/pyspark-b8aca4d1-4daf-4877-a0a1-e3f510a28d39
[INFO] 2018-10-30 23:03:01,938 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ac8be654-9174-4152-b50b-32d7a3d21e27
[INFO] 2018-10-30 23:03:01,939 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-19879042-047e-450e-bb89-567654306241
[WARN] 2018-10-30 23:09:49,732 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 23:09:50,650 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 23:09:50,678 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_DEBOOKING
[INFO] 2018-10-30 23:09:50,929 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 23:09:50,930 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 23:09:50,930 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 23:09:50,930 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 23:09:50,931 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 23:09:51,143 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 29832.
[INFO] 2018-10-30 23:09:51,168 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 23:09:51,188 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 23:09:51,191 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 23:09:51,192 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 23:09:51,201 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-ac7da501-36c6-48ad-96c2-c94e742b6bca
[INFO] 2018-10-30 23:09:51,218 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 23:09:51,232 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 23:09:51,411 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 23:09:51,461 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:09:51,555 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540966191555
[INFO] 2018-10-30 23:09:51,557 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-eb08abed-174f-40bb-a98f-8efa7c648dd8/userFiles-f8504a3f-dc2f-4714-afd6-2cf34ad79db2/etl_config.json
[INFO] 2018-10-30 23:09:51,571 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_DEBOOKING.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_DEBOOKING.py with timestamp 1540966191571
[INFO] 2018-10-30 23:09:51,571 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_DEBOOKING.py to /tmp/spark-eb08abed-174f-40bb-a98f-8efa7c648dd8/userFiles-f8504a3f-dc2f-4714-afd6-2cf34ad79db2/JB_WORK_DEBOOKING.py
[INFO] 2018-10-30 23:09:51,576 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540966191576
[INFO] 2018-10-30 23:09:51,577 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-eb08abed-174f-40bb-a98f-8efa7c648dd8/userFiles-f8504a3f-dc2f-4714-afd6-2cf34ad79db2/packages.zip
[INFO] 2018-10-30 23:09:51,647 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 23:09:51,673 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27542.
[INFO] 2018-10-30 23:09:51,674 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:27542
[INFO] 2018-10-30 23:09:51,676 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 23:09:51,710 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 27542, None)
[INFO] 2018-10-30 23:09:51,718 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:27542 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 27542, None)
[INFO] 2018-10-30 23:09:51,723 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 27542, None)
[INFO] 2018-10-30 23:09:51,724 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 27542, None)
[INFO] 2018-10-30 23:09:52,034 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 23:09:52,035 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 23:09:52,599 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 23:09:53,120 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 23:09:53,134 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:09:53,147 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 23:09:53,158 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 23:09:53,159 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 23:09:53,167 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 23:09:53,172 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 23:09:53,181 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 23:09:53,182 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 23:09:53,183 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0843290a-7842-4df9-8ad3-c33d00e3a28e
[INFO] 2018-10-30 23:09:53,184 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-eb08abed-174f-40bb-a98f-8efa7c648dd8
[INFO] 2018-10-30 23:09:53,184 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-eb08abed-174f-40bb-a98f-8efa7c648dd8/pyspark-19e3185b-7544-474a-98ab-686fe0a00b05
[WARN] 2018-10-30 23:20:56,963 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 23:20:57,906 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 23:20:57,933 org.apache.spark.SparkContext logInfo - Submitted application: JB_UPD_BOOKINGS
[INFO] 2018-10-30 23:20:58,150 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 23:20:58,150 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 23:20:58,151 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 23:20:58,151 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 23:20:58,151 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 23:20:58,364 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 16813.
[INFO] 2018-10-30 23:20:58,389 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 23:20:58,409 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 23:20:58,412 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 23:20:58,413 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 23:20:58,422 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-f12eea04-41ab-41e7-ac5c-310510591ba5
[INFO] 2018-10-30 23:20:58,440 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 23:20:58,454 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 23:20:58,644 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 23:20:58,704 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:20:58,802 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540966858802
[INFO] 2018-10-30 23:20:58,805 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-d109cc4f-0831-46a3-a127-195522732751/userFiles-11df5329-1598-4d18-8790-62153bbb69d9/etl_config.json
[INFO] 2018-10-30 23:20:58,819 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_UPD_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_UPD_BOOKINGS.py with timestamp 1540966858819
[INFO] 2018-10-30 23:20:58,820 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_UPD_BOOKINGS.py to /tmp/spark-d109cc4f-0831-46a3-a127-195522732751/userFiles-11df5329-1598-4d18-8790-62153bbb69d9/JB_UPD_BOOKINGS.py
[INFO] 2018-10-30 23:20:58,825 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540966858825
[INFO] 2018-10-30 23:20:58,826 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-d109cc4f-0831-46a3-a127-195522732751/userFiles-11df5329-1598-4d18-8790-62153bbb69d9/packages.zip
[INFO] 2018-10-30 23:20:58,895 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 23:20:58,922 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36056.
[INFO] 2018-10-30 23:20:58,923 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:36056
[INFO] 2018-10-30 23:20:58,925 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 23:20:58,960 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 36056, None)
[INFO] 2018-10-30 23:20:58,968 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:36056 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 36056, None)
[INFO] 2018-10-30 23:20:58,973 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 36056, None)
[INFO] 2018-10-30 23:20:58,975 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 36056, None)
[INFO] 2018-10-30 23:20:59,297 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 23:20:59,298 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 23:20:59,872 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 23:21:00,389 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 23:21:00,401 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:21:00,416 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 23:21:00,431 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 23:21:00,432 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 23:21:00,445 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 23:21:00,451 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 23:21:00,463 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 23:21:00,464 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 23:21:00,466 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a0c1eb22-ea51-47a6-8ce2-1bca5a3a07d2
[INFO] 2018-10-30 23:21:00,467 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d109cc4f-0831-46a3-a127-195522732751/pyspark-92b717f0-ff97-4aca-8445-e73eed485533
[INFO] 2018-10-30 23:21:00,467 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d109cc4f-0831-46a3-a127-195522732751
[WARN] 2018-10-30 23:23:14,753 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 23:23:15,615 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 23:23:15,642 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_PARTNER_PRODUCT_TRUNCATE
[INFO] 2018-10-30 23:23:15,847 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 23:23:15,848 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 23:23:15,848 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 23:23:15,849 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 23:23:15,862 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 23:23:16,068 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 19929.
[INFO] 2018-10-30 23:23:16,096 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 23:23:16,116 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 23:23:16,119 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 23:23:16,119 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 23:23:16,128 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-9f51a763-70c6-459a-85f6-a757f1932c4c
[INFO] 2018-10-30 23:23:16,146 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 23:23:16,160 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 23:23:16,340 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 23:23:16,397 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:23:16,489 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540966996488
[INFO] 2018-10-30 23:23:16,491 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-8036559d-f44d-4a38-944e-4deaf4ca8c25/userFiles-3d4735a4-b379-472e-8f21-2eaf16e6ae4c/etl_config.json
[INFO] 2018-10-30 23:23:16,503 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_PARTNER_PRODUCT_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_PARTNER_PRODUCT_TRUNCATE.py with timestamp 1540966996503
[INFO] 2018-10-30 23:23:16,503 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_PARTNER_PRODUCT_TRUNCATE.py to /tmp/spark-8036559d-f44d-4a38-944e-4deaf4ca8c25/userFiles-3d4735a4-b379-472e-8f21-2eaf16e6ae4c/JB_STG_PARTNER_PRODUCT_TRUNCATE.py
[INFO] 2018-10-30 23:23:16,506 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540966996506
[INFO] 2018-10-30 23:23:16,506 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-8036559d-f44d-4a38-944e-4deaf4ca8c25/userFiles-3d4735a4-b379-472e-8f21-2eaf16e6ae4c/packages.zip
[INFO] 2018-10-30 23:23:16,573 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 23:23:16,597 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18877.
[INFO] 2018-10-30 23:23:16,598 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:18877
[INFO] 2018-10-30 23:23:16,600 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 23:23:16,633 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18877, None)
[INFO] 2018-10-30 23:23:16,640 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:18877 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 18877, None)
[INFO] 2018-10-30 23:23:16,644 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18877, None)
[INFO] 2018-10-30 23:23:16,645 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 18877, None)
[INFO] 2018-10-30 23:23:16,944 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 23:23:16,944 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 23:23:17,495 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 23:23:18,389 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 23:23:18,402 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:23:18,418 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 23:23:18,429 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 23:23:18,430 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 23:23:18,438 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 23:23:18,442 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 23:23:18,450 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 23:23:18,451 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 23:23:18,452 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8036559d-f44d-4a38-944e-4deaf4ca8c25/pyspark-06d9caed-ce77-4740-a201-6b026f0b97a0
[INFO] 2018-10-30 23:23:18,453 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4515337c-990f-4ff6-a732-2eab251646bc
[INFO] 2018-10-30 23:23:18,453 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8036559d-f44d-4a38-944e-4deaf4ca8c25
[WARN] 2018-10-30 23:32:21,866 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-30 23:32:22,687 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-30 23:32:22,715 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_PARTNER_PRODUCT
[INFO] 2018-10-30 23:32:22,929 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-30 23:32:22,930 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-30 23:32:22,930 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-30 23:32:22,930 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-30 23:32:22,931 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-30 23:32:23,143 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 24849.
[INFO] 2018-10-30 23:32:23,170 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-30 23:32:23,190 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-30 23:32:23,193 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-30 23:32:23,194 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-30 23:32:23,203 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-a4fbb834-5d2d-496b-88c8-a45cf81e5e99
[INFO] 2018-10-30 23:32:23,222 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-30 23:32:23,236 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-30 23:32:23,417 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-30 23:32:23,470 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:32:23,565 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540967543565
[INFO] 2018-10-30 23:32:23,567 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-78a653ee-53c3-4db8-a62d-9dbfc83f75ee/userFiles-438fbaf4-7994-48fb-b47b-8748c1480078/etl_config.json
[INFO] 2018-10-30 23:32:23,580 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_PARTNER_PRODUCT.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_PARTNER_PRODUCT.py with timestamp 1540967543580
[INFO] 2018-10-30 23:32:23,580 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_PARTNER_PRODUCT.py to /tmp/spark-78a653ee-53c3-4db8-a62d-9dbfc83f75ee/userFiles-438fbaf4-7994-48fb-b47b-8748c1480078/JB_STG_PARTNER_PRODUCT.py
[INFO] 2018-10-30 23:32:23,584 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540967543584
[INFO] 2018-10-30 23:32:23,585 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-78a653ee-53c3-4db8-a62d-9dbfc83f75ee/userFiles-438fbaf4-7994-48fb-b47b-8748c1480078/packages.zip
[INFO] 2018-10-30 23:32:23,655 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-30 23:32:23,680 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36585.
[INFO] 2018-10-30 23:32:23,680 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:36585
[INFO] 2018-10-30 23:32:23,682 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-30 23:32:23,715 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 36585, None)
[INFO] 2018-10-30 23:32:23,719 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:36585 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 36585, None)
[INFO] 2018-10-30 23:32:23,729 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 36585, None)
[INFO] 2018-10-30 23:32:23,730 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 36585, None)
[INFO] 2018-10-30 23:32:24,045 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-30 23:32:24,046 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-30 23:32:24,586 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-30 23:32:27,696 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 210.999898 ms
[INFO] 2018-10-30 23:32:27,851 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-30 23:32:27,882 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-30 23:32:27,883 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-30 23:32:27,884 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-30 23:32:27,886 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-30 23:32:27,894 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-30 23:32:27,973 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-30 23:32:28,014 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-30 23:32:28,018 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:36585 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-30 23:32:28,021 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-30 23:32:28,037 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-30 23:32:28,038 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-30 23:32:28,091 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-30 23:32:28,114 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-30 23:32:28,120 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540967543565
[INFO] 2018-10-30 23:32:28,153 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-78a653ee-53c3-4db8-a62d-9dbfc83f75ee/userFiles-438fbaf4-7994-48fb-b47b-8748c1480078/etl_config.json
[INFO] 2018-10-30 23:32:28,159 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_STG_PARTNER_PRODUCT.py with timestamp 1540967543580
[INFO] 2018-10-30 23:32:28,160 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_STG_PARTNER_PRODUCT.py has been previously copied to /tmp/spark-78a653ee-53c3-4db8-a62d-9dbfc83f75ee/userFiles-438fbaf4-7994-48fb-b47b-8748c1480078/JB_STG_PARTNER_PRODUCT.py
[INFO] 2018-10-30 23:32:28,166 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540967543584
[INFO] 2018-10-30 23:32:28,167 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-78a653ee-53c3-4db8-a62d-9dbfc83f75ee/userFiles-438fbaf4-7994-48fb-b47b-8748c1480078/packages.zip
[INFO] 2018-10-30 23:32:29,001 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-30 23:32:29,041 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 526, boot = 419, init = 106, finish = 1
[INFO] 2018-10-30 23:32:29,063 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-30 23:32:29,081 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1002 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-30 23:32:29,085 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-30 23:32:29,093 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.175 s
[INFO] 2018-10-30 23:32:29,098 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.246839 s
[INFO] 2018-10-30 23:57:31,591 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-30 23:57:31,607 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-30 23:57:31,635 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-30 23:57:31,654 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-30 23:57:31,655 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-30 23:57:31,656 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-30 23:57:31,661 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-30 23:57:31,671 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-30 23:57:31,672 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-30 23:57:31,674 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-78a653ee-53c3-4db8-a62d-9dbfc83f75ee/pyspark-d5447303-93f2-48d4-9298-37910f363e05
[INFO] 2018-10-30 23:57:31,675 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c52a2259-f07d-44ac-b140-f1eb85822796
[INFO] 2018-10-30 23:57:31,675 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-78a653ee-53c3-4db8-a62d-9dbfc83f75ee
