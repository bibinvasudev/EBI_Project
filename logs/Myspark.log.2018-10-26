[WARN] 2018-10-26 00:00:49,658 org.apache.spark.HeartbeatReceiver logWarning - Removing executor driver with no recent heartbeats: 5059694 ms exceeds timeout 120000 ms
[ERROR] 2018-10-26 00:00:49,662 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost an executor driver (already removed): Executor heartbeat timed out after 5059694 ms
[INFO] 2018-10-26 00:00:49,663 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-26 00:00:49,665 org.apache.spark.SparkContext logWarning - Killing executors is not supported by current scheduler.
[INFO] 2018-10-26 00:00:49,666 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,669 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,670 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,671 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,672 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,675 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,675 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,675 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,676 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,676 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,676 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,677 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,677 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,677 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,678 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,678 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,678 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,679 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,679 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,679 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,680 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,680 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,680 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,681 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,681 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,682 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,682 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,683 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,683 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,683 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:00:49,683 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,684 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,685 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,685 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,685 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,686 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,686 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,686 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,686 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,687 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,687 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,688 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,688 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,688 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,688 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,689 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,689 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,689 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,689 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,690 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,690 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,690 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 00:00:49,691 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None) re-registering with master
[INFO] 2018-10-26 00:00:49,691 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,692 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-26 00:00:49,693 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 00:00:49,695 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 00:00:49,714 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 00:00:49,715 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 00:00:49,716 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 00:00:49,722 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 00:00:49,728 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 00:00:49,729 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 00:00:49,731 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b4eb1b9f-4f05-43d4-8351-a2be3cb70891
[INFO] 2018-10-26 00:00:49,731 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b4eb1b9f-4f05-43d4-8351-a2be3cb70891/pyspark-bbdba642-f849-4c34-b5e6-aa4beb197f4c
[INFO] 2018-10-26 00:00:49,732 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-9ea67e94-50b3-440a-bebd-1b34f5db47de
[WARN] 2018-10-26 00:50:59,563 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 00:51:00,444 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 00:51:00,471 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_INVOICE_TRUNCATE
[INFO] 2018-10-26 00:51:00,647 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 00:51:00,648 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 00:51:00,648 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 00:51:00,648 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 00:51:00,649 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 00:51:00,871 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 25826.
[INFO] 2018-10-26 00:51:00,901 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 00:51:00,925 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 00:51:00,929 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 00:51:00,929 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 00:51:00,940 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-ee6792b5-53f3-4d0f-847d-e31359aed48a
[INFO] 2018-10-26 00:51:00,961 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 00:51:00,978 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 00:51:01,167 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 00:51:01,218 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:51:01,321 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540261320
[INFO] 2018-10-26 00:51:01,324 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-60fd4c75-98de-4a1f-a636-09acdef5aa0b/userFiles-b4e02e59-6180-4e39-b5e6-6942c1ce58df/etl_config.json
[INFO] 2018-10-26 00:51:01,349 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_INVOICE_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_INVOICE_TRUNCATE.py with timestamp 1540540261349
[INFO] 2018-10-26 00:51:01,350 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_INVOICE_TRUNCATE.py to /tmp/spark-60fd4c75-98de-4a1f-a636-09acdef5aa0b/userFiles-b4e02e59-6180-4e39-b5e6-6942c1ce58df/JB_STG_INVOICE_TRUNCATE.py
[INFO] 2018-10-26 00:51:01,354 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540540261353
[INFO] 2018-10-26 00:51:01,354 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-60fd4c75-98de-4a1f-a636-09acdef5aa0b/userFiles-b4e02e59-6180-4e39-b5e6-6942c1ce58df/packages.zip
[INFO] 2018-10-26 00:51:01,419 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 00:51:01,442 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10117.
[INFO] 2018-10-26 00:51:01,443 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:10117
[INFO] 2018-10-26 00:51:01,445 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 00:51:01,477 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 10117, None)
[INFO] 2018-10-26 00:51:01,482 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:10117 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 10117, None)
[INFO] 2018-10-26 00:51:01,485 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 10117, None)
[INFO] 2018-10-26 00:51:01,485 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 10117, None)
[INFO] 2018-10-26 00:51:01,752 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 00:51:01,752 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 00:51:02,291 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 00:51:02,744 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 00:51:02,756 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:51:02,768 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 00:51:02,776 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 00:51:02,777 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 00:51:02,784 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 00:51:02,791 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 00:51:02,802 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 00:51:02,803 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 00:51:02,804 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-60fd4c75-98de-4a1f-a636-09acdef5aa0b
[INFO] 2018-10-26 00:51:02,805 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-60fd4c75-98de-4a1f-a636-09acdef5aa0b/pyspark-33d584d3-7dbe-4371-8317-3ab290542e51
[INFO] 2018-10-26 00:51:02,805 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b282a56e-e73e-44c1-b8b5-c4e69ff05109
[WARN] 2018-10-26 00:51:26,501 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 00:51:27,261 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 00:51:27,288 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_INVOICE
[INFO] 2018-10-26 00:51:27,497 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 00:51:27,497 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 00:51:27,498 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 00:51:27,498 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 00:51:27,505 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 00:51:27,694 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 23800.
[INFO] 2018-10-26 00:51:27,718 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 00:51:27,736 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 00:51:27,739 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 00:51:27,740 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 00:51:27,749 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-42d3477e-3a1d-4885-9b3e-fa78fa6bd7a0
[INFO] 2018-10-26 00:51:27,765 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 00:51:27,778 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 00:51:27,956 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 00:51:28,012 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:51:28,103 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540288102
[INFO] 2018-10-26 00:51:28,105 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-401bde58-8875-4d56-b145-482cab25306a/userFiles-f95cf04b-727d-4c00-abc4-01d621594908/etl_config.json
[INFO] 2018-10-26 00:51:28,116 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_INVOICE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_INVOICE.py with timestamp 1540540288116
[INFO] 2018-10-26 00:51:28,117 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_INVOICE.py to /tmp/spark-401bde58-8875-4d56-b145-482cab25306a/userFiles-f95cf04b-727d-4c00-abc4-01d621594908/JB_STG_INVOICE.py
[INFO] 2018-10-26 00:51:28,123 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540540288122
[INFO] 2018-10-26 00:51:28,123 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-401bde58-8875-4d56-b145-482cab25306a/userFiles-f95cf04b-727d-4c00-abc4-01d621594908/packages.zip
[INFO] 2018-10-26 00:51:28,189 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 00:51:28,215 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15886.
[INFO] 2018-10-26 00:51:28,216 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:15886
[INFO] 2018-10-26 00:51:28,218 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 00:51:28,254 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 15886, None)
[INFO] 2018-10-26 00:51:28,261 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:15886 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 15886, None)
[INFO] 2018-10-26 00:51:28,265 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 15886, None)
[INFO] 2018-10-26 00:51:28,266 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 15886, None)
[INFO] 2018-10-26 00:51:28,563 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 00:51:28,564 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 00:51:29,139 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 00:51:46,960 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 00:51:46,973 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:51:46,988 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 00:51:46,998 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 00:51:46,999 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 00:51:47,000 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 00:51:47,006 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 00:51:47,014 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 00:51:47,015 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 00:51:47,017 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-401bde58-8875-4d56-b145-482cab25306a/pyspark-1d74d89e-73fb-4616-90d2-4a2a1e2a12b5
[INFO] 2018-10-26 00:51:47,017 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-401bde58-8875-4d56-b145-482cab25306a
[INFO] 2018-10-26 00:51:47,018 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f139f3fd-d02d-45d3-b961-b297751fdcea
[WARN] 2018-10-26 00:52:11,740 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 00:52:12,512 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 00:52:12,534 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE_TRUNCATE
[INFO] 2018-10-26 00:52:12,692 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 00:52:12,693 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 00:52:12,693 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 00:52:12,693 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 00:52:12,694 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 00:52:12,889 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11681.
[INFO] 2018-10-26 00:52:12,913 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 00:52:12,932 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 00:52:12,935 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 00:52:12,936 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 00:52:12,944 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-ba178528-08e5-49c7-93b1-78fc6791d634
[INFO] 2018-10-26 00:52:12,962 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 00:52:12,975 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 00:52:13,149 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 00:52:13,200 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:52:13,291 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540333290
[INFO] 2018-10-26 00:52:13,292 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-3e5a21cf-19a9-49fd-869f-13950656b181/userFiles-3939604c-f0fb-437f-85b2-955751d081e5/etl_config.json
[INFO] 2018-10-26 00:52:13,308 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_TRUNCATE.py with timestamp 1540540333308
[INFO] 2018-10-26 00:52:13,309 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_TRUNCATE.py to /tmp/spark-3e5a21cf-19a9-49fd-869f-13950656b181/userFiles-3939604c-f0fb-437f-85b2-955751d081e5/JB_WORK_INVOICE_TRUNCATE.py
[INFO] 2018-10-26 00:52:13,314 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540540333314
[INFO] 2018-10-26 00:52:13,314 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-3e5a21cf-19a9-49fd-869f-13950656b181/userFiles-3939604c-f0fb-437f-85b2-955751d081e5/packages.zip
[INFO] 2018-10-26 00:52:13,377 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 00:52:13,401 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 13758.
[INFO] 2018-10-26 00:52:13,402 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:13758
[INFO] 2018-10-26 00:52:13,404 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 00:52:13,433 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 13758, None)
[INFO] 2018-10-26 00:52:13,438 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:13758 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 13758, None)
[INFO] 2018-10-26 00:52:13,441 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 13758, None)
[INFO] 2018-10-26 00:52:13,442 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 13758, None)
[INFO] 2018-10-26 00:52:13,718 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 00:52:13,719 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 00:52:14,276 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 00:52:14,838 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 00:52:14,853 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:52:14,868 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 00:52:14,877 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 00:52:14,878 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 00:52:14,889 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 00:52:14,894 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 00:52:14,902 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 00:52:14,903 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 00:52:14,904 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fe23fc88-93c9-4ab8-842a-ec1ed517772f
[INFO] 2018-10-26 00:52:14,905 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3e5a21cf-19a9-49fd-869f-13950656b181
[INFO] 2018-10-26 00:52:14,906 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3e5a21cf-19a9-49fd-869f-13950656b181/pyspark-a2ac8f9f-ce43-4883-9e3d-da3089979d40
[WARN] 2018-10-26 00:52:40,371 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 00:52:41,180 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 00:52:41,207 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE
[INFO] 2018-10-26 00:52:41,447 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 00:52:41,448 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 00:52:41,448 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 00:52:41,449 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 00:52:41,449 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 00:52:41,680 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 18641.
[INFO] 2018-10-26 00:52:41,707 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 00:52:41,726 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 00:52:41,729 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 00:52:41,730 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 00:52:41,738 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-392ba0ff-4c38-46bc-a4c0-95ad527ca5fa
[INFO] 2018-10-26 00:52:41,756 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 00:52:41,769 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 00:52:41,939 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 00:52:41,992 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:52:42,082 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540362082
[INFO] 2018-10-26 00:52:42,084 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-10fb2526-3b9c-4ed4-81b4-d65b87386e40/userFiles-35eb811a-dbda-4432-a4b7-062a820fc2f3/etl_config.json
[INFO] 2018-10-26 00:52:42,098 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE.py with timestamp 1540540362098
[INFO] 2018-10-26 00:52:42,098 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE.py to /tmp/spark-10fb2526-3b9c-4ed4-81b4-d65b87386e40/userFiles-35eb811a-dbda-4432-a4b7-062a820fc2f3/JB_WORK_INVOICE.py
[INFO] 2018-10-26 00:52:42,103 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540540362103
[INFO] 2018-10-26 00:52:42,104 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-10fb2526-3b9c-4ed4-81b4-d65b87386e40/userFiles-35eb811a-dbda-4432-a4b7-062a820fc2f3/packages.zip
[INFO] 2018-10-26 00:52:42,182 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 00:52:42,211 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 31097.
[INFO] 2018-10-26 00:52:42,212 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:31097
[INFO] 2018-10-26 00:52:42,214 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 00:52:42,248 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 31097, None)
[INFO] 2018-10-26 00:52:42,255 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:31097 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 31097, None)
[INFO] 2018-10-26 00:52:42,260 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 31097, None)
[INFO] 2018-10-26 00:52:42,261 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 31097, None)
[INFO] 2018-10-26 00:52:42,586 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 00:52:42,587 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 00:52:43,137 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 00:52:49,591 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 00:52:49,603 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:52:49,615 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 00:52:49,624 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 00:52:49,625 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 00:52:49,633 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 00:52:49,642 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 00:52:49,650 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 00:52:49,651 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 00:52:49,652 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-10fb2526-3b9c-4ed4-81b4-d65b87386e40
[INFO] 2018-10-26 00:52:49,653 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-81351f7b-47fa-47f3-a212-186f13c0db26
[INFO] 2018-10-26 00:52:49,653 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-10fb2526-3b9c-4ed4-81b4-d65b87386e40/pyspark-63f12df3-b43f-43e6-bdd3-474783fb3af7
[WARN] 2018-10-26 00:53:25,439 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 00:53:26,237 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 00:53:26,264 org.apache.spark.SparkContext logInfo - Submitted application: JB_UPD_INVOICE
[INFO] 2018-10-26 00:53:26,446 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 00:53:26,447 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 00:53:26,447 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 00:53:26,447 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 00:53:26,448 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 00:53:26,660 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 13228.
[INFO] 2018-10-26 00:53:26,688 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 00:53:26,708 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 00:53:26,711 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 00:53:26,711 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 00:53:26,721 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-39b59b95-9846-418c-802e-6267c6cbb023
[INFO] 2018-10-26 00:53:26,739 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 00:53:26,753 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 00:53:26,947 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 00:53:27,005 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:53:27,111 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540407111
[INFO] 2018-10-26 00:53:27,114 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-d69a0747-441c-4a64-8a4d-d622f8ea6209/userFiles-6e3379c9-8273-4238-8872-4ce1292c8dab/etl_config.json
[INFO] 2018-10-26 00:53:27,128 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_UPD_INVOICE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_UPD_INVOICE.py with timestamp 1540540407128
[INFO] 2018-10-26 00:53:27,129 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_UPD_INVOICE.py to /tmp/spark-d69a0747-441c-4a64-8a4d-d622f8ea6209/userFiles-6e3379c9-8273-4238-8872-4ce1292c8dab/JB_UPD_INVOICE.py
[INFO] 2018-10-26 00:53:27,133 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540540407133
[INFO] 2018-10-26 00:53:27,134 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-d69a0747-441c-4a64-8a4d-d622f8ea6209/userFiles-6e3379c9-8273-4238-8872-4ce1292c8dab/packages.zip
[INFO] 2018-10-26 00:53:27,207 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 00:53:27,231 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 16132.
[INFO] 2018-10-26 00:53:27,232 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:16132
[INFO] 2018-10-26 00:53:27,234 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 00:53:27,266 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 16132, None)
[INFO] 2018-10-26 00:53:27,272 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:16132 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 16132, None)
[INFO] 2018-10-26 00:53:27,276 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 16132, None)
[INFO] 2018-10-26 00:53:27,277 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 16132, None)
[INFO] 2018-10-26 00:53:27,550 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 00:53:27,551 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 00:53:28,106 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 00:53:30,271 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 00:53:30,283 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:53:30,296 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 00:53:30,310 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 00:53:30,310 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 00:53:30,320 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 00:53:30,326 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 00:53:30,340 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 00:53:30,341 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 00:53:30,343 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d69a0747-441c-4a64-8a4d-d622f8ea6209
[INFO] 2018-10-26 00:53:30,344 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4a29b957-8e6d-4a1d-af7e-3b0b2bce72b2
[INFO] 2018-10-26 00:53:30,345 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d69a0747-441c-4a64-8a4d-d622f8ea6209/pyspark-4d090b77-3036-4cc0-b244-96ebf14a4fd0
[WARN] 2018-10-26 00:55:53,504 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 00:55:54,290 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 00:55:54,317 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_PARTITION_TRUNCATE
[INFO] 2018-10-26 00:55:54,568 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 00:55:54,569 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 00:55:54,569 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 00:55:54,570 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 00:55:54,570 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 00:55:54,776 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 31526.
[INFO] 2018-10-26 00:55:54,802 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 00:55:54,822 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 00:55:54,825 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 00:55:54,825 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 00:55:54,834 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-75c6ae37-5283-416c-9a4c-f9bce3557dc6
[INFO] 2018-10-26 00:55:54,852 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 00:55:54,866 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 00:55:55,057 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 00:55:55,117 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:55:55,223 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540555223
[INFO] 2018-10-26 00:55:55,226 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-99272e45-136e-445d-b674-9190183487e0/userFiles-c9faa677-32f8-41f1-beb2-191eb6fb4cf6/etl_config.json
[INFO] 2018-10-26 00:55:55,241 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540540555241
[INFO] 2018-10-26 00:55:55,241 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py to /tmp/spark-99272e45-136e-445d-b674-9190183487e0/userFiles-c9faa677-32f8-41f1-beb2-191eb6fb4cf6/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 00:55:55,245 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540540555245
[INFO] 2018-10-26 00:55:55,246 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-99272e45-136e-445d-b674-9190183487e0/userFiles-c9faa677-32f8-41f1-beb2-191eb6fb4cf6/packages.zip
[INFO] 2018-10-26 00:55:55,313 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 00:55:55,338 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23927.
[INFO] 2018-10-26 00:55:55,339 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:23927
[INFO] 2018-10-26 00:55:55,340 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 00:55:55,370 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 23927, None)
[INFO] 2018-10-26 00:55:55,377 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:23927 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 23927, None)
[INFO] 2018-10-26 00:55:55,381 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 23927, None)
[INFO] 2018-10-26 00:55:55,382 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 23927, None)
[INFO] 2018-10-26 00:55:55,655 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 00:55:55,655 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 00:55:56,201 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 00:55:59,093 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 203.64702 ms
[INFO] 2018-10-26 00:55:59,245 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 00:55:59,267 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 00:55:59,268 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 00:55:59,269 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 00:55:59,272 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 00:55:59,285 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 00:55:59,374 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 00:55:59,417 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 00:55:59,421 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:23927 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 00:55:59,423 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 00:55:59,441 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 00:55:59,442 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 00:55:59,494 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 00:55:59,509 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 00:55:59,515 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540555223
[INFO] 2018-10-26 00:55:59,547 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-99272e45-136e-445d-b674-9190183487e0/userFiles-c9faa677-32f8-41f1-beb2-191eb6fb4cf6/etl_config.json
[INFO] 2018-10-26 00:55:59,557 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540540555245
[INFO] 2018-10-26 00:55:59,558 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-99272e45-136e-445d-b674-9190183487e0/userFiles-c9faa677-32f8-41f1-beb2-191eb6fb4cf6/packages.zip
[INFO] 2018-10-26 00:55:59,565 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540540555241
[INFO] 2018-10-26 00:55:59,566 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-99272e45-136e-445d-b674-9190183487e0/userFiles-c9faa677-32f8-41f1-beb2-191eb6fb4cf6/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 00:56:00,395 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 00:56:00,434 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 489, boot = 406, init = 82, finish = 1
[INFO] 2018-10-26 00:56:00,453 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 00:56:00,470 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 986 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 00:56:00,474 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 00:56:00,485 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.179 s
[INFO] 2018-10-26 00:56:00,490 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.244574 s
[INFO] 2018-10-26 00:56:00,748 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-26 00:56:00,750 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-26 00:56:00,751 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-26 00:56:00,751 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 00:56:00,751 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 00:56:00,752 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-26 00:56:00,758 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-26 00:56:00,761 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-26 00:56:00,763 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:23927 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-26 00:56:00,764 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 00:56:00,765 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 00:56:00,765 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 00:56:00,767 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 00:56:00,768 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 00:56:00,918 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 00:56:00,954 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -464, init = 506, finish = 0
[INFO] 2018-10-26 00:56:00,960 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-26 00:56:00,962 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 196 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 00:56:00,962 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 00:56:00,964 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.210 s
[INFO] 2018-10-26 00:56:00,966 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.216336 s
[INFO] 2018-10-26 00:56:01,389 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-26 00:56:01,391 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-26 00:56:01,392 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-26 00:56:01,392 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 00:56:01,392 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 00:56:01,393 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-26 00:56:01,399 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-26 00:56:01,402 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-26 00:56:01,403 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:23927 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-26 00:56:01,404 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 00:56:01,405 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 00:56:01,406 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-26 00:56:01,407 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 00:56:01,408 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-26 00:56:03,177 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 00:56:03,201 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -2202, init = 2243, finish = 0
[INFO] 2018-10-26 00:56:03,203 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1665 bytes result sent to driver
[INFO] 2018-10-26 00:56:03,206 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 1799 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 00:56:03,206 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 00:56:03,209 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 1.812 s
[INFO] 2018-10-26 00:56:03,210 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 1.819735 s
[INFO] 2018-10-26 00:56:05,280 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 00:56:05,296 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:56:05,314 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 00:56:05,333 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 00:56:05,334 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 00:56:05,348 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 00:56:05,353 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 00:56:05,366 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 00:56:05,367 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 00:56:05,369 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-99272e45-136e-445d-b674-9190183487e0
[INFO] 2018-10-26 00:56:05,370 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1e61bcb4-5d1e-432b-8e87-c2226f062e65
[INFO] 2018-10-26 00:56:05,371 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-99272e45-136e-445d-b674-9190183487e0/pyspark-d272119b-fba8-4329-b213-4dac05c381cb
[WARN] 2018-10-26 00:57:38,447 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 00:57:39,278 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 00:57:39,301 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_TO_EDW_INVOICE
[INFO] 2018-10-26 00:57:39,496 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 00:57:39,496 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 00:57:39,497 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 00:57:39,497 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 00:57:39,497 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 00:57:39,748 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 21715.
[INFO] 2018-10-26 00:57:39,777 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 00:57:39,798 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 00:57:39,802 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 00:57:39,802 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 00:57:39,812 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-64c665fc-dee8-4eda-97b9-3647c48ca996
[INFO] 2018-10-26 00:57:39,833 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 00:57:39,848 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 00:57:40,038 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 00:57:40,091 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:57:40,199 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540660198
[INFO] 2018-10-26 00:57:40,201 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-47043317-cd34-4cdc-a3d3-0081fd13b09b/userFiles-a35d6f92-f3d4-4c8e-9315-f42d56b024b7/etl_config.json
[INFO] 2018-10-26 00:57:40,214 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_EDW_INVOICE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_EDW_INVOICE.py with timestamp 1540540660214
[INFO] 2018-10-26 00:57:40,215 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_EDW_INVOICE.py to /tmp/spark-47043317-cd34-4cdc-a3d3-0081fd13b09b/userFiles-a35d6f92-f3d4-4c8e-9315-f42d56b024b7/JB_WORK_TO_EDW_INVOICE.py
[INFO] 2018-10-26 00:57:40,220 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540540660220
[INFO] 2018-10-26 00:57:40,221 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-47043317-cd34-4cdc-a3d3-0081fd13b09b/userFiles-a35d6f92-f3d4-4c8e-9315-f42d56b024b7/packages.zip
[INFO] 2018-10-26 00:57:40,288 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 00:57:40,316 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24216.
[INFO] 2018-10-26 00:57:40,317 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:24216
[INFO] 2018-10-26 00:57:40,319 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 00:57:40,352 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 24216, None)
[INFO] 2018-10-26 00:57:40,359 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:24216 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 24216, None)
[INFO] 2018-10-26 00:57:40,364 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 24216, None)
[INFO] 2018-10-26 00:57:40,365 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 24216, None)
[INFO] 2018-10-26 00:57:40,660 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 00:57:40,660 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 00:57:41,151 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 00:57:41,192 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 00:57:41,202 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 00:57:41,214 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 00:57:41,226 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 00:57:41,227 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 00:57:41,239 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 00:57:41,244 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 00:57:41,254 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 00:57:41,255 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 00:57:41,257 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-47043317-cd34-4cdc-a3d3-0081fd13b09b/pyspark-43a8aaaf-5531-459b-aeda-29ae4d5b81e4
[INFO] 2018-10-26 00:57:41,258 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-faf66d83-f48d-4590-b3c2-2175d8470811
[INFO] 2018-10-26 00:57:41,259 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-47043317-cd34-4cdc-a3d3-0081fd13b09b
[WARN] 2018-10-26 01:02:14,273 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 01:02:15,118 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 01:02:15,145 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_TO_EDW_INVOICE
[INFO] 2018-10-26 01:02:15,305 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 01:02:15,306 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 01:02:15,306 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 01:02:15,307 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 01:02:15,307 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 01:02:15,510 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 28062.
[INFO] 2018-10-26 01:02:15,534 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 01:02:15,553 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 01:02:15,556 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 01:02:15,556 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 01:02:15,564 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-9b336c09-d471-4911-bc54-663bc82568c9
[INFO] 2018-10-26 01:02:15,581 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 01:02:15,594 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 01:02:15,765 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 01:02:15,816 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:02:15,909 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540540935908
[INFO] 2018-10-26 01:02:15,911 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-1518ac5d-f8cb-4207-9b4f-207dd08b8ac0/userFiles-24da74ef-da20-4b98-b160-a3c98941d186/etl_config.json
[INFO] 2018-10-26 01:02:15,923 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_EDW_INVOICE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_EDW_INVOICE.py with timestamp 1540540935923
[INFO] 2018-10-26 01:02:15,923 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_EDW_INVOICE.py to /tmp/spark-1518ac5d-f8cb-4207-9b4f-207dd08b8ac0/userFiles-24da74ef-da20-4b98-b160-a3c98941d186/JB_WORK_TO_EDW_INVOICE.py
[INFO] 2018-10-26 01:02:15,928 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540540935928
[INFO] 2018-10-26 01:02:15,929 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-1518ac5d-f8cb-4207-9b4f-207dd08b8ac0/userFiles-24da74ef-da20-4b98-b160-a3c98941d186/packages.zip
[INFO] 2018-10-26 01:02:15,996 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 01:02:16,020 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24448.
[INFO] 2018-10-26 01:02:16,021 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:24448
[INFO] 2018-10-26 01:02:16,023 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 01:02:16,059 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 24448, None)
[INFO] 2018-10-26 01:02:16,065 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:24448 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 24448, None)
[INFO] 2018-10-26 01:02:16,070 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 24448, None)
[INFO] 2018-10-26 01:02:16,071 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 24448, None)
[INFO] 2018-10-26 01:02:16,346 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 01:02:16,347 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 01:02:16,900 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 01:03:42,774 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 01:03:42,787 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:03:42,802 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 01:03:42,818 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 01:03:42,819 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 01:03:42,821 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 01:03:42,828 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 01:03:42,838 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 01:03:42,839 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 01:03:42,840 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1518ac5d-f8cb-4207-9b4f-207dd08b8ac0/pyspark-28810fdd-ab39-4178-ad63-a8ba5d969302
[INFO] 2018-10-26 01:03:42,841 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7fd912cb-b477-432d-8564-c4cdf5872481
[INFO] 2018-10-26 01:03:42,841 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1518ac5d-f8cb-4207-9b4f-207dd08b8ac0
[WARN] 2018-10-26 01:05:56,114 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 01:05:56,873 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 01:05:56,900 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-26 01:05:57,069 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 01:05:57,070 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 01:05:57,070 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 01:05:57,070 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 01:05:57,071 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 01:05:57,286 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 16179.
[INFO] 2018-10-26 01:05:57,311 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 01:05:57,330 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 01:05:57,332 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 01:05:57,333 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 01:05:57,342 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-241316db-b2e5-49d4-ac62-32bc636796c4
[INFO] 2018-10-26 01:05:57,358 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 01:05:57,372 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 01:05:57,561 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 01:05:57,612 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:05:57,700 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540541157700
[INFO] 2018-10-26 01:05:57,702 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-2ba14783-6a70-41a1-892b-e12198d8a875/userFiles-5406e9a2-fdfa-47d6-b432-273aebf647c8/etl_config.json
[INFO] 2018-10-26 01:05:57,715 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540541157714
[INFO] 2018-10-26 01:05:57,716 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-2ba14783-6a70-41a1-892b-e12198d8a875/userFiles-5406e9a2-fdfa-47d6-b432-273aebf647c8/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 01:05:57,720 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540541157720
[INFO] 2018-10-26 01:05:57,720 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-2ba14783-6a70-41a1-892b-e12198d8a875/userFiles-5406e9a2-fdfa-47d6-b432-273aebf647c8/packages.zip
[INFO] 2018-10-26 01:05:57,793 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 01:05:57,825 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12425.
[INFO] 2018-10-26 01:05:57,826 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:12425
[INFO] 2018-10-26 01:05:57,828 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 01:05:57,863 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 12425, None)
[INFO] 2018-10-26 01:05:57,868 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:12425 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 12425, None)
[INFO] 2018-10-26 01:05:57,873 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 12425, None)
[INFO] 2018-10-26 01:05:57,874 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 12425, None)
[INFO] 2018-10-26 01:05:58,168 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 01:05:58,169 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 01:05:58,770 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 01:06:01,586 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 202.392698 ms
[INFO] 2018-10-26 01:06:01,737 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 01:06:01,757 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 01:06:01,758 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 01:06:01,758 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 01:06:01,760 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 01:06:01,768 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 01:06:01,839 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 01:06:01,869 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 01:06:01,873 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:12425 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 01:06:01,876 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 01:06:01,891 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 01:06:01,892 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 01:06:01,942 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 01:06:01,954 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 01:06:01,959 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540541157714
[INFO] 2018-10-26 01:06:01,988 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-2ba14783-6a70-41a1-892b-e12198d8a875/userFiles-5406e9a2-fdfa-47d6-b432-273aebf647c8/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 01:06:01,992 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540541157700
[INFO] 2018-10-26 01:06:01,994 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-2ba14783-6a70-41a1-892b-e12198d8a875/userFiles-5406e9a2-fdfa-47d6-b432-273aebf647c8/etl_config.json
[INFO] 2018-10-26 01:06:01,997 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540541157720
[INFO] 2018-10-26 01:06:01,999 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-2ba14783-6a70-41a1-892b-e12198d8a875/userFiles-5406e9a2-fdfa-47d6-b432-273aebf647c8/packages.zip
[INFO] 2018-10-26 01:06:02,636 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 01:06:02,672 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 471, boot = 403, init = 67, finish = 1
[INFO] 2018-10-26 01:06:02,693 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 01:06:02,710 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 780 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 01:06:02,717 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 01:06:02,723 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.931 s
[INFO] 2018-10-26 01:06:02,729 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 0.991069 s
[INFO] 2018-10-26 01:06:02,951 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-26 01:06:02,954 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-26 01:06:02,954 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-26 01:06:02,954 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 01:06:02,954 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 01:06:02,955 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-26 01:06:02,961 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-26 01:06:02,963 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-26 01:06:02,964 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:12425 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-26 01:06:02,966 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 01:06:02,967 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 01:06:02,967 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 01:06:02,968 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 01:06:02,969 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 01:06:03,121 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 01:06:03,148 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -427, init = 468, finish = 0
[INFO] 2018-10-26 01:06:03,151 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-26 01:06:03,155 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 187 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 01:06:03,156 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 01:06:03,158 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.201 s
[INFO] 2018-10-26 01:06:03,159 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.207336 s
[INFO] 2018-10-26 01:06:03,474 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-26 01:06:03,475 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-26 01:06:03,476 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-26 01:06:03,476 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 01:06:03,477 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 01:06:03,477 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-26 01:06:03,483 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-26 01:06:03,485 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-26 01:06:03,487 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:12425 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-26 01:06:03,488 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 01:06:03,489 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 01:06:03,490 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-26 01:06:03,491 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 01:06:03,492 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-26 01:06:03,779 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 01:06:03,810 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -612, init = 654, finish = 0
[INFO] 2018-10-26 01:06:03,813 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-26 01:06:03,817 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 326 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 01:06:03,818 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 01:06:03,819 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.340 s
[INFO] 2018-10-26 01:06:03,822 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.348253 s
[INFO] 2018-10-26 01:06:04,021 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 01:06:04,032 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:06:04,049 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 01:06:04,060 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 01:06:04,061 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 01:06:04,072 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 01:06:04,076 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 01:06:04,086 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 01:06:04,088 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 01:06:04,090 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2ba14783-6a70-41a1-892b-e12198d8a875
[INFO] 2018-10-26 01:06:04,091 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3347996e-c57f-46a8-bb08-5882e3823405
[INFO] 2018-10-26 01:06:04,092 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2ba14783-6a70-41a1-892b-e12198d8a875/pyspark-9d0b7022-036e-43e5-ac2f-a48e780805d7
[WARN] 2018-10-26 01:06:54,634 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 01:06:55,464 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 01:06:55,489 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_LOAD
[INFO] 2018-10-26 01:06:55,732 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 01:06:55,732 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 01:06:55,733 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 01:06:55,733 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 01:06:55,734 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 01:06:55,929 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 37068.
[INFO] 2018-10-26 01:06:55,954 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 01:06:55,972 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 01:06:55,975 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 01:06:55,976 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 01:06:55,984 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-96c2705d-248b-4281-8b0b-6fdfbd89e5ef
[INFO] 2018-10-26 01:06:56,002 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 01:06:56,015 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 01:06:56,183 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 01:06:56,231 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:06:56,322 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540541216321
[INFO] 2018-10-26 01:06:56,323 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-824143d7-af48-4b72-a7cc-2a6ebd225bbd/userFiles-9913a44e-9cd9-47a5-94d4-f193b712c9c2/etl_config.json
[INFO] 2018-10-26 01:06:56,336 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py with timestamp 1540541216336
[INFO] 2018-10-26 01:06:56,336 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py to /tmp/spark-824143d7-af48-4b72-a7cc-2a6ebd225bbd/userFiles-9913a44e-9cd9-47a5-94d4-f193b712c9c2/JB_INVOICE_TIER_LOAD.py
[INFO] 2018-10-26 01:06:56,341 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540541216340
[INFO] 2018-10-26 01:06:56,341 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-824143d7-af48-4b72-a7cc-2a6ebd225bbd/userFiles-9913a44e-9cd9-47a5-94d4-f193b712c9c2/packages.zip
[INFO] 2018-10-26 01:06:56,406 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 01:06:56,430 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11901.
[INFO] 2018-10-26 01:06:56,431 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:11901
[INFO] 2018-10-26 01:06:56,433 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 01:06:56,461 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 11901, None)
[INFO] 2018-10-26 01:06:56,465 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:11901 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 11901, None)
[INFO] 2018-10-26 01:06:56,468 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 11901, None)
[INFO] 2018-10-26 01:06:56,469 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 11901, None)
[INFO] 2018-10-26 01:06:56,764 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 01:06:56,765 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 01:06:57,292 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 01:06:57,336 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 01:06:57,350 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:06:57,362 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 01:06:57,372 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 01:06:57,372 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 01:06:57,384 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 01:06:57,392 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 01:06:57,398 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 01:06:57,398 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 01:06:57,399 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b839bc01-b2ad-402f-85ff-a6e09db62ebe
[INFO] 2018-10-26 01:06:57,401 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-824143d7-af48-4b72-a7cc-2a6ebd225bbd/pyspark-b3da107c-7f53-4a5d-83cc-3b11948e9ecd
[INFO] 2018-10-26 01:06:57,401 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-824143d7-af48-4b72-a7cc-2a6ebd225bbd
[WARN] 2018-10-26 01:09:03,283 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 01:09:04,098 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 01:09:04,126 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_LOAD
[INFO] 2018-10-26 01:09:04,268 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 01:09:04,268 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 01:09:04,268 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 01:09:04,269 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 01:09:04,269 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 01:09:04,481 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 13786.
[INFO] 2018-10-26 01:09:04,511 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 01:09:04,542 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 01:09:04,546 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 01:09:04,547 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 01:09:04,558 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-3bbd76c4-8db9-41f4-a8d4-15a721de3c56
[INFO] 2018-10-26 01:09:04,580 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 01:09:04,607 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 01:09:04,824 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 01:09:04,884 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:09:04,995 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540541344994
[INFO] 2018-10-26 01:09:04,997 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-fbfbe5ef-dfe8-4325-83b6-9cb23f32d011/userFiles-21088fa3-b550-4288-ae56-8a49f893afbd/etl_config.json
[INFO] 2018-10-26 01:09:05,014 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py with timestamp 1540541345014
[INFO] 2018-10-26 01:09:05,015 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py to /tmp/spark-fbfbe5ef-dfe8-4325-83b6-9cb23f32d011/userFiles-21088fa3-b550-4288-ae56-8a49f893afbd/JB_INVOICE_TIER_LOAD.py
[INFO] 2018-10-26 01:09:05,020 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540541345019
[INFO] 2018-10-26 01:09:05,020 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-fbfbe5ef-dfe8-4325-83b6-9cb23f32d011/userFiles-21088fa3-b550-4288-ae56-8a49f893afbd/packages.zip
[INFO] 2018-10-26 01:09:05,092 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 01:09:05,116 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30285.
[INFO] 2018-10-26 01:09:05,117 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:30285
[INFO] 2018-10-26 01:09:05,119 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 01:09:05,152 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 30285, None)
[INFO] 2018-10-26 01:09:05,159 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:30285 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 30285, None)
[INFO] 2018-10-26 01:09:05,163 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 30285, None)
[INFO] 2018-10-26 01:09:05,164 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 30285, None)
[INFO] 2018-10-26 01:09:05,448 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 01:09:05,449 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 01:09:06,013 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 01:09:08,987 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 213.649798 ms
[INFO] 2018-10-26 01:09:09,144 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 01:09:09,171 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 01:09:09,172 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 01:09:09,173 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 01:09:09,174 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 01:09:09,185 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 01:09:09,261 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 01:09:09,302 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 01:09:09,305 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:30285 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 01:09:09,307 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 01:09:09,321 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 01:09:09,322 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 01:09:09,370 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 01:09:09,384 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 01:09:09,390 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540541344994
[INFO] 2018-10-26 01:09:09,422 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-fbfbe5ef-dfe8-4325-83b6-9cb23f32d011/userFiles-21088fa3-b550-4288-ae56-8a49f893afbd/etl_config.json
[INFO] 2018-10-26 01:09:09,426 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540541345019
[INFO] 2018-10-26 01:09:09,428 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-fbfbe5ef-dfe8-4325-83b6-9cb23f32d011/userFiles-21088fa3-b550-4288-ae56-8a49f893afbd/packages.zip
[INFO] 2018-10-26 01:09:09,434 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py with timestamp 1540541345014
[INFO] 2018-10-26 01:09:09,435 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py has been previously copied to /tmp/spark-fbfbe5ef-dfe8-4325-83b6-9cb23f32d011/userFiles-21088fa3-b550-4288-ae56-8a49f893afbd/JB_INVOICE_TIER_LOAD.py
[INFO] 2018-10-26 01:09:10,096 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 01:09:10,137 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 460, boot = 374, init = 86, finish = 0
[INFO] 2018-10-26 01:09:10,161 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 01:09:10,177 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 818 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 01:09:10,181 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 01:09:10,192 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.983 s
[INFO] 2018-10-26 01:09:10,197 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.052744 s
[INFO] 2018-10-26 01:09:11,848 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 01:09:11,861 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:09:11,877 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 01:09:11,886 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 01:09:11,887 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 01:09:11,899 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 01:09:11,905 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 01:09:11,911 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 01:09:11,912 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 01:09:11,913 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e126e2b1-2e9f-4bfc-af36-1c10ad4edd86
[INFO] 2018-10-26 01:09:11,913 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fbfbe5ef-dfe8-4325-83b6-9cb23f32d011/pyspark-a117ecf8-230d-4835-a263-fe540ecfe9c4
[INFO] 2018-10-26 01:09:11,914 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fbfbe5ef-dfe8-4325-83b6-9cb23f32d011
[WARN] 2018-10-26 01:15:07,610 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2018-10-26 01:22:49,931 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 01:22:50,728 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 01:22:50,753 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS
[INFO] 2018-10-26 01:22:50,890 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 01:22:50,890 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 01:22:50,890 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 01:22:50,891 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 01:22:50,891 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 01:22:51,082 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 13589.
[INFO] 2018-10-26 01:22:51,106 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 01:22:51,126 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 01:22:51,129 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 01:22:51,130 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 01:22:51,139 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-8cec3a01-f179-40a0-95f0-676c4272e723
[INFO] 2018-10-26 01:22:51,156 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 01:22:51,170 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 01:22:51,337 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-26 01:22:51,343 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-26 01:22:51,393 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 01:22:51,486 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540542171485
[INFO] 2018-10-26 01:22:51,487 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-5e31c665-ff56-49cd-a092-7a2f18e841e8/userFiles-67e7e631-56dd-423c-9296-05652baf8fe6/etl_config.json
[INFO] 2018-10-26 01:22:51,501 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py with timestamp 1540542171501
[INFO] 2018-10-26 01:22:51,502 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py to /tmp/spark-5e31c665-ff56-49cd-a092-7a2f18e841e8/userFiles-67e7e631-56dd-423c-9296-05652baf8fe6/JB_WORK_BOOKINGS.py
[INFO] 2018-10-26 01:22:51,507 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540542171507
[INFO] 2018-10-26 01:22:51,508 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-5e31c665-ff56-49cd-a092-7a2f18e841e8/userFiles-67e7e631-56dd-423c-9296-05652baf8fe6/packages.zip
[INFO] 2018-10-26 01:22:51,578 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 01:22:51,605 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15897.
[INFO] 2018-10-26 01:22:51,606 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:15897
[INFO] 2018-10-26 01:22:51,607 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 01:22:51,642 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 15897, None)
[INFO] 2018-10-26 01:22:51,646 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:15897 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 15897, None)
[INFO] 2018-10-26 01:22:51,649 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 15897, None)
[INFO] 2018-10-26 01:22:51,649 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 15897, None)
[INFO] 2018-10-26 01:22:51,949 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 01:22:51,950 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 01:22:52,549 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 01:24:31,329 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 01:24:31,341 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 01:24:31,355 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 01:24:31,366 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 01:24:31,367 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 01:24:31,368 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 01:24:31,374 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 01:24:31,382 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 01:24:31,383 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 01:24:31,384 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5e31c665-ff56-49cd-a092-7a2f18e841e8/pyspark-7a46de98-65c0-4f0e-b50f-78af0ea0c9a5
[INFO] 2018-10-26 01:24:31,385 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5e31c665-ff56-49cd-a092-7a2f18e841e8
[INFO] 2018-10-26 01:24:31,385 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-9452d9cd-3908-4408-85cc-b301de9ef1a4
[WARN] 2018-10-26 01:24:45,278 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 01:24:46,049 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 01:24:46,076 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-26 01:24:46,268 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 01:24:46,269 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 01:24:46,269 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 01:24:46,270 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 01:24:46,270 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 01:24:46,472 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 25328.
[INFO] 2018-10-26 01:24:46,498 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 01:24:46,517 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 01:24:46,520 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 01:24:46,521 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 01:24:46,530 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-3fbf2779-e15f-449a-8fcd-832bcbc0a39d
[INFO] 2018-10-26 01:24:46,547 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 01:24:46,561 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 01:24:46,733 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 01:24:46,780 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:24:46,876 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:25328/files/etl_config.json with timestamp 1540542286876
[INFO] 2018-10-26 01:24:46,878 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-61d852fe-f05e-4098-ab70-daf140d69fbe/userFiles-3103835b-5da8-43bb-9176-1366e5728508/etl_config.json
[INFO] 2018-10-26 01:24:46,890 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:25328/files/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540542286890
[INFO] 2018-10-26 01:24:46,890 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-61d852fe-f05e-4098-ab70-daf140d69fbe/userFiles-3103835b-5da8-43bb-9176-1366e5728508/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 01:24:46,894 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:25328/files/packages.zip with timestamp 1540542286894
[INFO] 2018-10-26 01:24:46,894 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-61d852fe-f05e-4098-ab70-daf140d69fbe/userFiles-3103835b-5da8-43bb-9176-1366e5728508/packages.zip
[INFO] 2018-10-26 01:24:46,989 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-26 01:24:47,061 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 43 ms (0 ms spent in bootstraps)
[INFO] 2018-10-26 01:24:47,161 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181026012447-0011
[INFO] 2018-10-26 01:24:47,165 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,167 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/0 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,169 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/1 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,170 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/1 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,170 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/2 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,171 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/2 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,172 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/3 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,173 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/3 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,174 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/4 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,174 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11867.
[INFO] 2018-10-26 01:24:47,174 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/4 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,175 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/5 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,175 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:11867
[INFO] 2018-10-26 01:24:47,176 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/5 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,176 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/6 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,177 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/6 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,177 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/7 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,177 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 01:24:47,178 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/7 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,184 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/8 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,185 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/8 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,185 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/9 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,186 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/9 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,187 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/10 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,188 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/10 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,188 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/11 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,189 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/11 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,189 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/12 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,190 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/12 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,191 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/13 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,191 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/13 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,192 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/14 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,193 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/14 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,193 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/15 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,194 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/15 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,194 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/16 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,195 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/16 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,196 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/17 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,196 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/17 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,196 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/18 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,197 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/18 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,197 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/19 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,198 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/19 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,198 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/20 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,198 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/20 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,198 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/21 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,199 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/21 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,199 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/22 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,199 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/22 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,200 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/23 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,200 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/23 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,200 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/24 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,201 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/24 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,201 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/25 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,202 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/25 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,202 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/26 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,203 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/26 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,203 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/27 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,203 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/27 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,204 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/28 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,204 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/28 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,204 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/29 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:47,205 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/29 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:47,208 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/0 is now RUNNING
[INFO] 2018-10-26 01:24:47,208 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/1 is now RUNNING
[INFO] 2018-10-26 01:24:47,209 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/2 is now RUNNING
[INFO] 2018-10-26 01:24:47,209 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 11867, None)
[INFO] 2018-10-26 01:24:47,209 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/3 is now RUNNING
[INFO] 2018-10-26 01:24:47,210 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/4 is now RUNNING
[INFO] 2018-10-26 01:24:47,210 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/5 is now RUNNING
[INFO] 2018-10-26 01:24:47,210 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/6 is now RUNNING
[INFO] 2018-10-26 01:24:47,211 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/7 is now RUNNING
[INFO] 2018-10-26 01:24:47,211 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/8 is now RUNNING
[INFO] 2018-10-26 01:24:47,211 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/9 is now RUNNING
[INFO] 2018-10-26 01:24:47,212 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/10 is now RUNNING
[INFO] 2018-10-26 01:24:47,212 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/11 is now RUNNING
[INFO] 2018-10-26 01:24:47,212 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/12 is now RUNNING
[INFO] 2018-10-26 01:24:47,212 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/13 is now RUNNING
[INFO] 2018-10-26 01:24:47,213 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/14 is now RUNNING
[INFO] 2018-10-26 01:24:47,213 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/15 is now RUNNING
[INFO] 2018-10-26 01:24:47,213 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/16 is now RUNNING
[INFO] 2018-10-26 01:24:47,213 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/17 is now RUNNING
[INFO] 2018-10-26 01:24:47,214 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/18 is now RUNNING
[INFO] 2018-10-26 01:24:47,214 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/19 is now RUNNING
[INFO] 2018-10-26 01:24:47,214 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:11867 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 11867, None)
[INFO] 2018-10-26 01:24:47,215 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/20 is now RUNNING
[INFO] 2018-10-26 01:24:47,215 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/21 is now RUNNING
[INFO] 2018-10-26 01:24:47,217 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/22 is now RUNNING
[INFO] 2018-10-26 01:24:47,218 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/23 is now RUNNING
[INFO] 2018-10-26 01:24:47,219 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 11867, None)
[INFO] 2018-10-26 01:24:47,220 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/24 is now RUNNING
[INFO] 2018-10-26 01:24:47,220 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 11867, None)
[INFO] 2018-10-26 01:24:47,221 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/25 is now RUNNING
[INFO] 2018-10-26 01:24:47,222 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/26 is now RUNNING
[INFO] 2018-10-26 01:24:47,230 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/27 is now RUNNING
[INFO] 2018-10-26 01:24:47,241 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/28 is now RUNNING
[INFO] 2018-10-26 01:24:47,242 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/29 is now RUNNING
[INFO] 2018-10-26 01:24:47,368 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-26 01:24:47,557 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 01:24:47,558 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 01:24:47,981 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 01:24:50,980 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 265.453201 ms
[INFO] 2018-10-26 01:24:51,146 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 01:24:51,168 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 01:24:51,169 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 01:24:51,169 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 01:24:51,171 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 01:24:51,186 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 01:24:51,263 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 01:24:51,304 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 01:24:51,308 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:11867 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 01:24:51,312 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 01:24:51,327 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 01:24:51,328 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 01:24:51,827 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50526) with ID 26
[INFO] 2018-10-26 01:24:51,846 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 26, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 01:24:51,938 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50504) with ID 2
[INFO] 2018-10-26 01:24:52,005 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50506) with ID 14
[INFO] 2018-10-26 01:24:52,018 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50520) with ID 9
[INFO] 2018-10-26 01:24:52,033 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50528) with ID 16
[INFO] 2018-10-26 01:24:52,040 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50524) with ID 13
[INFO] 2018-10-26 01:24:52,044 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50522) with ID 0
[INFO] 2018-10-26 01:24:52,045 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50532) with ID 22
[INFO] 2018-10-26 01:24:52,063 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50536) with ID 27
[ERROR] 2018-10-26 01:24:52,074 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7991000932345371085
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,082 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50530) with ID 5
[INFO] 2018-10-26 01:24:52,096 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50540) with ID 8
[INFO] 2018-10-26 01:24:52,107 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50556) with ID 6
[ERROR] 2018-10-26 01:24:52,113 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5396312525296240149
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,120 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8090407081499423268
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,146 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50550) with ID 7
[ERROR] 2018-10-26 01:24:52,163 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7553951568494903570
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,168 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 13 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 01:24:52,178 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8588266430772643568
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,179 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 13 (epoch 0)
[ERROR] 2018-10-26 01:24:52,184 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 26 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,197 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 13 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,200 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 13 successfully in removeExecutor
[WARN] 2018-10-26 01:24:52,200 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,201 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 13 (epoch 0)
[ERROR] 2018-10-26 01:24:52,202 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 27 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 01:24:52,204 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7320731815761550197
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,204 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 6, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-26 01:24:52,207 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,207 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50570) with ID 10
[ERROR] 2018-10-26 01:24:52,209 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 14 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 01:24:52,212 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6572107072271671282
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,294 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7706073631023237540
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,274 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9002772198626888721
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,259 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4909044077007624470
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,247 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4987815266642748179
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,221 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7996401859735668142
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,218 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50554) with ID 1
[INFO] 2018-10-26 01:24:52,212 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 26 (epoch 1)
[INFO] 2018-10-26 01:24:52,592 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/13 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 01:24:52,485 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6760306358417445782
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,482 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7896844642532422046
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,633 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50558) with ID 19
[INFO] 2018-10-26 01:24:52,632 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 26 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,635 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/13 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,636 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 26 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,636 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 26 (epoch 1)
[INFO] 2018-10-26 01:24:52,636 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/30 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:52,636 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50546) with ID 29
[INFO] 2018-10-26 01:24:52,638 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 27 (epoch 2)
[INFO] 2018-10-26 01:24:52,638 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 27 from BlockManagerMaster.
[ERROR] 2018-10-26 01:24:52,639 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 16 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,638 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/30 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,639 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/30 is now RUNNING
[INFO] 2018-10-26 01:24:52,640 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/14 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:52,640 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/14 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,639 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 27 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,640 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50566) with ID 12
[INFO] 2018-10-26 01:24:52,641 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/31 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:52,641 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 27 (epoch 2)
[INFO] 2018-10-26 01:24:52,642 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/31 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,642 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/26 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:52,642 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 3)
[INFO] 2018-10-26 01:24:52,643 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/26 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,643 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50560) with ID 17
[INFO] 2018-10-26 01:24:52,643 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,643 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/32 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:52,644 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,644 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/32 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,644 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 3)
[INFO] 2018-10-26 01:24:52,644 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/31 is now RUNNING
[INFO] 2018-10-26 01:24:52,645 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 14 (epoch 4)
[INFO] 2018-10-26 01:24:52,645 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50576) with ID 25
[INFO] 2018-10-26 01:24:52,645 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 14 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,645 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/32 is now RUNNING
[INFO] 2018-10-26 01:24:52,646 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 14 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,646 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/16 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:52,646 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 14 (epoch 4)
[INFO] 2018-10-26 01:24:52,646 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/16 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,647 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/33 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:52,647 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 16 (epoch 5)
[INFO] 2018-10-26 01:24:52,647 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 16 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,647 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/33 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,648 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 16 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,648 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50574) with ID 28
[INFO] 2018-10-26 01:24:52,648 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 16 (epoch 5)
[INFO] 2018-10-26 01:24:52,648 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/33 is now RUNNING
[INFO] 2018-10-26 01:24:52,649 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:52,649 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/2 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,649 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50572) with ID 15
[INFO] 2018-10-26 01:24:52,649 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/34 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-26 01:24:52,650 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 9 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,651 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 9 (epoch 6)
[ERROR] 2018-10-26 01:24:52,651 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 7 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,651 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 9 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,652 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50568) with ID 18
[INFO] 2018-10-26 01:24:52,652 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 9 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,652 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/34 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,652 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 9 (epoch 6)
[INFO] 2018-10-26 01:24:52,652 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/34 is now RUNNING
[INFO] 2018-10-26 01:24:52,653 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 7 (epoch 7)
[INFO] 2018-10-26 01:24:52,653 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/27 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:52,653 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 7 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,653 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/27 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,654 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50584) with ID 23
[INFO] 2018-10-26 01:24:52,654 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 7 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,656 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/35 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:52,657 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 7 (epoch 7)
[INFO] 2018-10-26 01:24:52,657 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/35 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,657 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/35 is now RUNNING
[INFO] 2018-10-26 01:24:52,658 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50548) with ID 20
[ERROR] 2018-10-26 01:24:52,659 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 8 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,660 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 8 (epoch 8)
[INFO] 2018-10-26 01:24:52,660 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 13 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,660 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 8 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,661 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 13 requested
[INFO] 2018-10-26 01:24:52,661 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 8 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,661 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 8 (epoch 8)
[INFO] 2018-10-26 01:24:52,662 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 13
[ERROR] 2018-10-26 01:24:52,662 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,662 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 9)
[INFO] 2018-10-26 01:24:52,663 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,663 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 14 requested
[INFO] 2018-10-26 01:24:52,663 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 14
[INFO] 2018-10-26 01:24:52,663 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 14 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,663 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,663 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 26 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,663 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 26 requested
[INFO] 2018-10-26 01:24:52,664 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 26
[INFO] 2018-10-26 01:24:52,664 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 9)
[ERROR] 2018-10-26 01:24:52,664 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,665 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 10)
[INFO] 2018-10-26 01:24:52,665 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 16 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,665 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 16 requested
[INFO] 2018-10-26 01:24:52,665 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,665 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 16
[INFO] 2018-10-26 01:24:52,666 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-26 01:24:52,666 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-26 01:24:52,666 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,666 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,666 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 10)
[INFO] 2018-10-26 01:24:52,667 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 27 requested
[INFO] 2018-10-26 01:24:52,667 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 27 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,667 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 27
[ERROR] 2018-10-26 01:24:52,668 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 10 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,668 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 10 (epoch 11)
[INFO] 2018-10-26 01:24:52,668 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 10 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,669 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 10 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,670 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 10 (epoch 11)
[INFO] 2018-10-26 01:24:52,693 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50564) with ID 4
[ERROR] 2018-10-26 01:24:52,696 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 6 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 01:24:52,696 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 01:24:52,697 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 22 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,698 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 6 (epoch 12)
[INFO] 2018-10-26 01:24:52,698 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 6 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,698 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 6 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,699 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 6 (epoch 12)
[INFO] 2018-10-26 01:24:52,699 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 29, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 01:24:52,699 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 22 (epoch 13)
[ERROR] 2018-10-26 01:24:52,725 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7258788494808251710
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,721 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8512401263274927785
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,714 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50586) with ID 11
[ERROR] 2018-10-26 01:24:52,708 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8632626565514886014
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,727 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 22 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,730 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 22 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,730 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 22 (epoch 13)
[ERROR] 2018-10-26 01:24:52,732 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5577234896659170658
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,745 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5855385716287443473
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,746 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5843861081964008942
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,749 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 25 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,750 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 25 (epoch 14)
[INFO] 2018-10-26 01:24:52,750 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 25 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,751 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 25 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,751 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 25 (epoch 14)
[ERROR] 2018-10-26 01:24:52,752 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 15 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,752 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 15 (epoch 15)
[INFO] 2018-10-26 01:24:52,753 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 15 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,753 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 15 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,753 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50578) with ID 3
[INFO] 2018-10-26 01:24:52,753 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 15 (epoch 15)
[ERROR] 2018-10-26 01:24:52,761 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 12 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,762 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 12 (epoch 16)
[INFO] 2018-10-26 01:24:52,762 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 12 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,762 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 12 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,762 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 12 (epoch 16)
[ERROR] 2018-10-26 01:24:52,764 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7991237573273305322
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,854 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/9 is now EXITED (Command exited with code 1)
[WARN] 2018-10-26 01:24:52,854 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:50540
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 01:24:52,850 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:50550
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,813 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8373177582561241991
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,797 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7443789486634570820
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,795 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 01:24:52,775 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7336809848451389563
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,772 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9133786228696918964
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,772 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5396834896108071274
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:52,921 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 23 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 01:24:52,923 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5808392267349046784
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,921 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 17)
[WARN] 2018-10-26 01:24:52,920 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:50522
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 01:24:52,916 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:50520
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,916 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/9 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,925 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,926 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/36 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-26 01:24:52,925 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5966585915407777310
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,924 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50580) with ID 24
[INFO] 2018-10-26 01:24:52,927 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/36 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,928 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/7 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:52,929 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/7 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,929 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/37 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:52,930 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/37 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,930 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/8 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:52,930 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/8 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:52,927 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,931 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/38 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-26 01:24:52,929 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 19 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,931 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 17)
[INFO] 2018-10-26 01:24:52,931 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/38 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:52,932 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 23 (epoch 18)
[INFO] 2018-10-26 01:24:52,932 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/36 is now RUNNING
[INFO] 2018-10-26 01:24:52,932 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 9 requested
[INFO] 2018-10-26 01:24:52,932 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 9 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,932 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 9
[INFO] 2018-10-26 01:24:52,932 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/37 is now RUNNING
[ERROR] 2018-10-26 01:24:52,933 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 18 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:52,933 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/38 is now RUNNING
[INFO] 2018-10-26 01:24:52,933 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 23 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,934 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 7 requested
[INFO] 2018-10-26 01:24:52,934 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 23 successfully in removeExecutor
[INFO] 2018-10-26 01:24:52,933 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/0 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 01:24:53,003 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5586694877975761115
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:52,934 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 23 (epoch 18)
[INFO] 2018-10-26 01:24:52,934 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 7 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:52,934 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 7
[INFO] 2018-10-26 01:24:53,076 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 19 (epoch 19)
[INFO] 2018-10-26 01:24:53,077 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 8 requested
[INFO] 2018-10-26 01:24:53,075 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/0 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,077 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 8
[INFO] 2018-10-26 01:24:53,077 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 8 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,078 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/39 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,078 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 19 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,078 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/39 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,078 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 19 successfully in removeExecutor
[ERROR] 2018-10-26 01:24:53,079 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 29 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,079 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/39 is now RUNNING
[WARN] 2018-10-26 01:24:53,080 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 29): ExecutorLostFailure (executor 29 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,080 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/5 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,079 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 19 (epoch 19)
[INFO] 2018-10-26 01:24:53,080 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/5 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,080 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 18 (epoch 20)
[INFO] 2018-10-26 01:24:53,081 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/40 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,081 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 18 from BlockManagerMaster.
[ERROR] 2018-10-26 01:24:53,081 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 11 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,081 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/40 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,081 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 18 successfully in removeExecutor
[INFO] 2018-10-26 01:24:53,082 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/40 is now RUNNING
[ERROR] 2018-10-26 01:24:53,082 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,082 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 18 (epoch 20)
[INFO] 2018-10-26 01:24:53,082 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/10 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,083 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/10 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,083 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/41 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,083 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 29 (epoch 21)
[INFO] 2018-10-26 01:24:53,083 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/41 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,083 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 29 from BlockManagerMaster.
[ERROR] 2018-10-26 01:24:53,083 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 20 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,084 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/41 is now RUNNING
[INFO] 2018-10-26 01:24:53,084 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 29 successfully in removeExecutor
[ERROR] 2018-10-26 01:24:53,084 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 17 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,084 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 29 (epoch 21)
[ERROR] 2018-10-26 01:24:53,085 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 28 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,085 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 11 (epoch 22)
[ERROR] 2018-10-26 01:24:53,085 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,085 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,086 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 11 successfully in removeExecutor
[INFO] 2018-10-26 01:24:53,086 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 24, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 01:24:53,086 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 11 (epoch 22)
[INFO] 2018-10-26 01:24:53,087 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-26 01:24:53,087 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,087 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-26 01:24:53,087 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,087 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 23)
[INFO] 2018-10-26 01:24:53,088 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,088 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 5 requested
[INFO] 2018-10-26 01:24:53,088 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 5
[INFO] 2018-10-26 01:24:53,088 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-26 01:24:53,089 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 10 requested
[INFO] 2018-10-26 01:24:53,089 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 23)
[INFO] 2018-10-26 01:24:53,089 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 10 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,089 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 10
[INFO] 2018-10-26 01:24:53,090 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 20 (epoch 24)
[INFO] 2018-10-26 01:24:53,090 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 20 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,090 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 20 successfully in removeExecutor
[INFO] 2018-10-26 01:24:53,090 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 20 (epoch 24)
[INFO] 2018-10-26 01:24:53,091 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 17 (epoch 25)
[INFO] 2018-10-26 01:24:53,091 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 17 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,092 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 17 successfully in removeExecutor
[INFO] 2018-10-26 01:24:53,092 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 17 (epoch 25)
[INFO] 2018-10-26 01:24:53,093 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 28 (epoch 26)
[INFO] 2018-10-26 01:24:53,093 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 28 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,093 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 28 successfully in removeExecutor
[INFO] 2018-10-26 01:24:53,094 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 28 (epoch 26)
[INFO] 2018-10-26 01:24:53,094 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 27)
[INFO] 2018-10-26 01:24:53,094 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,095 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-26 01:24:53,095 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 27)
[ERROR] 2018-10-26 01:24:53,095 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 24 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,096 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/22 is now EXITED (Command exited with code 1)
[WARN] 2018-10-26 01:24:53,096 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 01:24:53,096 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/22 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,096 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/42 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-26 01:24:53,097 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-26 01:24:53,097 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/42 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,099 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 01:24:53,099 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 22 requested
[INFO] 2018-10-26 01:24:53,100 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 22
[INFO] 2018-10-26 01:24:53,099 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 22 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,101 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/42 is now RUNNING
[INFO] 2018-10-26 01:24:53,102 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-26 01:24:53,104 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 1.894 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-26 01:24:53,108 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 24 (epoch 28)
[INFO] 2018-10-26 01:24:53,109 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 24 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,110 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.962314 s
[INFO] 2018-10-26 01:24:53,110 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 24 successfully in removeExecutor
[INFO] 2018-10-26 01:24:53,111 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 24 (epoch 28)
[INFO] 2018-10-26 01:24:53,115 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:50634) with ID 21
[INFO] 2018-10-26 01:24:53,127 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/25 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,127 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/25 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,127 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 25 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,127 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 25 requested
[INFO] 2018-10-26 01:24:53,128 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 25
[INFO] 2018-10-26 01:24:53,128 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/43 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,128 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/43 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,128 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/43 is now RUNNING
[INFO] 2018-10-26 01:24:53,139 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/15 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,139 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/15 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,139 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 15 requested
[INFO] 2018-10-26 01:24:53,140 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 15
[INFO] 2018-10-26 01:24:53,139 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 15 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,139 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/44 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,140 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/44 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,143 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/6 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,144 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/6 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,144 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/45 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,144 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 6 requested
[INFO] 2018-10-26 01:24:53,144 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 6 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,144 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/45 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,144 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 6
[INFO] 2018-10-26 01:24:53,152 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/44 is now RUNNING
[INFO] 2018-10-26 01:24:53,158 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/45 is now RUNNING
[WARN] 2018-10-26 01:24:53,162 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:50584
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 01:24:53,167 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6990569550393257677
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:53,169 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/23 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,169 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 01:24:53,169 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/23 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,170 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/46 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,170 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 23 requested
[INFO] 2018-10-26 01:24:53,170 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 23 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,170 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 23
[INFO] 2018-10-26 01:24:53,170 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/46 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,174 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/46 is now RUNNING
[INFO] 2018-10-26 01:24:53,175 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/12 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,175 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/12 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,175 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 12 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,175 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 12 requested
[INFO] 2018-10-26 01:24:53,175 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 12
[WARN] 2018-10-26 01:24:53,176 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:50558
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:53,176 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/47 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,176 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/47 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,179 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:24:53,182 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/47 is now RUNNING
[INFO] 2018-10-26 01:24:53,183 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,183 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/1 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,184 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,184 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-26 01:24:53,184 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-26 01:24:53,185 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/48 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,185 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/48 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,185 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/19 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 01:24:53,186 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026012447-0011/19 removed: Command exited with code 1
[INFO] 2018-10-26 01:24:53,186 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026012447-0011/49 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 01:24:53,186 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 19 requested
[INFO] 2018-10-26 01:24:53,186 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 19 from BlockManagerMaster.
[INFO] 2018-10-26 01:24:53,186 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026012447-0011/49 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 01:24:53,186 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 19
[INFO] 2018-10-26 01:24:53,186 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-26 01:24:53,188 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-26 01:24:53,190 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/48 is now RUNNING
[INFO] 2018-10-26 01:24:53,191 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026012447-0011/49 is now RUNNING
[ERROR] 2018-10-26 01:24:53,198 org.apache.spark.network.server.TransportRequestHandler processOneWayMessage - Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 01:24:53,201 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 01:24:53,303 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 01:24:53,304 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 01:24:53,305 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 01:24:53,307 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 01:24:53,313 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 01:24:53,313 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 01:24:53,314 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d198ec27-70e9-40c9-a504-646a8ed59094
[INFO] 2018-10-26 01:24:53,315 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-61d852fe-f05e-4098-ab70-daf140d69fbe/pyspark-c17d5210-1f79-4cc5-af57-9f685b5fa9f8
[INFO] 2018-10-26 01:24:53,315 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-61d852fe-f05e-4098-ab70-daf140d69fbe
[WARN] 2018-10-26 01:34:48,976 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 01:34:49,768 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 01:34:49,793 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKING_DAY_FLAG_TEMP_TRUNCATE
[INFO] 2018-10-26 01:34:50,075 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 01:34:50,075 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 01:34:50,075 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 01:34:50,076 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 01:34:50,076 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 01:34:50,307 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 32613.
[INFO] 2018-10-26 01:34:50,336 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 01:34:50,354 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 01:34:50,357 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 01:34:50,358 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 01:34:50,367 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-4208159c-23b4-434b-ba85-c9b28e4a96e5
[INFO] 2018-10-26 01:34:50,383 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 01:34:50,397 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 01:34:50,571 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 01:34:50,622 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:34:50,713 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540542890712
[INFO] 2018-10-26 01:34:50,715 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-1c976a6a-850c-4484-a6c2-3e5b1c99d636/userFiles-a8ac9aaf-7e8a-4c3f-918c-59a66546c8af/etl_config.json
[INFO] 2018-10-26 01:34:50,728 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKING_DAY_FLAG_TEMP_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKING_DAY_FLAG_TEMP_TRUNCATE.py with timestamp 1540542890728
[INFO] 2018-10-26 01:34:50,729 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKING_DAY_FLAG_TEMP_TRUNCATE.py to /tmp/spark-1c976a6a-850c-4484-a6c2-3e5b1c99d636/userFiles-a8ac9aaf-7e8a-4c3f-918c-59a66546c8af/JB_BOOKING_DAY_FLAG_TEMP_TRUNCATE.py
[INFO] 2018-10-26 01:34:50,733 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540542890733
[INFO] 2018-10-26 01:34:50,733 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-1c976a6a-850c-4484-a6c2-3e5b1c99d636/userFiles-a8ac9aaf-7e8a-4c3f-918c-59a66546c8af/packages.zip
[INFO] 2018-10-26 01:34:50,803 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 01:34:50,829 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27344.
[INFO] 2018-10-26 01:34:50,831 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:27344
[INFO] 2018-10-26 01:34:50,832 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 01:34:50,866 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 27344, None)
[INFO] 2018-10-26 01:34:50,873 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:27344 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 27344, None)
[INFO] 2018-10-26 01:34:50,877 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 27344, None)
[INFO] 2018-10-26 01:34:50,878 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 27344, None)
[INFO] 2018-10-26 01:34:51,161 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 01:34:51,162 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 01:34:51,762 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 01:34:52,171 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 01:34:52,183 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:34:52,195 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 01:34:52,207 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 01:34:52,208 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 01:34:52,218 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 01:34:52,224 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 01:34:52,232 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 01:34:52,233 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 01:34:52,234 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1c976a6a-850c-4484-a6c2-3e5b1c99d636/pyspark-d9d7d7ae-0cc6-4b7c-a539-bc8932caa6ea
[INFO] 2018-10-26 01:34:52,234 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1c976a6a-850c-4484-a6c2-3e5b1c99d636
[INFO] 2018-10-26 01:34:52,235 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ddd66090-efbe-4a85-98fa-3390b6183865
[WARN] 2018-10-26 01:38:39,530 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 01:38:40,296 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 01:38:40,322 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_REJ_TRUNCATE
[INFO] 2018-10-26 01:38:40,547 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 01:38:40,548 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 01:38:40,548 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 01:38:40,549 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 01:38:40,549 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 01:38:40,776 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 21501.
[INFO] 2018-10-26 01:38:40,800 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 01:38:40,819 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 01:38:40,822 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 01:38:40,822 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 01:38:40,831 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-cd0af9cd-a39b-442a-9932-0029c288a40f
[INFO] 2018-10-26 01:38:40,848 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 01:38:40,861 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 01:38:41,030 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 01:38:41,090 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:38:41,180 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540543121179
[INFO] 2018-10-26 01:38:41,182 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-f8d67987-47ec-4a4a-a360-854045bbd0f7/userFiles-5b7cd439-9c35-41f6-9bf4-e4acc3222ad7/etl_config.json
[INFO] 2018-10-26 01:38:41,196 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_REJ_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_REJ_TRUNCATE.py with timestamp 1540543121196
[INFO] 2018-10-26 01:38:41,197 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_REJ_TRUNCATE.py to /tmp/spark-f8d67987-47ec-4a4a-a360-854045bbd0f7/userFiles-5b7cd439-9c35-41f6-9bf4-e4acc3222ad7/JB_BOOKINGS_REJ_TRUNCATE.py
[INFO] 2018-10-26 01:38:41,201 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540543121201
[INFO] 2018-10-26 01:38:41,202 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-f8d67987-47ec-4a4a-a360-854045bbd0f7/userFiles-5b7cd439-9c35-41f6-9bf4-e4acc3222ad7/packages.zip
[INFO] 2018-10-26 01:38:41,274 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 01:38:41,301 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11702.
[INFO] 2018-10-26 01:38:41,302 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:11702
[INFO] 2018-10-26 01:38:41,304 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 01:38:41,341 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 11702, None)
[INFO] 2018-10-26 01:38:41,349 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:11702 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 11702, None)
[INFO] 2018-10-26 01:38:41,352 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 11702, None)
[INFO] 2018-10-26 01:38:41,353 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 11702, None)
[INFO] 2018-10-26 01:38:41,625 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 01:38:41,626 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 01:38:42,176 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 01:38:42,478 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 01:38:42,493 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 01:38:42,505 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 01:38:42,516 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 01:38:42,517 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 01:38:42,528 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 01:38:42,536 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 01:38:42,549 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 01:38:42,550 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 01:38:42,551 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5a521538-10e4-42d2-8eaf-4c77e7964507
[INFO] 2018-10-26 01:38:42,552 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f8d67987-47ec-4a4a-a360-854045bbd0f7/pyspark-8e28a9ac-2cf8-409f-9fa9-ff87491fdca4
[INFO] 2018-10-26 01:38:42,552 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f8d67987-47ec-4a4a-a360-854045bbd0f7
[WARN] 2018-10-26 02:17:19,457 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 02:17:20,273 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 02:17:20,312 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-26 02:17:20,589 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 02:17:20,589 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 02:17:20,590 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 02:17:20,590 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 02:17:20,590 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 02:17:20,789 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 13270.
[INFO] 2018-10-26 02:17:20,813 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 02:17:20,833 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 02:17:20,836 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 02:17:20,837 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 02:17:20,847 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-ab8012bd-48df-47d1-8343-382753a9d00b
[INFO] 2018-10-26 02:17:20,865 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 02:17:20,879 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 02:17:21,053 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 02:17:21,102 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 02:17:21,197 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:13270/files/etl_config.json with timestamp 1540545441197
[INFO] 2018-10-26 02:17:21,199 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-6b0ef17d-a10c-4795-8ebf-80491fc58276/userFiles-379046ec-2d60-4b6d-9197-60401d8d12d8/etl_config.json
[INFO] 2018-10-26 02:17:21,212 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:13270/files/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540545441212
[INFO] 2018-10-26 02:17:21,213 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-6b0ef17d-a10c-4795-8ebf-80491fc58276/userFiles-379046ec-2d60-4b6d-9197-60401d8d12d8/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 02:17:21,218 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:13270/files/packages.zip with timestamp 1540545441218
[INFO] 2018-10-26 02:17:21,218 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-6b0ef17d-a10c-4795-8ebf-80491fc58276/userFiles-379046ec-2d60-4b6d-9197-60401d8d12d8/packages.zip
[INFO] 2018-10-26 02:17:21,302 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-26 02:17:21,369 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 42 ms (0 ms spent in bootstraps)
[INFO] 2018-10-26 02:17:21,465 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181026021721-0019
[INFO] 2018-10-26 02:17:21,467 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,468 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/0 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,469 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/1 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,469 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/1 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,470 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/2 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,470 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/2 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,470 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/3 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,471 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/3 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,471 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/4 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,472 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/4 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,472 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/5 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,472 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/5 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,473 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/6 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,473 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/6 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,473 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/7 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,474 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/7 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,474 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/8 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,475 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/8 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,475 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/9 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,476 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/9 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,476 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/10 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,476 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 20049.
[INFO] 2018-10-26 02:17:21,477 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:20049
[INFO] 2018-10-26 02:17:21,476 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/10 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,478 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/11 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,478 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/11 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,479 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/12 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,479 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/12 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,479 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/13 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,481 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 02:17:21,481 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/13 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,483 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/14 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,484 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/14 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,485 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/15 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,485 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/15 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,485 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/16 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,487 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/16 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,488 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/17 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,488 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/17 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,489 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/18 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,490 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/18 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,490 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/19 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,492 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/19 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,493 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/20 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,494 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/20 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,495 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/21 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,496 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/21 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,496 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/22 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,496 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/22 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,497 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/23 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,497 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/23 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,499 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/24 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,499 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/24 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,499 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/25 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,501 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/25 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,501 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/26 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,501 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/26 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,502 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/27 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,502 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/27 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,502 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/28 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,503 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/28 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,503 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/29 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:21,504 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/29 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:21,506 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/0 is now RUNNING
[INFO] 2018-10-26 02:17:21,507 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/1 is now RUNNING
[INFO] 2018-10-26 02:17:21,507 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/2 is now RUNNING
[INFO] 2018-10-26 02:17:21,507 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/3 is now RUNNING
[INFO] 2018-10-26 02:17:21,508 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/4 is now RUNNING
[INFO] 2018-10-26 02:17:21,508 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/5 is now RUNNING
[INFO] 2018-10-26 02:17:21,509 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/6 is now RUNNING
[INFO] 2018-10-26 02:17:21,509 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/7 is now RUNNING
[INFO] 2018-10-26 02:17:21,510 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/8 is now RUNNING
[INFO] 2018-10-26 02:17:21,511 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/9 is now RUNNING
[INFO] 2018-10-26 02:17:21,521 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 20049, None)
[INFO] 2018-10-26 02:17:21,522 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/10 is now RUNNING
[INFO] 2018-10-26 02:17:21,523 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/11 is now RUNNING
[INFO] 2018-10-26 02:17:21,525 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/12 is now RUNNING
[INFO] 2018-10-26 02:17:21,526 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:20049 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 20049, None)
[INFO] 2018-10-26 02:17:21,528 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/13 is now RUNNING
[INFO] 2018-10-26 02:17:21,530 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/14 is now RUNNING
[INFO] 2018-10-26 02:17:21,532 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 20049, None)
[INFO] 2018-10-26 02:17:21,532 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/15 is now RUNNING
[INFO] 2018-10-26 02:17:21,533 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 20049, None)
[INFO] 2018-10-26 02:17:21,534 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/16 is now RUNNING
[INFO] 2018-10-26 02:17:21,536 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/17 is now RUNNING
[INFO] 2018-10-26 02:17:21,537 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/18 is now RUNNING
[INFO] 2018-10-26 02:17:21,538 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/19 is now RUNNING
[INFO] 2018-10-26 02:17:21,540 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/20 is now RUNNING
[INFO] 2018-10-26 02:17:21,541 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/21 is now RUNNING
[INFO] 2018-10-26 02:17:21,542 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/22 is now RUNNING
[INFO] 2018-10-26 02:17:21,543 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/23 is now RUNNING
[INFO] 2018-10-26 02:17:21,544 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/24 is now RUNNING
[INFO] 2018-10-26 02:17:21,544 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/25 is now RUNNING
[INFO] 2018-10-26 02:17:21,545 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/26 is now RUNNING
[INFO] 2018-10-26 02:17:21,546 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/27 is now RUNNING
[INFO] 2018-10-26 02:17:21,546 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/28 is now RUNNING
[INFO] 2018-10-26 02:17:21,553 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/29 is now RUNNING
[INFO] 2018-10-26 02:17:21,716 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-26 02:17:22,003 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 02:17:22,004 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 02:17:22,479 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 02:17:25,667 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12994) with ID 8
[INFO] 2018-10-26 02:17:25,718 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13002) with ID 1
[INFO] 2018-10-26 02:17:25,772 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13006) with ID 12
[INFO] 2018-10-26 02:17:25,774 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13012) with ID 9
[INFO] 2018-10-26 02:17:25,781 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13018) with ID 3
[INFO] 2018-10-26 02:17:25,813 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13034) with ID 17
[INFO] 2018-10-26 02:17:25,847 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 252.574076 ms
[INFO] 2018-10-26 02:17:25,853 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13000) with ID 14
[INFO] 2018-10-26 02:17:25,861 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13026) with ID 2
[ERROR] 2018-10-26 02:17:25,882 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8138099497183298109
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:25,885 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13024) with ID 16
[ERROR] 2018-10-26 02:17:25,882 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8301079093151497929
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:25,892 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8887804633348252184
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:25,903 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4946225158716539072
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:25,903 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13028) with ID 19
[ERROR] 2018-10-26 02:17:25,934 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:17:25,939 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 12 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:25,940 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 0)
[ERROR] 2018-10-26 02:17:25,942 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:17:25,943 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 9 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:25,949 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:25,950 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-26 02:17:25,952 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 0)
[INFO] 2018-10-26 02:17:25,954 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 12 (epoch 1)
[INFO] 2018-10-26 02:17:25,954 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 12 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:25,955 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 12 successfully in removeExecutor
[INFO] 2018-10-26 02:17:25,955 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 12 (epoch 1)
[INFO] 2018-10-26 02:17:25,956 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 2)
[INFO] 2018-10-26 02:17:25,956 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:25,956 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-26 02:17:25,957 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 2)
[INFO] 2018-10-26 02:17:25,957 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 9 (epoch 3)
[INFO] 2018-10-26 02:17:25,957 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 9 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:25,958 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 9 successfully in removeExecutor
[INFO] 2018-10-26 02:17:25,958 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 9 (epoch 3)
[INFO] 2018-10-26 02:17:25,958 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13036) with ID 25
[ERROR] 2018-10-26 02:17:25,961 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6946374264164734104
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:25,978 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 17 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:25,978 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 17 (epoch 4)
[INFO] 2018-10-26 02:17:25,979 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 17 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:25,979 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 17 successfully in removeExecutor
[INFO] 2018-10-26 02:17:25,980 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 17 (epoch 4)
[INFO] 2018-10-26 02:17:25,986 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13016) with ID 7
[ERROR] 2018-10-26 02:17:25,993 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6408690464839088891
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,001 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5410739369969655820
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,004 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8135976997555792443
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,007 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[ERROR] 2018-10-26 02:17:26,009 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5655103236926073596
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,010 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 8 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,011 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 8 (epoch 5)
[INFO] 2018-10-26 02:17:26,012 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 8 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,013 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 8 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,013 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 8 (epoch 5)
[ERROR] 2018-10-26 02:17:26,019 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 16 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,031 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[ERROR] 2018-10-26 02:17:26,031 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 14 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,032 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 02:17:26,032 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 02:17:26,034 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 02:17:26,036 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13030) with ID 23
[INFO] 2018-10-26 02:17:26,039 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13052) with ID 22
[INFO] 2018-10-26 02:17:26,041 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[ERROR] 2018-10-26 02:17:26,053 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 19 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,058 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13032) with ID 4
[INFO] 2018-10-26 02:17:26,075 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13058) with ID 15
[ERROR] 2018-10-26 02:17:26,076 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8234811043345815694
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,094 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4825858147935228312
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,098 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13044) with ID 29
[ERROR] 2018-10-26 02:17:26,103 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7026611467418575686
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,117 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[ERROR] 2018-10-26 02:17:26,119 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 25 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:17:26,120 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 7 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:17:26,126 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6982781862612728629
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,143 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8371478033331731532
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,143 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:17:26,145 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:17:26,145 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6987453243771106961
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,153 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[ERROR] 2018-10-26 02:17:26,155 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5489608347934257902
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,155 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:20049 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 02:17:26,159 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 02:17:26,163 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13062) with ID 24
[INFO] 2018-10-26 02:17:26,165 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13060) with ID 28
[INFO] 2018-10-26 02:17:26,167 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13050) with ID 11
[INFO] 2018-10-26 02:17:26,169 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13066) with ID 21
[INFO] 2018-10-26 02:17:26,171 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 02:17:26,173 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[ERROR] 2018-10-26 02:17:26,191 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 22 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,191 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 16 (epoch 6)
[INFO] 2018-10-26 02:17:26,192 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 16 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,192 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 16 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,192 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 16 (epoch 6)
[INFO] 2018-10-26 02:17:26,193 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 14 (epoch 7)
[INFO] 2018-10-26 02:17:26,193 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 14 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,193 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 14 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,193 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 14 (epoch 7)
[INFO] 2018-10-26 02:17:26,194 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 19 (epoch 8)
[INFO] 2018-10-26 02:17:26,194 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 19 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,194 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 19 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,194 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 19 (epoch 8)
[INFO] 2018-10-26 02:17:26,194 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 25 (epoch 9)
[INFO] 2018-10-26 02:17:26,195 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 25 from BlockManagerMaster.
[ERROR] 2018-10-26 02:17:26,195 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 23 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,195 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 25 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,195 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 25 (epoch 9)
[INFO] 2018-10-26 02:17:26,195 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13056) with ID 20
[INFO] 2018-10-26 02:17:26,196 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 7 (epoch 10)
[INFO] 2018-10-26 02:17:26,196 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 7 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,196 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 7 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,196 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 7 (epoch 10)
[INFO] 2018-10-26 02:17:26,196 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 11)
[INFO] 2018-10-26 02:17:26,197 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,198 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,198 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 11)
[INFO] 2018-10-26 02:17:26,199 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 12)
[INFO] 2018-10-26 02:17:26,199 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,200 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,201 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 12)
[INFO] 2018-10-26 02:17:26,201 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 22 (epoch 13)
[INFO] 2018-10-26 02:17:26,201 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 22 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,201 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 22 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,201 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 22 (epoch 13)
[INFO] 2018-10-26 02:17:26,202 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 23 (epoch 14)
[INFO] 2018-10-26 02:17:26,202 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 23 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,202 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 23 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,202 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 23 (epoch 14)
[ERROR] 2018-10-26 02:17:26,217 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6985312223050973844
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,218 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 24, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-26 02:17:26,223 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6685109717518266824
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,244 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 15 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,245 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13070) with ID 18
[INFO] 2018-10-26 02:17:26,245 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 15 (epoch 15)
[INFO] 2018-10-26 02:17:26,245 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 15 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,246 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13072) with ID 5
[INFO] 2018-10-26 02:17:26,246 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 15 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,246 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 15 (epoch 15)
[ERROR] 2018-10-26 02:17:26,246 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5321157696563745393
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,249 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7308821179470019664
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 02:17:26,550 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:13058
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,314 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/3 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 02:17:26,305 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5682220630199243571
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,282 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5598153465062631749
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,257 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7797125618573270574
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,252 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 29 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:17:26,584 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8986545840650794827
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,581 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/3 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,585 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 29 (epoch 16)
[INFO] 2018-10-26 02:17:26,585 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13080) with ID 6
[INFO] 2018-10-26 02:17:26,586 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 29 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,586 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/30 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,586 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 29 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,587 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/30 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,587 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 29 (epoch 16)
[ERROR] 2018-10-26 02:17:26,587 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 24 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,587 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/30 is now RUNNING
[INFO] 2018-10-26 02:17:26,587 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/9 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,587 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/9 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,588 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/31 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,588 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/31 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,588 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/31 is now RUNNING
[INFO] 2018-10-26 02:17:26,588 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/12 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,589 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/12 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,589 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/32 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,589 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/32 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,589 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/32 is now RUNNING
[INFO] 2018-10-26 02:17:26,589 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/17 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,589 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/17 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,590 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/33 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,590 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/33 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,590 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/33 is now RUNNING
[INFO] 2018-10-26 02:17:26,591 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/16 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,591 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/16 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,591 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/34 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,591 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/34 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,592 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/8 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,592 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/8 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,592 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/35 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,592 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/35 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,594 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/34 is now RUNNING
[INFO] 2018-10-26 02:17:26,595 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/35 is now RUNNING
[INFO] 2018-10-26 02:17:26,595 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,595 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/1 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,596 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/36 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,596 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/36 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[WARN] 2018-10-26 02:17:26,596 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,597 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/36 is now RUNNING
[INFO] 2018-10-26 02:17:26,597 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/19 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,598 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/19 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,598 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/37 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,598 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/37 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/37 is now RUNNING
[INFO] 2018-10-26 02:17:26,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/14 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,599 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/14 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,599 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 21, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:17:26,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/38 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,600 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/38 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/38 is now RUNNING
[INFO] 2018-10-26 02:17:26,600 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13068) with ID 13
[INFO] 2018-10-26 02:17:26,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/25 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,600 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/25 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,601 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/39 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,601 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13078) with ID 0
[INFO] 2018-10-26 02:17:26,601 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/39 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,601 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/39 is now RUNNING
[INFO] 2018-10-26 02:17:26,601 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,601 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/4 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,602 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13110) with ID 26
[INFO] 2018-10-26 02:17:26,602 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/40 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,602 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/40 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,603 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/7 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 02:17:26,603 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 11 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,603 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/7 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,604 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,604 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/41 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,604 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-26 02:17:26,605 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-26 02:17:26,605 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 24 (epoch 17)
[INFO] 2018-10-26 02:17:26,605 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/41 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,606 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/40 is now RUNNING
[INFO] 2018-10-26 02:17:26,606 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/41 is now RUNNING
[INFO] 2018-10-26 02:17:26,606 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/22 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,606 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/22 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,606 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/42 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,607 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/42 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,607 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/42 is now RUNNING
[INFO] 2018-10-26 02:17:26,605 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 9 requested
[INFO] 2018-10-26 02:17:26,605 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 9 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,607 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 9
[INFO] 2018-10-26 02:17:26,607 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/23 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,607 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 12 requested
[INFO] 2018-10-26 02:17:26,607 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 24 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,608 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 12
[INFO] 2018-10-26 02:17:26,607 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/23 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,608 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 17 requested
[INFO] 2018-10-26 02:17:26,608 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 17
[INFO] 2018-10-26 02:17:26,608 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 24 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,608 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 12 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,609 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 24 (epoch 17)
[INFO] 2018-10-26 02:17:26,609 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 16 requested
[INFO] 2018-10-26 02:17:26,608 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/43 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,609 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 11 (epoch 18)
[INFO] 2018-10-26 02:17:26,609 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 16
[INFO] 2018-10-26 02:17:26,609 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 17 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,610 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 8 requested
[INFO] 2018-10-26 02:17:26,609 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/43 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,610 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 8
[INFO] 2018-10-26 02:17:26,610 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 16 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,610 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-26 02:17:26,610 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/43 is now RUNNING
[INFO] 2018-10-26 02:17:26,610 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-26 02:17:26,610 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[ERROR] 2018-10-26 02:17:26,611 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 20 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,610 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,611 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 8 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,611 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 11 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,611 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[ERROR] 2018-10-26 02:17:26,611 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,611 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/2 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,611 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 11 (epoch 18)
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 19 requested
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 20 (epoch 19)
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 19 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/44 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 20 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 19
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 20 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/44 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,612 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 20 (epoch 19)
[INFO] 2018-10-26 02:17:26,613 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/44 is now RUNNING
[INFO] 2018-10-26 02:17:26,613 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 14 requested
[INFO] 2018-10-26 02:17:26,613 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 14 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,613 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 20)
[INFO] 2018-10-26 02:17:26,613 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 14
[INFO] 2018-10-26 02:17:26,613 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/15 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 02:17:26,613 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 28 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,613 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,613 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/15 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 25 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 25 requested
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 20)
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/45 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 28 (epoch 21)
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 25
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/45 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,614 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 28 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,615 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/45 is now RUNNING
[INFO] 2018-10-26 02:17:26,615 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 28 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,615 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-26 02:17:26,615 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 28 (epoch 21)
[INFO] 2018-10-26 02:17:26,615 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,615 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-26 02:17:26,616 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 7 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,616 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 7 requested
[INFO] 2018-10-26 02:17:26,616 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 7
[ERROR] 2018-10-26 02:17:26,616 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 18 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 18 (epoch 22)
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 22 requested
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 22 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 22
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 18 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 23 requested
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 18 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 23 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 18 (epoch 22)
[INFO] 2018-10-26 02:17:26,617 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 23
[INFO] 2018-10-26 02:17:26,618 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,618 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-26 02:17:26,618 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-26 02:17:26,619 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/29 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,619 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 15 requested
[INFO] 2018-10-26 02:17:26,619 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 15 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,619 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 15
[INFO] 2018-10-26 02:17:26,619 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/29 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,619 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/46 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,619 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 29 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,619 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 29 requested
[INFO] 2018-10-26 02:17:26,620 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/46 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,620 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 29
[WARN] 2018-10-26 02:17:26,625 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:13062
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 02:17:26,625 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:13050
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,628 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/46 is now RUNNING
[ERROR] 2018-10-26 02:17:26,635 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 21 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 02:17:26,635 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,636 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 21 (epoch 23)
[INFO] 2018-10-26 02:17:26,636 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 21 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,636 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 21 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,636 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 6, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:17:26,637 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 21 (epoch 23)
[INFO] 2018-10-26 02:17:26,663 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13098) with ID 27
[ERROR] 2018-10-26 02:17:26,677 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7242618011105303357
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,683 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4847722779374451322
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:26,698 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:13076) with ID 10
[ERROR] 2018-10-26 02:17:26,712 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 13 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,713 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 13 (epoch 24)
[INFO] 2018-10-26 02:17:26,714 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 13 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,714 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 13 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,714 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 13 (epoch 24)
[INFO] 2018-10-26 02:17:26,715 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/24 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,715 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/24 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,716 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 24 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,716 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 24 requested
[INFO] 2018-10-26 02:17:26,716 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 24
[INFO] 2018-10-26 02:17:26,716 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/47 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,717 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/47 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,719 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/11 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,719 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/11 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,719 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/48 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,719 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 11 requested
[INFO] 2018-10-26 02:17:26,719 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,720 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/48 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,720 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 11
[ERROR] 2018-10-26 02:17:26,720 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6516918279178303703
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,722 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 6 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 02:17:26,722 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,723 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 6 (epoch 25)
[INFO] 2018-10-26 02:17:26,723 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 6 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,724 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 0, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:17:26,724 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 6 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,724 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 6 (epoch 25)
[INFO] 2018-10-26 02:17:26,731 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/47 is now RUNNING
[INFO] 2018-10-26 02:17:26,732 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/48 is now RUNNING
[ERROR] 2018-10-26 02:17:26,753 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 26 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,754 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 26 (epoch 26)
[INFO] 2018-10-26 02:17:26,754 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 26 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,754 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 26 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,755 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 26 (epoch 26)
[ERROR] 2018-10-26 02:17:26,758 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4860579498495474491
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,784 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8727906298591082029
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,773 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8240171244944539569
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 02:17:26,949 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:13056
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:17:26,953 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,953 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/20 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,953 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/20 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,954 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/49 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[WARN] 2018-10-26 02:17:26,954 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,954 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/49 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[ERROR] 2018-10-26 02:17:26,956 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-26 02:17:26,958 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 02:17:26,959 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 20 requested
[INFO] 2018-10-26 02:17:26,959 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 20 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,959 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 20
[INFO] 2018-10-26 02:17:26,963 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-26 02:17:26,965 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 0.903 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[ERROR] 2018-10-26 02:17:26,968 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 27 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,971 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/49 is now RUNNING
[INFO] 2018-10-26 02:17:26,971 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 0.963237 s
[INFO] 2018-10-26 02:17:26,973 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 27)
[INFO] 2018-10-26 02:17:26,974 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,974 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,974 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 27)
[INFO] 2018-10-26 02:17:26,974 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 27 (epoch 28)
[INFO] 2018-10-26 02:17:26,974 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 27 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,975 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 27 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,975 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 27 (epoch 28)
[ERROR] 2018-10-26 02:17:26,986 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 10 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:17:26,987 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 10 (epoch 29)
[INFO] 2018-10-26 02:17:26,987 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 10 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,987 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 10 successfully in removeExecutor
[INFO] 2018-10-26 02:17:26,987 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 10 (epoch 29)
[INFO] 2018-10-26 02:17:26,988 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/5 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:26,988 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/5 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:26,989 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/50 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:26,989 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:26,989 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/50 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:26,989 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 5 requested
[INFO] 2018-10-26 02:17:26,989 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 5
[INFO] 2018-10-26 02:17:26,992 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/50 is now RUNNING
[INFO] 2018-10-26 02:17:27,023 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/18 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:27,023 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/18 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:27,024 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 18 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:27,024 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 18 requested
[INFO] 2018-10-26 02:17:27,024 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 18
[INFO] 2018-10-26 02:17:27,025 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/51 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:27,026 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/51 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:27,035 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/21 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:17:27,035 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026021721-0019/21 removed: Command exited with code 1
[INFO] 2018-10-26 02:17:27,035 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026021721-0019/52 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:17:27,035 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 21 requested
[INFO] 2018-10-26 02:17:27,035 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 21
[INFO] 2018-10-26 02:17:27,035 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 21 from BlockManagerMaster.
[INFO] 2018-10-26 02:17:27,035 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026021721-0019/52 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:17:27,039 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 02:17:27,048 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/51 is now RUNNING
[INFO] 2018-10-26 02:17:27,050 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 02:17:27,057 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-26 02:17:27,057 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026021721-0019/52 is now RUNNING
[INFO] 2018-10-26 02:17:27,058 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[ERROR] 2018-10-26 02:17:27,071 org.apache.spark.network.server.TransportRequestHandler processOneWayMessage - Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find AppClient.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:27,071 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[ERROR] 2018-10-26 02:17:27,072 org.apache.spark.network.server.TransportRequestHandler processOneWayMessage - Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find AppClient.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:17:27,084 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 02:17:27,085 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 02:17:27,086 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 02:17:27,090 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 02:17:27,094 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 02:17:27,095 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 02:17:27,096 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6b0ef17d-a10c-4795-8ebf-80491fc58276
[INFO] 2018-10-26 02:17:27,096 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ce6db3ab-7e8f-44da-b0be-598d00759366
[INFO] 2018-10-26 02:17:27,097 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6b0ef17d-a10c-4795-8ebf-80491fc58276/pyspark-6e1a7dd8-a03b-4421-b005-9e91787b91c2
[WARN] 2018-10-26 02:22:42,609 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 02:22:43,395 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 02:22:43,421 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-26 02:22:43,627 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 02:22:43,628 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 02:22:43,628 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 02:22:43,628 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 02:22:43,629 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 02:22:43,826 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 21338.
[INFO] 2018-10-26 02:22:43,851 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 02:22:43,870 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 02:22:43,873 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 02:22:43,873 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 02:22:43,883 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-fd9cb890-8448-411b-bcaa-3c8cc930ff86
[INFO] 2018-10-26 02:22:43,899 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 02:22:43,913 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 02:22:44,085 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 02:22:44,141 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 02:22:44,231 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:21338/files/etl_config.json with timestamp 1540545764230
[INFO] 2018-10-26 02:22:44,233 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-5f1d366d-31ef-43ee-8a12-eb97c80b57e6/userFiles-3029aaaa-96bb-4333-98d2-e39135efce35/etl_config.json
[INFO] 2018-10-26 02:22:44,246 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:21338/files/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540545764246
[INFO] 2018-10-26 02:22:44,247 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-5f1d366d-31ef-43ee-8a12-eb97c80b57e6/userFiles-3029aaaa-96bb-4333-98d2-e39135efce35/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 02:22:44,253 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:21338/files/packages.zip with timestamp 1540545764253
[INFO] 2018-10-26 02:22:44,253 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-5f1d366d-31ef-43ee-8a12-eb97c80b57e6/userFiles-3029aaaa-96bb-4333-98d2-e39135efce35/packages.zip
[INFO] 2018-10-26 02:22:44,344 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-26 02:22:44,411 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 37 ms (0 ms spent in bootstraps)
[INFO] 2018-10-26 02:22:44,517 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181026022244-0001
[INFO] 2018-10-26 02:22:44,521 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/0 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,522 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/0 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,522 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/1 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,523 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/1 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,523 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/2 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,524 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/2 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,524 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/3 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,525 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/3 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,525 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/4 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,526 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/4 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,526 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/5 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,527 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/5 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,527 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/6 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,528 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/6 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,528 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/7 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,528 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10980.
[INFO] 2018-10-26 02:22:44,530 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:10980
[INFO] 2018-10-26 02:22:44,530 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/7 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,531 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/8 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,532 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/8 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,532 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 02:22:44,532 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/9 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,533 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/9 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,534 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/10 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,535 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/10 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,536 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/11 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,536 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/11 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,536 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/12 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,537 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/12 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,537 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/13 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,538 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/13 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,538 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/14 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,539 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/14 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,539 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/15 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,540 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/15 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,541 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/16 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,541 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/16 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,542 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/17 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,542 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/17 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,543 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/18 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,543 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/18 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,543 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/19 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,544 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/19 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,544 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/20 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,545 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/20 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,545 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/21 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,545 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/21 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,546 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/22 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,546 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/22 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,546 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/23 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,547 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/23 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,547 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/24 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,547 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/24 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,548 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/25 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,548 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/25 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,549 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/26 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,549 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/26 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,549 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/27 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,550 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/27 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,550 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/28 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,550 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/28 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,551 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/29 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:44,551 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/29 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:44,563 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/0 is now RUNNING
[INFO] 2018-10-26 02:22:44,565 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/1 is now RUNNING
[INFO] 2018-10-26 02:22:44,567 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 10980, None)
[INFO] 2018-10-26 02:22:44,569 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/2 is now RUNNING
[INFO] 2018-10-26 02:22:44,571 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:10980 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 10980, None)
[INFO] 2018-10-26 02:22:44,571 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/3 is now RUNNING
[INFO] 2018-10-26 02:22:44,572 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/4 is now RUNNING
[INFO] 2018-10-26 02:22:44,573 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/5 is now RUNNING
[INFO] 2018-10-26 02:22:44,574 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 10980, None)
[INFO] 2018-10-26 02:22:44,574 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 10980, None)
[INFO] 2018-10-26 02:22:44,576 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/6 is now RUNNING
[INFO] 2018-10-26 02:22:44,578 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/7 is now RUNNING
[INFO] 2018-10-26 02:22:44,580 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/8 is now RUNNING
[INFO] 2018-10-26 02:22:44,581 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/9 is now RUNNING
[INFO] 2018-10-26 02:22:44,582 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/10 is now RUNNING
[INFO] 2018-10-26 02:22:44,583 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/11 is now RUNNING
[INFO] 2018-10-26 02:22:44,585 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/12 is now RUNNING
[INFO] 2018-10-26 02:22:44,587 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/13 is now RUNNING
[INFO] 2018-10-26 02:22:44,587 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/14 is now RUNNING
[INFO] 2018-10-26 02:22:44,590 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/15 is now RUNNING
[INFO] 2018-10-26 02:22:44,590 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/16 is now RUNNING
[INFO] 2018-10-26 02:22:44,676 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/17 is now RUNNING
[INFO] 2018-10-26 02:22:44,678 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/18 is now RUNNING
[INFO] 2018-10-26 02:22:44,681 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/19 is now RUNNING
[INFO] 2018-10-26 02:22:44,682 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/20 is now RUNNING
[INFO] 2018-10-26 02:22:44,683 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/21 is now RUNNING
[INFO] 2018-10-26 02:22:44,684 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/22 is now RUNNING
[INFO] 2018-10-26 02:22:44,685 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/23 is now RUNNING
[INFO] 2018-10-26 02:22:44,685 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/24 is now RUNNING
[INFO] 2018-10-26 02:22:44,686 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/25 is now RUNNING
[INFO] 2018-10-26 02:22:44,687 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/26 is now RUNNING
[INFO] 2018-10-26 02:22:44,688 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/27 is now RUNNING
[INFO] 2018-10-26 02:22:44,689 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/28 is now RUNNING
[INFO] 2018-10-26 02:22:44,691 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/29 is now RUNNING
[INFO] 2018-10-26 02:22:44,748 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-26 02:22:44,942 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 02:22:44,943 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 02:22:45,384 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 02:22:48,280 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 207.028965 ms
[INFO] 2018-10-26 02:22:48,299 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17777) with ID 0
[INFO] 2018-10-26 02:22:48,313 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17783) with ID 6
[INFO] 2018-10-26 02:22:48,318 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17785) with ID 5
[INFO] 2018-10-26 02:22:48,414 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[ERROR] 2018-10-26 02:22:48,424 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8047382668261963062
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:22:48,434 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8457110921822995946
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:22:48,434 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 02:22:48,436 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 02:22:48,436 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 02:22:48,437 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 02:22:48,443 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 02:22:48,447 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17803) with ID 11
[ERROR] 2018-10-26 02:22:48,456 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:22:48,474 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4983974191102749108
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:22:48,485 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:48,508 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[ERROR] 2018-10-26 02:22:48,520 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 6 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:22:48,529 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6348695422049658243
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:22:48,535 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17797) with ID 1
[INFO] 2018-10-26 02:22:48,539 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 02:22:48,541 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:10980 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 02:22:48,544 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 02:22:48,555 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 02:22:48,556 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 02:22:48,558 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17819) with ID 18
[INFO] 2018-10-26 02:22:48,573 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 0)
[INFO] 2018-10-26 02:22:48,580 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,581 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,582 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 0)
[INFO] 2018-10-26 02:22:48,583 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 1)
[INFO] 2018-10-26 02:22:48,583 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,584 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,584 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 1)
[INFO] 2018-10-26 02:22:48,584 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 6 (epoch 2)
[INFO] 2018-10-26 02:22:48,584 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 6 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,584 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 6 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,584 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 6 (epoch 2)
[INFO] 2018-10-26 02:22:48,590 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 11, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-26 02:22:48,603 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 11 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 02:22:48,612 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:48,614 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 1, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:22:48,616 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 11 (epoch 3)
[INFO] 2018-10-26 02:22:48,617 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,617 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 11 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,617 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 11 (epoch 3)
[ERROR] 2018-10-26 02:22:48,632 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4700355636943281921
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:22:48,660 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 02:22:48,661 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:48,662 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 4)
[INFO] 2018-10-26 02:22:48,662 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,662 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 18, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:22:48,662 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,663 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17813) with ID 9
[INFO] 2018-10-26 02:22:48,663 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 4)
[INFO] 2018-10-26 02:22:48,669 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17811) with ID 24
[INFO] 2018-10-26 02:22:48,738 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17823) with ID 21
[ERROR] 2018-10-26 02:22:48,738 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5356305969646580517
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:22:48,745 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6392942464760353171
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:22:48,769 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 18 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 02:22:48,769 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:48,770 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 18 (epoch 5)
[INFO] 2018-10-26 02:22:48,770 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 21, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:22:48,770 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 18 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,774 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 18 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,774 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 18 (epoch 5)
[INFO] 2018-10-26 02:22:48,777 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17839) with ID 2
[ERROR] 2018-10-26 02:22:48,796 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7216856244812999428
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:22:48,797 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17837) with ID 10
[ERROR] 2018-10-26 02:22:48,806 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 9 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:48,807 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 9 (epoch 6)
[INFO] 2018-10-26 02:22:48,807 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 9 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,809 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 9 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,809 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 9 (epoch 6)
[INFO] 2018-10-26 02:22:48,831 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17827) with ID 4
[ERROR] 2018-10-26 02:22:48,840 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 24 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:48,841 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 24 (epoch 7)
[INFO] 2018-10-26 02:22:48,842 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 24 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,843 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 24 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,843 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 24 (epoch 7)
[ERROR] 2018-10-26 02:22:48,890 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6960571613936651734
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:22:48,892 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7201844659544466871
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:22:48,908 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 21 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 02:22:48,909 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:22:48,915 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-26 02:22:48,920 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 02:22:48,921 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17845) with ID 16
[ERROR] 2018-10-26 02:22:48,921 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7119236465902706932
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:22:48,924 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-26 02:22:48,926 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17853) with ID 3
[INFO] 2018-10-26 02:22:48,928 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 0.464 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-26 02:22:48,931 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17841) with ID 23
[INFO] 2018-10-26 02:22:48,934 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17851) with ID 14
[INFO] 2018-10-26 02:22:48,936 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 0.520916 s
[INFO] 2018-10-26 02:22:48,937 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 21 (epoch 8)
[INFO] 2018-10-26 02:22:48,938 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 21 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,938 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 21 successfully in removeExecutor
[ERROR] 2018-10-26 02:22:48,938 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 10 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:48,939 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 21 (epoch 8)
[INFO] 2018-10-26 02:22:48,940 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17865) with ID 13
[INFO] 2018-10-26 02:22:48,941 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 10 (epoch 9)
[INFO] 2018-10-26 02:22:48,942 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 10 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,942 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 10 successfully in removeExecutor
[ERROR] 2018-10-26 02:22:48,942 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:48,943 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 10 (epoch 9)
[INFO] 2018-10-26 02:22:48,943 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 10)
[INFO] 2018-10-26 02:22:48,944 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,944 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-26 02:22:48,944 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 10)
[INFO] 2018-10-26 02:22:48,946 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17849) with ID 7
[INFO] 2018-10-26 02:22:48,947 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17843) with ID 12
[INFO] 2018-10-26 02:22:48,952 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17861) with ID 26
[INFO] 2018-10-26 02:22:48,960 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:22:48,962 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022244-0001/0 removed: Command exited with code 1
[INFO] 2018-10-26 02:22:48,963 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/30 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:48,963 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,964 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/30 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:48,964 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-26 02:22:48,965 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-26 02:22:48,979 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:17867) with ID 20
[INFO] 2018-10-26 02:22:48,996 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 02:22:48,996 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/5 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:22:48,996 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022244-0001/5 removed: Command exited with code 1
[INFO] 2018-10-26 02:22:48,997 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 5 requested
[INFO] 2018-10-26 02:22:48,997 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:48,997 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 5
[INFO] 2018-10-26 02:22:49,000 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/31 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:49,001 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/31 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:49,001 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/11 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:22:49,002 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022244-0001/11 removed: Command exited with code 1
[INFO] 2018-10-26 02:22:49,002 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022244-0001/32 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:22:49,002 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[INFO] 2018-10-26 02:22:49,003 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022244-0001/32 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:22:49,002 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 11 requested
[INFO] 2018-10-26 02:22:49,003 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022244-0001/30 is now RUNNING
[INFO] 2018-10-26 02:22:49,003 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 11
[ERROR] 2018-10-26 02:22:49,006 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8462762893688191324
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:22:49,011 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8380157075099920050
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:22:49,014 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[ERROR] 2018-10-26 02:22:49,022 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8106183352628583920
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:22:49,024 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-26 02:22:49,025 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[ERROR] 2018-10-26 02:22:49,028 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 16 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:22:49,041 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[ERROR] 2018-10-26 02:22:49,052 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6472778592296631327
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:22:49,057 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 02:22:49,057 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 02:22:49,059 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[ERROR] 2018-10-26 02:22:49,061 org.apache.spark.network.server.TransportRequestHandler processOneWayMessage - Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:22:49,062 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 02:22:49,067 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 02:22:49,068 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 02:22:49,069 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5f1d366d-31ef-43ee-8a12-eb97c80b57e6/pyspark-b3a394b7-45f2-42da-99c2-9f65d5a4a5b9
[INFO] 2018-10-26 02:22:49,069 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e3ff7346-d011-4548-a682-f5b835cf4f6f
[INFO] 2018-10-26 02:22:49,069 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5f1d366d-31ef-43ee-8a12-eb97c80b57e6
[WARN] 2018-10-26 02:29:23,604 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 02:29:24,358 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 02:29:24,384 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-26 02:29:24,568 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 02:29:24,568 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 02:29:24,568 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 02:29:24,569 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 02:29:24,569 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 02:29:24,763 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 15359.
[INFO] 2018-10-26 02:29:24,786 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 02:29:24,805 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 02:29:24,808 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 02:29:24,809 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 02:29:24,819 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-0e97b55f-614f-41ea-ac5a-edf5692b911e
[INFO] 2018-10-26 02:29:24,836 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 02:29:24,850 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 02:29:25,015 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 02:29:25,062 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 02:29:25,154 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:15359/files/etl_config.json with timestamp 1540546165154
[INFO] 2018-10-26 02:29:25,156 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-2ca40b1e-6807-43cc-ab4d-24ba27c556bb/userFiles-79811b7c-0d63-4dcf-b965-ca371fdde125/etl_config.json
[INFO] 2018-10-26 02:29:25,169 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:15359/files/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540546165169
[INFO] 2018-10-26 02:29:25,170 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-2ca40b1e-6807-43cc-ab4d-24ba27c556bb/userFiles-79811b7c-0d63-4dcf-b965-ca371fdde125/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 02:29:25,174 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:15359/files/packages.zip with timestamp 1540546165174
[INFO] 2018-10-26 02:29:25,175 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-2ca40b1e-6807-43cc-ab4d-24ba27c556bb/userFiles-79811b7c-0d63-4dcf-b965-ca371fdde125/packages.zip
[INFO] 2018-10-26 02:29:25,269 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-26 02:29:25,352 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 44 ms (0 ms spent in bootstraps)
[INFO] 2018-10-26 02:29:25,473 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181026022925-0003
[INFO] 2018-10-26 02:29:25,477 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/0 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,479 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/0 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,479 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/1 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,480 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/1 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,481 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/2 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,482 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/2 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,482 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/3 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,483 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/3 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,483 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/4 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,484 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/4 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,485 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/5 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,485 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/5 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,486 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 21131.
[INFO] 2018-10-26 02:29:25,487 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/6 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,488 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/6 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,488 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:21131
[INFO] 2018-10-26 02:29:25,488 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/7 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,489 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/7 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,490 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/8 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,490 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/8 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,490 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 02:29:25,491 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/9 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,491 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/9 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,491 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/10 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,492 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/10 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,492 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/11 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,493 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/11 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,493 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/12 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,494 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/12 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,494 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/13 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,495 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/13 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,495 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/14 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,496 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/14 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,496 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/15 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,497 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/15 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,497 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/16 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,498 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/16 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,498 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/17 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,499 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/17 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,499 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/18 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,499 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/18 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,500 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/19 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,500 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/19 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,500 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/20 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,501 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/20 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,501 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/21 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,502 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/21 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,502 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/22 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,503 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/22 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,503 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/23 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,503 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/23 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,504 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/24 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,504 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/24 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,504 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/25 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,505 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/25 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,505 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/26 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,506 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/26 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,506 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/27 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,506 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/27 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,507 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/28 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,507 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/28 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,507 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/29 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:25,508 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/29 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:25,510 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/0 is now RUNNING
[INFO] 2018-10-26 02:29:25,511 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/1 is now RUNNING
[INFO] 2018-10-26 02:29:25,512 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/2 is now RUNNING
[INFO] 2018-10-26 02:29:25,512 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/3 is now RUNNING
[INFO] 2018-10-26 02:29:25,512 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/4 is now RUNNING
[INFO] 2018-10-26 02:29:25,513 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/5 is now RUNNING
[INFO] 2018-10-26 02:29:25,513 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/6 is now RUNNING
[INFO] 2018-10-26 02:29:25,514 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/7 is now RUNNING
[INFO] 2018-10-26 02:29:25,516 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/8 is now RUNNING
[INFO] 2018-10-26 02:29:25,518 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/9 is now RUNNING
[INFO] 2018-10-26 02:29:25,520 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/10 is now RUNNING
[INFO] 2018-10-26 02:29:25,524 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/11 is now RUNNING
[INFO] 2018-10-26 02:29:25,529 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/12 is now RUNNING
[INFO] 2018-10-26 02:29:25,529 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 21131, None)
[INFO] 2018-10-26 02:29:25,533 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:21131 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 21131, None)
[INFO] 2018-10-26 02:29:25,533 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/13 is now RUNNING
[INFO] 2018-10-26 02:29:25,536 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 21131, None)
[INFO] 2018-10-26 02:29:25,537 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 21131, None)
[INFO] 2018-10-26 02:29:25,541 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/14 is now RUNNING
[INFO] 2018-10-26 02:29:25,544 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/15 is now RUNNING
[INFO] 2018-10-26 02:29:25,545 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/16 is now RUNNING
[INFO] 2018-10-26 02:29:25,548 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/17 is now RUNNING
[INFO] 2018-10-26 02:29:25,552 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/18 is now RUNNING
[INFO] 2018-10-26 02:29:25,554 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/19 is now RUNNING
[INFO] 2018-10-26 02:29:25,561 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/20 is now RUNNING
[INFO] 2018-10-26 02:29:25,563 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/21 is now RUNNING
[INFO] 2018-10-26 02:29:25,565 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/22 is now RUNNING
[INFO] 2018-10-26 02:29:25,570 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/23 is now RUNNING
[INFO] 2018-10-26 02:29:25,572 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/24 is now RUNNING
[INFO] 2018-10-26 02:29:25,575 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/25 is now RUNNING
[INFO] 2018-10-26 02:29:25,581 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/26 is now RUNNING
[INFO] 2018-10-26 02:29:25,585 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/27 is now RUNNING
[INFO] 2018-10-26 02:29:25,589 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/28 is now RUNNING
[INFO] 2018-10-26 02:29:25,593 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/29 is now RUNNING
[INFO] 2018-10-26 02:29:25,716 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-26 02:29:25,923 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 02:29:25,924 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 02:29:26,365 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 02:29:29,733 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 251.021079 ms
[INFO] 2018-10-26 02:29:29,888 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 02:29:29,914 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 02:29:29,914 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 02:29:29,915 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 02:29:29,917 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 02:29:29,927 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 02:29:29,952 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12036) with ID 23
[INFO] 2018-10-26 02:29:29,997 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12034) with ID 7
[INFO] 2018-10-26 02:29:30,011 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 02:29:30,018 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12044) with ID 0
[ERROR] 2018-10-26 02:29:30,024 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8111849894100425651
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:30,049 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 02:29:30,052 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:21131 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 02:29:30,056 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[ERROR] 2018-10-26 02:29:30,059 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 23 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:30,067 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12070) with ID 24
[INFO] 2018-10-26 02:29:30,068 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12066) with ID 13
[INFO] 2018-10-26 02:29:30,069 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 02:29:30,070 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 02:29:30,095 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 23 (epoch 0)
[INFO] 2018-10-26 02:29:30,103 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 23 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:30,105 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 23 successfully in removeExecutor
[INFO] 2018-10-26 02:29:30,106 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 23 (epoch 0)
[INFO] 2018-10-26 02:29:30,111 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 7, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-26 02:29:30,126 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6298974835796681400
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,166 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7495475724857869747
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,184 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:30,188 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 1)
[INFO] 2018-10-26 02:29:30,188 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:30,189 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-26 02:29:30,189 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 1)
[ERROR] 2018-10-26 02:29:30,219 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 24 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:30,219 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 24 (epoch 2)
[INFO] 2018-10-26 02:29:30,220 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 24 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:30,220 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 24 successfully in removeExecutor
[INFO] 2018-10-26 02:29:30,221 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 24 (epoch 2)
[ERROR] 2018-10-26 02:29:30,252 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5824092952887844447
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,255 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4986241807696211737
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,282 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 13 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:30,283 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 13 (epoch 3)
[INFO] 2018-10-26 02:29:30,283 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 13 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:30,284 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 13 successfully in removeExecutor
[INFO] 2018-10-26 02:29:30,284 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 13 (epoch 3)
[ERROR] 2018-10-26 02:29:30,292 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 7 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 02:29:30,299 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:30,301 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12096) with ID 25
[INFO] 2018-10-26 02:29:30,302 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 25, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:29:30,309 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 7 (epoch 4)
[INFO] 2018-10-26 02:29:30,309 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 7 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:30,309 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 7 successfully in removeExecutor
[INFO] 2018-10-26 02:29:30,310 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 7 (epoch 4)
[INFO] 2018-10-26 02:29:30,324 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12064) with ID 15
[INFO] 2018-10-26 02:29:30,352 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12084) with ID 17
[ERROR] 2018-10-26 02:29:30,370 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8493872661926228295
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:30,378 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12074) with ID 16
[INFO] 2018-10-26 02:29:30,388 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12080) with ID 3
[ERROR] 2018-10-26 02:29:30,402 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 25 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-26 02:29:30,403 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 25): ExecutorLostFailure (executor 25 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:30,404 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 25 (epoch 5)
[INFO] 2018-10-26 02:29:30,404 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 16, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:29:30,404 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 25 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:30,404 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 25 successfully in removeExecutor
[INFO] 2018-10-26 02:29:30,405 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12106) with ID 10
[INFO] 2018-10-26 02:29:30,405 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 25 (epoch 5)
[INFO] 2018-10-26 02:29:30,407 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12090) with ID 11
[INFO] 2018-10-26 02:29:30,419 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12126) with ID 20
[INFO] 2018-10-26 02:29:30,420 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12108) with ID 2
[INFO] 2018-10-26 02:29:30,421 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12072) with ID 4
[INFO] 2018-10-26 02:29:30,425 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12110) with ID 8
[INFO] 2018-10-26 02:29:30,427 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12124) with ID 5
[INFO] 2018-10-26 02:29:30,439 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12088) with ID 6
[INFO] 2018-10-26 02:29:30,456 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12116) with ID 22
[INFO] 2018-10-26 02:29:30,459 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12086) with ID 18
[INFO] 2018-10-26 02:29:30,462 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12094) with ID 21
[INFO] 2018-10-26 02:29:30,464 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12118) with ID 12
[INFO] 2018-10-26 02:29:30,469 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12104) with ID 14
[INFO] 2018-10-26 02:29:30,472 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12112) with ID 26
[INFO] 2018-10-26 02:29:30,481 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/23 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:30,483 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/23 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:30,484 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12122) with ID 19
[INFO] 2018-10-26 02:29:30,485 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 23 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:30,485 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/30 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:30,485 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 23 requested
[INFO] 2018-10-26 02:29:30,486 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/30 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:30,486 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 23
[INFO] 2018-10-26 02:29:30,490 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12102) with ID 28
[ERROR] 2018-10-26 02:29:30,494 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8578416954925993905
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,502 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6153440846290364610
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:30,513 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/30 is now RUNNING
[INFO] 2018-10-26 02:29:30,514 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12100) with ID 1
[INFO] 2018-10-26 02:29:30,524 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12092) with ID 27
[ERROR] 2018-10-26 02:29:30,529 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 10 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:30,530 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 10 (epoch 6)
[INFO] 2018-10-26 02:29:30,530 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 10 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:30,531 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 10 successfully in removeExecutor
[ERROR] 2018-10-26 02:29:30,531 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7073163164616007274
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:30,531 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 10 (epoch 6)
[ERROR] 2018-10-26 02:29:30,536 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7288707705203959065
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,599 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5489802492722311703
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,590 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4989091937661263797
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,588 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 17 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-26 02:29:30,574 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4936123309584953107
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,551 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6239506377910036410
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,548 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4950280146510612550
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,543 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6771200758706486272
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,695 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9023879869374139609
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:30,669 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/24 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 02:29:30,625 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9207962187036405147
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,620 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5715805437725171521
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,606 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7635682094582092832
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,606 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7110046588191778789
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,605 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6717542152621836461
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:30,603 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 17 (epoch 7)
[ERROR] 2018-10-26 02:29:30,602 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 11 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:30,832 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 17 from BlockManagerMaster.
[ERROR] 2018-10-26 02:29:30,831 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8503166708723571968
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,829 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4774238721500199543
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,827 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8820908669043623522
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:30,826 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7008514489138831800
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:30,825 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/24 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:30,834 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12130) with ID 9
[INFO] 2018-10-26 02:29:30,833 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 17 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,070 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/31 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-26 02:29:31,070 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6806395126810330910
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:31,071 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/31 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,071 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 17 (epoch 7)
[INFO] 2018-10-26 02:29:31,072 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/31 is now RUNNING
[ERROR] 2018-10-26 02:29:31,072 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5688143063407946902
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:31,072 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:12114) with ID 29
[INFO] 2018-10-26 02:29:31,072 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/13 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,072 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 11 (epoch 8)
[INFO] 2018-10-26 02:29:31,073 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/13 removed: Command exited with code 1
[ERROR] 2018-10-26 02:29:31,073 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 16 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,073 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,074 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/32 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,074 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 11 successfully in removeExecutor
[WARN] 2018-10-26 02:29:31,074 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 16): ExecutorLostFailure (executor 16 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,074 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/32 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,074 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 11 (epoch 8)
[ERROR] 2018-10-26 02:29:31,075 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,075 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/32 is now RUNNING
[INFO] 2018-10-26 02:29:31,075 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/7 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 02:29:31,075 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 22 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,075 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 16 (epoch 9)
[INFO] 2018-10-26 02:29:31,076 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/7 removed: Command exited with code 1
[ERROR] 2018-10-26 02:29:31,076 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 21 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,076 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 16 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,076 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/33 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,076 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 16 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,077 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 16 (epoch 9)
[INFO] 2018-10-26 02:29:31,077 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 4, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-26 02:29:31,077 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/33 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,077 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 10)
[INFO] 2018-10-26 02:29:31,078 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/33 is now RUNNING
[INFO] 2018-10-26 02:29:31,078 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,078 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/0 removed: Command exited with code 1
[ERROR] 2018-10-26 02:29:31,078 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,078 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,079 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/34 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,079 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[ERROR] 2018-10-26 02:29:31,079 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,079 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 10)
[INFO] 2018-10-26 02:29:31,080 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 22 (epoch 11)
[INFO] 2018-10-26 02:29:31,080 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/34 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,080 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 22 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,080 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/34 is now RUNNING
[INFO] 2018-10-26 02:29:31,080 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 22 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,080 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/25 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 02:29:31,080 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 8 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,081 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/25 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,080 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 22 (epoch 11)
[INFO] 2018-10-26 02:29:31,081 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/35 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,081 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 21 (epoch 12)
[ERROR] 2018-10-26 02:29:31,081 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 12 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,081 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/35 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,081 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 21 from BlockManagerMaster.
[ERROR] 2018-10-26 02:29:31,082 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 18 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,082 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/35 is now RUNNING
[INFO] 2018-10-26 02:29:31,082 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 21 successfully in removeExecutor
[ERROR] 2018-10-26 02:29:31,082 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 26 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,082 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/10 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,082 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 21 (epoch 12)
[ERROR] 2018-10-26 02:29:31,083 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 15 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,083 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 13)
[INFO] 2018-10-26 02:29:31,083 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/10 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,083 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[ERROR] 2018-10-26 02:29:31,083 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 20 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,083 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,083 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/36 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,084 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 13)
[INFO] 2018-10-26 02:29:31,084 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 24 requested
[INFO] 2018-10-26 02:29:31,084 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 24 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,084 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 24
[INFO] 2018-10-26 02:29:31,084 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 14)
[INFO] 2018-10-26 02:29:31,084 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/36 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,084 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[ERROR] 2018-10-26 02:29:31,084 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 6 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,085 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,085 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/36 is now RUNNING
[INFO] 2018-10-26 02:29:31,085 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 14)
[INFO] 2018-10-26 02:29:31,085 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/11 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,085 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 8 (epoch 15)
[INFO] 2018-10-26 02:29:31,085 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 13 requested
[INFO] 2018-10-26 02:29:31,085 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 13 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,086 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 13
[INFO] 2018-10-26 02:29:31,085 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/11 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,086 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 8 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,086 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/37 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,086 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 8 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,086 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 8 (epoch 15)
[INFO] 2018-10-26 02:29:31,086 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/37 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 12 (epoch 16)
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/37 is now RUNNING
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 7 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/17 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 7 requested
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/17 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 12 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/38 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,087 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 7
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 12 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/38 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 12 (epoch 16)
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 18 (epoch 17)
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/38 is now RUNNING
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-26 02:29:31,089 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/15 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,088 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 18 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,089 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/15 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,089 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 18 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,089 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/39 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,089 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 18 (epoch 17)
[INFO] 2018-10-26 02:29:31,089 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 25 requested
[INFO] 2018-10-26 02:29:31,089 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 25 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,090 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 26 (epoch 18)
[INFO] 2018-10-26 02:29:31,090 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 25
[INFO] 2018-10-26 02:29:31,089 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/39 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,090 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 26 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,090 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/39 is now RUNNING
[INFO] 2018-10-26 02:29:31,090 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 26 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,090 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/12 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,090 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 26 (epoch 18)
[INFO] 2018-10-26 02:29:31,091 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/12 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,091 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 10 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,091 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 15 (epoch 19)
[INFO] 2018-10-26 02:29:31,091 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 10 requested
[INFO] 2018-10-26 02:29:31,091 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 15 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,091 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/40 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,092 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 15 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,091 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 10
[INFO] 2018-10-26 02:29:31,092 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 15 (epoch 19)
[INFO] 2018-10-26 02:29:31,092 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/40 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,092 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 20 (epoch 20)
[INFO] 2018-10-26 02:29:31,092 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 11 requested
[INFO] 2018-10-26 02:29:31,092 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,092 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,093 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/2 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,093 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 11
[INFO] 2018-10-26 02:29:31,093 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/41 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,093 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 20 from BlockManagerMaster.
[ERROR] 2018-10-26 02:29:31,093 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,093 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 20 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,094 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/41 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,094 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 20 (epoch 20)
[WARN] 2018-10-26 02:29:31,094 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,094 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 6 (epoch 21)
[INFO] 2018-10-26 02:29:31,094 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/22 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,094 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 6 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,094 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/22 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,095 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 6 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,095 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/42 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-26 02:29:31,095 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-26 02:29:31,095 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 6 (epoch 21)
[INFO] 2018-10-26 02:29:31,095 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/42 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,095 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/21 is now EXITED (Command exited with code 1)
[WARN] 2018-10-26 02:29:31,235 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:12112
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 02:29:31,222 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:12126
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 02:29:31,217 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:12086
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 02:29:31,214 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:12088
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 02:29:31,213 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:12124
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-26 02:29:31,206 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:12100
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:31,161 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8978354851528711382
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-26 02:29:31,158 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8916733397823865614
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:31,097 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[WARN] 2018-10-26 02:29:31,569 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:12110
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-26 02:29:31,561 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/21 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,570 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 17 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,570 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 17 requested
[INFO] 2018-10-26 02:29:31,571 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-26 02:29:31,571 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/43 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,571 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 17
[INFO] 2018-10-26 02:29:31,572 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 15 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,572 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 15 requested
[INFO] 2018-10-26 02:29:31,572 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/43 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,572 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 15
[INFO] 2018-10-26 02:29:31,573 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/40 is now RUNNING
[INFO] 2018-10-26 02:29:31,573 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 12 requested
[INFO] 2018-10-26 02:29:31,573 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 12 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,573 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 12
[INFO] 2018-10-26 02:29:31,573 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/41 is now RUNNING
[INFO] 2018-10-26 02:29:31,574 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-26 02:29:31,574 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,574 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/42 is now RUNNING
[INFO] 2018-10-26 02:29:31,574 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-26 02:29:31,574 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/43 is now RUNNING
[INFO] 2018-10-26 02:29:31,575 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 1.624 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-26 02:29:31,575 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 22 requested
[INFO] 2018-10-26 02:29:31,575 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 22 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,575 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 22
[INFO] 2018-10-26 02:29:31,575 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/16 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,576 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/16 removed: Command exited with code 1
[ERROR] 2018-10-26 02:29:31,576 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 19 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,577 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/44 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-26 02:29:31,577 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 14 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,577 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/44 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[ERROR] 2018-10-26 02:29:31,578 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 28 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,578 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/44 is now RUNNING
[INFO] 2018-10-26 02:29:31,578 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/1 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-26 02:29:31,578 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 27 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,579 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/1 removed: Command exited with code 1
[ERROR] 2018-10-26 02:29:31,579 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,579 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/45 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,580 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/45 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,580 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 21 requested
[INFO] 2018-10-26 02:29:31,580 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 21 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,580 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 21
[INFO] 2018-10-26 02:29:31,580 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/45 is now RUNNING
[INFO] 2018-10-26 02:29:31,580 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 16 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,580 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 16 requested
[INFO] 2018-10-26 02:29:31,581 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 16
[INFO] 2018-10-26 02:29:31,581 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/6 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,582 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,582 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-26 02:29:31,582 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-26 02:29:31,582 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 22)
[INFO] 2018-10-26 02:29:31,582 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/6 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,583 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,583 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 6 requested
[INFO] 2018-10-26 02:29:31,583 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/46 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,584 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 6
[INFO] 2018-10-26 02:29:31,584 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 6 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,584 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/46 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,584 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,583 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.694651 s
[INFO] 2018-10-26 02:29:31,584 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 22)
[INFO] 2018-10-26 02:29:31,584 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/5 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,588 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/5 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,588 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,588 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 5 requested
[INFO] 2018-10-26 02:29:31,588 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 19 (epoch 23)
[INFO] 2018-10-26 02:29:31,589 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 5
[INFO] 2018-10-26 02:29:31,589 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/47 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,589 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 19 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,590 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 19 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,590 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/47 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,590 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 19 (epoch 23)
[INFO] 2018-10-26 02:29:31,590 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/18 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,590 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 14 (epoch 24)
[ERROR] 2018-10-26 02:29:31,590 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 9 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,590 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 14 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,590 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/18 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,591 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 14 successfully in removeExecutor
[ERROR] 2018-10-26 02:29:31,591 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 29 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-26 02:29:31,591 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 14 (epoch 24)
[INFO] 2018-10-26 02:29:31,591 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/48 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,591 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 28 (epoch 25)
[INFO] 2018-10-26 02:29:31,592 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 18 requested
[INFO] 2018-10-26 02:29:31,592 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/48 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,592 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 18 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,592 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/46 is now RUNNING
[INFO] 2018-10-26 02:29:31,592 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 18
[INFO] 2018-10-26 02:29:31,593 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/20 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,593 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 28 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,593 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/20 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,593 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 28 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,594 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 28 (epoch 25)
[INFO] 2018-10-26 02:29:31,594 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/49 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,594 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 20 requested
[INFO] 2018-10-26 02:29:31,594 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 27 (epoch 26)
[INFO] 2018-10-26 02:29:31,594 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 20 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,594 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/49 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,594 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 20
[INFO] 2018-10-26 02:29:31,595 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/47 is now RUNNING
[INFO] 2018-10-26 02:29:31,595 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 27 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,595 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/48 is now RUNNING
[INFO] 2018-10-26 02:29:31,596 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 27 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,596 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 27 (epoch 26)
[INFO] 2018-10-26 02:29:31,596 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/49 is now RUNNING
[INFO] 2018-10-26 02:29:31,596 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 27)
[INFO] 2018-10-26 02:29:31,596 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/26 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,596 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,597 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/26 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,597 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,597 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 27)
[INFO] 2018-10-26 02:29:31,597 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 9 (epoch 28)
[INFO] 2018-10-26 02:29:31,597 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 26 requested
[INFO] 2018-10-26 02:29:31,597 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 26 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,597 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/50 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,598 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 9 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,598 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 26
[INFO] 2018-10-26 02:29:31,598 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/50 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,598 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 9 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/8 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,599 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 9 (epoch 28)
[INFO] 2018-10-26 02:29:31,599 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/8 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,599 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 29 (epoch 29)
[INFO] 2018-10-26 02:29:31,599 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 8 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/51 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,599 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 29 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,599 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 8 requested
[INFO] 2018-10-26 02:29:31,600 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 29 successfully in removeExecutor
[INFO] 2018-10-26 02:29:31,600 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/51 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,600 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 29 (epoch 29)
[INFO] 2018-10-26 02:29:31,600 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 8
[INFO] 2018-10-26 02:29:31,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/50 is now RUNNING
[INFO] 2018-10-26 02:29:31,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/51 is now RUNNING
[INFO] 2018-10-26 02:29:31,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,601 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/4 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,601 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/52 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,601 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-26 02:29:31,601 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,601 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-26 02:29:31,601 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/52 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,602 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/52 is now RUNNING
[INFO] 2018-10-26 02:29:31,602 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/19 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,602 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/19 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,602 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/53 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,602 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 19 requested
[INFO] 2018-10-26 02:29:31,602 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 19 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,602 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 19
[INFO] 2018-10-26 02:29:31,602 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/53 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,603 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/14 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,603 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/14 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,603 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/54 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,603 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 14 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,603 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 14 requested
[INFO] 2018-10-26 02:29:31,603 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/54 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,604 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 14
[INFO] 2018-10-26 02:29:31,604 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/53 is now RUNNING
[INFO] 2018-10-26 02:29:31,604 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/54 is now RUNNING
[INFO] 2018-10-26 02:29:31,604 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/28 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,604 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/28 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,604 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 28 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,604 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 28 requested
[INFO] 2018-10-26 02:29:31,605 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/55 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,605 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 28
[INFO] 2018-10-26 02:29:31,605 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/55 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,605 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/27 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,605 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/27 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,605 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/56 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,605 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 27 requested
[INFO] 2018-10-26 02:29:31,605 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 27 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,606 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 27
[INFO] 2018-10-26 02:29:31,606 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/56 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,606 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/55 is now RUNNING
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/56 is now RUNNING
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/3 is now EXITED (Command exited with code 1)
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181026022925-0003/3 removed: Command exited with code 1
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181026022925-0003/57 on worker-20181026022153-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181026022925-0003/57 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-26 02:29:31,607 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181026022925-0003/57 is now RUNNING
[INFO] 2018-10-26 02:29:31,647 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 02:29:31,812 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 02:29:31,819 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-26 02:29:31,820 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-26 02:29:31,831 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 02:29:31,843 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 02:29:31,844 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 02:29:31,847 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 02:29:31,851 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 02:29:31,854 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 02:29:31,855 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 02:29:31,856 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2ca40b1e-6807-43cc-ab4d-24ba27c556bb
[INFO] 2018-10-26 02:29:31,856 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2ca40b1e-6807-43cc-ab4d-24ba27c556bb/pyspark-30201662-7055-4fd8-a712-e099d64b1e45
[INFO] 2018-10-26 02:29:31,857 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-86194f13-07e2-452b-982f-a99599bd5ecd
[WARN] 2018-10-26 02:31:50,352 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2018-10-26 02:32:04,086 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2018-10-26 02:32:05,235 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 02:45:30,145 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 02:45:30,983 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 02:45:31,009 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_LOAD
[INFO] 2018-10-26 02:45:31,159 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 02:45:31,159 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 02:45:31,160 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 02:45:31,160 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 02:45:31,161 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 02:45:31,367 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 36483.
[INFO] 2018-10-26 02:45:31,391 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 02:45:31,413 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 02:45:31,417 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 02:45:31,417 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 02:45:31,428 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1c075689-a471-4803-bb7f-e5abf403d89d
[INFO] 2018-10-26 02:45:31,447 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 02:45:31,463 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 02:45:31,631 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-26 02:45:31,637 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-26 02:45:31,688 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 02:45:31,787 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540547131786
[INFO] 2018-10-26 02:45:31,788 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-45d2a969-e15d-4b6b-858b-18cb484c08f6/userFiles-ffc6d9f0-17f4-4b78-9967-5bfbfbbe2f8e/etl_config.json
[INFO] 2018-10-26 02:45:31,799 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py with timestamp 1540547131799
[INFO] 2018-10-26 02:45:31,800 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py to /tmp/spark-45d2a969-e15d-4b6b-858b-18cb484c08f6/userFiles-ffc6d9f0-17f4-4b78-9967-5bfbfbbe2f8e/JB_INVOICE_TIER_LOAD.py
[INFO] 2018-10-26 02:45:31,803 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540547131803
[INFO] 2018-10-26 02:45:31,803 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-45d2a969-e15d-4b6b-858b-18cb484c08f6/userFiles-ffc6d9f0-17f4-4b78-9967-5bfbfbbe2f8e/packages.zip
[INFO] 2018-10-26 02:45:31,859 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 02:45:31,883 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29531.
[INFO] 2018-10-26 02:45:31,884 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:29531
[INFO] 2018-10-26 02:45:31,886 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 02:45:31,915 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 29531, None)
[INFO] 2018-10-26 02:45:31,922 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:29531 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 29531, None)
[INFO] 2018-10-26 02:45:31,926 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 29531, None)
[INFO] 2018-10-26 02:45:31,927 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 29531, None)
[INFO] 2018-10-26 02:45:32,202 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 02:45:32,203 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 02:45:32,756 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 02:45:35,621 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 206.711259 ms
[INFO] 2018-10-26 02:45:35,775 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 02:45:35,797 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 02:45:35,797 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 02:45:35,798 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 02:45:35,800 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 02:45:35,808 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 02:45:35,883 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 02:45:35,922 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 02:45:35,925 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:29531 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 02:45:35,928 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 02:45:35,941 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 02:45:35,942 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 02:45:35,992 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 02:45:36,007 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 02:45:36,012 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540547131786
[INFO] 2018-10-26 02:45:36,047 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-45d2a969-e15d-4b6b-858b-18cb484c08f6/userFiles-ffc6d9f0-17f4-4b78-9967-5bfbfbbe2f8e/etl_config.json
[INFO] 2018-10-26 02:45:36,053 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540547131803
[INFO] 2018-10-26 02:45:36,054 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-45d2a969-e15d-4b6b-858b-18cb484c08f6/userFiles-ffc6d9f0-17f4-4b78-9967-5bfbfbbe2f8e/packages.zip
[INFO] 2018-10-26 02:45:36,059 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py with timestamp 1540547131799
[INFO] 2018-10-26 02:45:36,062 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py has been previously copied to /tmp/spark-45d2a969-e15d-4b6b-858b-18cb484c08f6/userFiles-ffc6d9f0-17f4-4b78-9967-5bfbfbbe2f8e/JB_INVOICE_TIER_LOAD.py
[INFO] 2018-10-26 02:45:36,748 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 02:45:36,787 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 469, boot = 366, init = 103, finish = 0
[INFO] 2018-10-26 02:45:36,811 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 02:45:36,825 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 845 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 02:45:36,830 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 02:45:36,842 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.012 s
[INFO] 2018-10-26 02:45:36,847 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.071073 s
[INFO] 2018-10-26 02:45:37,537 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 02:45:37,547 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 02:45:37,562 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 02:45:37,572 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 02:45:37,572 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 02:45:37,585 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 02:45:37,588 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 02:45:37,596 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 02:45:37,597 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 02:45:37,599 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c18fb619-b424-43bc-9713-3515f46ba073
[INFO] 2018-10-26 02:45:37,599 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-45d2a969-e15d-4b6b-858b-18cb484c08f6/pyspark-a5c79759-c777-499f-87b7-2541cd01007d
[INFO] 2018-10-26 02:45:37,600 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-45d2a969-e15d-4b6b-858b-18cb484c08f6
[WARN] 2018-10-26 03:24:29,120 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 03:24:29,890 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 03:24:29,916 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-26 03:24:30,098 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 03:24:30,098 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 03:24:30,098 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 03:24:30,099 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 03:24:30,100 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 03:24:30,310 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 36315.
[INFO] 2018-10-26 03:24:30,336 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 03:24:30,356 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 03:24:30,359 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 03:24:30,359 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 03:24:30,368 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-55d311f0-dd16-4386-9a6c-73d55c56283e
[INFO] 2018-10-26 03:24:30,386 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 03:24:30,399 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 03:24:30,581 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-26 03:24:30,587 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-26 03:24:30,642 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 03:24:30,742 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540549470741
[INFO] 2018-10-26 03:24:30,744 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-fdbf63cb-9367-4e06-823a-fdd5b46305c6/userFiles-dd971f40-4a26-494c-a796-0dfb1896cd6d/etl_config.json
[INFO] 2018-10-26 03:24:30,758 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540549470758
[INFO] 2018-10-26 03:24:30,759 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-fdbf63cb-9367-4e06-823a-fdd5b46305c6/userFiles-dd971f40-4a26-494c-a796-0dfb1896cd6d/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 03:24:30,764 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540549470764
[INFO] 2018-10-26 03:24:30,764 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-fdbf63cb-9367-4e06-823a-fdd5b46305c6/userFiles-dd971f40-4a26-494c-a796-0dfb1896cd6d/packages.zip
[INFO] 2018-10-26 03:24:30,833 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 03:24:30,860 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 19230.
[INFO] 2018-10-26 03:24:30,861 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:19230
[INFO] 2018-10-26 03:24:30,863 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 03:24:30,897 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 19230, None)
[INFO] 2018-10-26 03:24:30,903 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:19230 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 19230, None)
[INFO] 2018-10-26 03:24:30,907 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 19230, None)
[INFO] 2018-10-26 03:24:30,908 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 19230, None)
[INFO] 2018-10-26 03:24:31,215 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 03:24:31,216 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 03:24:31,736 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 03:24:34,721 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 236.418494 ms
[INFO] 2018-10-26 03:24:34,910 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 03:24:34,938 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 03:24:34,939 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 03:24:34,940 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 03:24:34,941 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 03:24:34,950 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 03:24:35,018 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 03:24:35,059 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 03:24:35,063 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:19230 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 03:24:35,066 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 03:24:35,096 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 03:24:35,097 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 03:24:35,147 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 03:24:35,160 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 03:24:35,167 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540549470758
[INFO] 2018-10-26 03:24:35,199 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-fdbf63cb-9367-4e06-823a-fdd5b46305c6/userFiles-dd971f40-4a26-494c-a796-0dfb1896cd6d/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 03:24:35,204 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540549470741
[INFO] 2018-10-26 03:24:35,204 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-fdbf63cb-9367-4e06-823a-fdd5b46305c6/userFiles-dd971f40-4a26-494c-a796-0dfb1896cd6d/etl_config.json
[INFO] 2018-10-26 03:24:35,210 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540549470764
[INFO] 2018-10-26 03:24:35,211 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-fdbf63cb-9367-4e06-823a-fdd5b46305c6/userFiles-dd971f40-4a26-494c-a796-0dfb1896cd6d/packages.zip
[INFO] 2018-10-26 03:24:35,857 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 03:24:35,897 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 470, boot = 403, init = 66, finish = 1
[INFO] 2018-10-26 03:24:35,918 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1706 bytes result sent to driver
[INFO] 2018-10-26 03:24:35,933 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 797 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 03:24:35,938 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 03:24:35,946 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.976 s
[INFO] 2018-10-26 03:24:35,952 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.040742 s
[INFO] 2018-10-26 03:24:36,159 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-26 03:24:36,160 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-26 03:24:36,161 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-26 03:24:36,161 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 03:24:36,161 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 03:24:36,162 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-26 03:24:36,165 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-26 03:24:36,167 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-26 03:24:36,168 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:19230 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-26 03:24:36,170 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 03:24:36,171 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 03:24:36,171 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 03:24:36,173 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 03:24:36,174 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 03:24:36,313 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 03:24:36,347 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -396, init = 437, finish = 0
[INFO] 2018-10-26 03:24:36,350 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-26 03:24:36,353 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 181 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 03:24:36,353 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 03:24:36,354 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.191 s
[INFO] 2018-10-26 03:24:36,356 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.196216 s
[INFO] 2018-10-26 03:24:36,546 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-26 03:24:36,547 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-26 03:24:36,548 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-26 03:24:36,548 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 03:24:36,548 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 03:24:36,549 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-26 03:24:36,552 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-26 03:24:36,554 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-26 03:24:36,556 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:19230 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-26 03:24:36,556 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 03:24:36,557 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 03:24:36,557 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-26 03:24:36,559 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 03:24:36,560 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-26 03:24:36,778 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 03:24:36,813 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -418, init = 460, finish = 0
[INFO] 2018-10-26 03:24:36,814 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-26 03:24:36,817 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 259 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 03:24:36,818 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 03:24:36,819 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.269 s
[INFO] 2018-10-26 03:24:36,821 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.274216 s
[INFO] 2018-10-26 03:24:37,068 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 03:24:37,081 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 03:24:37,099 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 03:24:37,119 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 03:24:37,120 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 03:24:37,128 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 03:24:37,132 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 03:24:37,141 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 03:24:37,142 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 03:24:37,143 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fdbf63cb-9367-4e06-823a-fdd5b46305c6/pyspark-ed103b60-dc99-4725-a0e1-8422aa130a71
[INFO] 2018-10-26 03:24:37,143 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c2952567-fc95-485c-b84a-8b3596687c15
[INFO] 2018-10-26 03:24:37,144 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fdbf63cb-9367-4e06-823a-fdd5b46305c6
[WARN] 2018-10-26 03:28:15,866 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 03:28:16,694 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 03:28:16,721 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-26 03:28:16,870 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 03:28:16,871 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 03:28:16,871 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 03:28:16,872 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 03:28:16,872 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 03:28:17,077 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 19923.
[INFO] 2018-10-26 03:28:17,102 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 03:28:17,122 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 03:28:17,125 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 03:28:17,126 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 03:28:17,135 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-baa801a2-686f-4185-a95d-359b98556788
[INFO] 2018-10-26 03:28:17,152 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 03:28:17,166 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 03:28:17,337 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-26 03:28:17,343 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-26 03:28:17,390 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 03:28:17,488 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540549697488
[INFO] 2018-10-26 03:28:17,490 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-c8213e14-f616-4755-8fee-84b18dd065eb/userFiles-08f5434e-dc1e-43fb-b721-ed3f9a162a92/etl_config.json
[INFO] 2018-10-26 03:28:17,502 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540549697502
[INFO] 2018-10-26 03:28:17,502 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-c8213e14-f616-4755-8fee-84b18dd065eb/userFiles-08f5434e-dc1e-43fb-b721-ed3f9a162a92/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 03:28:17,507 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540549697507
[INFO] 2018-10-26 03:28:17,507 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-c8213e14-f616-4755-8fee-84b18dd065eb/userFiles-08f5434e-dc1e-43fb-b721-ed3f9a162a92/packages.zip
[INFO] 2018-10-26 03:28:17,573 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 03:28:17,598 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32554.
[INFO] 2018-10-26 03:28:17,599 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:32554
[INFO] 2018-10-26 03:28:17,601 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 03:28:17,634 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 32554, None)
[INFO] 2018-10-26 03:28:17,641 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:32554 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 32554, None)
[INFO] 2018-10-26 03:28:17,645 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 32554, None)
[INFO] 2018-10-26 03:28:17,646 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 32554, None)
[INFO] 2018-10-26 03:28:17,956 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 03:28:17,957 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 03:28:18,493 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 03:28:19,025 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 03:28:19,035 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 03:28:19,047 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 03:28:19,059 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 03:28:19,060 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 03:28:19,067 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 03:28:19,074 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 03:28:19,083 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 03:28:19,083 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 03:28:19,084 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c8213e14-f616-4755-8fee-84b18dd065eb
[INFO] 2018-10-26 03:28:19,085 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c8213e14-f616-4755-8fee-84b18dd065eb/pyspark-47898ed6-dd46-458f-b476-e627e74e0ec0
[INFO] 2018-10-26 03:28:19,085 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2757f11a-76c7-4f1a-9050-783d0ce7a839
[WARN] 2018-10-26 03:35:18,823 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 03:35:19,595 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 03:35:19,622 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_LOAD
[INFO] 2018-10-26 03:35:19,842 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 03:35:19,842 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 03:35:19,843 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 03:35:19,843 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 03:35:19,850 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 03:35:20,040 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 15554.
[INFO] 2018-10-26 03:35:20,067 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 03:35:20,089 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 03:35:20,093 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 03:35:20,093 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 03:35:20,102 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-003dabd4-bffa-478e-b769-d50bf49bb59f
[INFO] 2018-10-26 03:35:20,119 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 03:35:20,132 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 03:35:20,324 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-26 03:35:20,331 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-26 03:35:20,407 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 03:35:20,523 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540550120522
[INFO] 2018-10-26 03:35:20,526 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-429bdfb4-31f1-4539-8461-8b0091f5bd76/userFiles-d7893f64-8dff-43d9-83e5-84396d828059/etl_config.json
[INFO] 2018-10-26 03:35:20,544 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py with timestamp 1540550120543
[INFO] 2018-10-26 03:35:20,544 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_LOAD.py to /tmp/spark-429bdfb4-31f1-4539-8461-8b0091f5bd76/userFiles-d7893f64-8dff-43d9-83e5-84396d828059/JB_INVOICE_TIER_LOAD.py
[INFO] 2018-10-26 03:35:20,549 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540550120548
[INFO] 2018-10-26 03:35:20,549 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-429bdfb4-31f1-4539-8461-8b0091f5bd76/userFiles-d7893f64-8dff-43d9-83e5-84396d828059/packages.zip
[INFO] 2018-10-26 03:35:20,628 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 03:35:20,653 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 31150.
[INFO] 2018-10-26 03:35:20,654 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:31150
[INFO] 2018-10-26 03:35:20,656 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 03:35:20,689 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 31150, None)
[INFO] 2018-10-26 03:35:20,695 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:31150 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 31150, None)
[INFO] 2018-10-26 03:35:20,699 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 31150, None)
[INFO] 2018-10-26 03:35:20,700 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 31150, None)
[INFO] 2018-10-26 03:35:21,024 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 03:35:21,024 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 03:35:21,629 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 03:36:00,880 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 03:36:00,892 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 03:36:00,905 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 03:36:00,919 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 03:36:00,919 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 03:36:00,920 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 03:36:00,928 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 03:36:00,936 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 03:36:00,936 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 03:36:00,937 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-76ca611d-c5db-4ce3-92df-b89985033dd0
[INFO] 2018-10-26 03:36:00,938 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-429bdfb4-31f1-4539-8461-8b0091f5bd76/pyspark-1b641090-a2c6-4007-b921-d33d28c2b22d
[INFO] 2018-10-26 03:36:00,938 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-429bdfb4-31f1-4539-8461-8b0091f5bd76
[WARN] 2018-10-26 05:47:16,814 org.apache.spark.HeartbeatReceiver logWarning - Removing executor driver with no recent heartbeats: 10990499 ms exceeds timeout 120000 ms
[ERROR] 2018-10-26 05:47:16,816 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost an executor driver (already removed): Executor heartbeat timed out after 10990499 ms
[WARN] 2018-10-26 05:47:16,820 org.apache.spark.SparkContext logWarning - Killing executors is not supported by current scheduler.
[WARN] 2018-10-26 09:38:06,531 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:38:07,301 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:38:07,327 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_PARTITION_TRUNCATE
[INFO] 2018-10-26 09:38:07,557 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:38:07,557 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:38:07,558 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:38:07,558 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:38:07,558 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:38:07,774 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 12936.
[INFO] 2018-10-26 09:38:07,798 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:38:07,818 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:38:07,821 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:38:07,821 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:38:07,830 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-cbdf70b3-b8a4-4d5d-973b-625dedec8922
[INFO] 2018-10-26 09:38:07,848 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:38:07,861 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:38:08,042 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:38:08,099 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:38:08,212 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540571888211
[INFO] 2018-10-26 09:38:08,216 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-52e89bf4-5214-4058-bdaa-535736755551/userFiles-ffdafb9a-bace-42f9-bf72-4134a47ecb1e/etl_config.json
[INFO] 2018-10-26 09:38:08,230 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540571888230
[INFO] 2018-10-26 09:38:08,231 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py to /tmp/spark-52e89bf4-5214-4058-bdaa-535736755551/userFiles-ffdafb9a-bace-42f9-bf72-4134a47ecb1e/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 09:38:08,236 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540571888236
[INFO] 2018-10-26 09:38:08,237 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-52e89bf4-5214-4058-bdaa-535736755551/userFiles-ffdafb9a-bace-42f9-bf72-4134a47ecb1e/packages.zip
[INFO] 2018-10-26 09:38:08,315 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:38:08,346 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34442.
[INFO] 2018-10-26 09:38:08,347 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:34442
[INFO] 2018-10-26 09:38:08,349 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:38:08,385 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34442, None)
[INFO] 2018-10-26 09:38:08,390 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:34442 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 34442, None)
[INFO] 2018-10-26 09:38:08,393 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34442, None)
[INFO] 2018-10-26 09:38:08,394 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 34442, None)
[INFO] 2018-10-26 09:38:08,683 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:38:08,683 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:38:09,215 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:38:12,228 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 235.00356 ms
[INFO] 2018-10-26 09:38:12,378 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 09:38:12,401 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 09:38:12,402 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 09:38:12,403 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:38:12,404 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:38:12,413 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 09:38:12,483 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 09:38:12,521 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 09:38:12,525 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:34442 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:38:12,528 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:38:12,544 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:38:12,546 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 09:38:12,595 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:38:12,610 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 09:38:12,618 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540571888211
[INFO] 2018-10-26 09:38:12,652 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-52e89bf4-5214-4058-bdaa-535736755551/userFiles-ffdafb9a-bace-42f9-bf72-4134a47ecb1e/etl_config.json
[INFO] 2018-10-26 09:38:12,658 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540571888236
[INFO] 2018-10-26 09:38:12,659 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-52e89bf4-5214-4058-bdaa-535736755551/userFiles-ffdafb9a-bace-42f9-bf72-4134a47ecb1e/packages.zip
[INFO] 2018-10-26 09:38:12,663 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540571888230
[INFO] 2018-10-26 09:38:12,665 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-52e89bf4-5214-4058-bdaa-535736755551/userFiles-ffdafb9a-bace-42f9-bf72-4134a47ecb1e/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 09:38:13,310 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:38:13,343 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 452, boot = 389, init = 62, finish = 1
[INFO] 2018-10-26 09:38:13,363 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 09:38:13,378 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 794 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:38:13,382 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:38:13,396 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.961 s
[INFO] 2018-10-26 09:38:13,401 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.022419 s
[INFO] 2018-10-26 09:38:13,583 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-26 09:38:13,585 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-26 09:38:13,585 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-26 09:38:13,586 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:38:13,586 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:38:13,588 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-26 09:38:13,594 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-26 09:38:13,596 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-26 09:38:13,597 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:34442 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:38:13,599 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:38:13,600 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:38:13,600 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 09:38:13,602 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:38:13,603 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 09:38:13,747 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:38:13,779 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -382, init = 424, finish = 0
[INFO] 2018-10-26 09:38:13,783 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-26 09:38:13,785 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 184 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:38:13,785 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:38:13,788 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.197 s
[INFO] 2018-10-26 09:38:13,789 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.205014 s
[INFO] 2018-10-26 09:38:14,085 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-26 09:38:14,087 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-26 09:38:14,087 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-26 09:38:14,087 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:38:14,088 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:38:14,089 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-26 09:38:14,094 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-26 09:38:14,096 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-26 09:38:14,098 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:34442 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:38:14,099 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:38:14,100 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:38:14,100 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-26 09:38:14,101 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:38:14,102 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-26 09:38:14,682 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:38:14,716 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -889, init = 931, finish = 0
[INFO] 2018-10-26 09:38:14,720 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-26 09:38:14,723 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 621 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:38:14,724 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:38:14,725 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.634 s
[INFO] 2018-10-26 09:38:14,726 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.640573 s
[INFO] 2018-10-26 09:38:14,966 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:38:14,981 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:38:15,007 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:38:15,036 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:38:15,037 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:38:15,050 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:38:15,055 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:38:15,069 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:38:15,070 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:38:15,072 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d8bad83f-25f4-4b44-b104-727d1527d5b9
[INFO] 2018-10-26 09:38:15,073 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-52e89bf4-5214-4058-bdaa-535736755551
[INFO] 2018-10-26 09:38:15,074 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-52e89bf4-5214-4058-bdaa-535736755551/pyspark-9a431a70-fb6d-4dbe-8e64-5e2c3d414841
[WARN] 2018-10-26 09:38:38,672 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:38:39,479 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:38:39,504 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_REJ_TRUNCATE
[INFO] 2018-10-26 09:38:39,663 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:38:39,663 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:38:39,664 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:38:39,664 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:38:39,664 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:38:39,869 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 22606.
[INFO] 2018-10-26 09:38:39,893 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:38:39,913 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:38:39,916 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:38:39,916 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:38:39,925 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1fb62f1f-67da-4496-a4cc-b7c2e6e78ad1
[INFO] 2018-10-26 09:38:39,942 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:38:39,956 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:38:40,134 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:38:40,183 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:38:40,292 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540571920292
[INFO] 2018-10-26 09:38:40,294 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-17b6a618-4e7f-44a4-912b-2d45133b6e10/userFiles-5d1ef85d-aaf0-4f80-8baa-d92028e930d8/etl_config.json
[INFO] 2018-10-26 09:38:40,309 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_REJ_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_REJ_TRUNCATE.py with timestamp 1540571920309
[INFO] 2018-10-26 09:38:40,310 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_REJ_TRUNCATE.py to /tmp/spark-17b6a618-4e7f-44a4-912b-2d45133b6e10/userFiles-5d1ef85d-aaf0-4f80-8baa-d92028e930d8/JB_BOOKINGS_REJ_TRUNCATE.py
[INFO] 2018-10-26 09:38:40,314 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540571920314
[INFO] 2018-10-26 09:38:40,315 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-17b6a618-4e7f-44a4-912b-2d45133b6e10/userFiles-5d1ef85d-aaf0-4f80-8baa-d92028e930d8/packages.zip
[INFO] 2018-10-26 09:38:40,389 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:38:40,413 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 22592.
[INFO] 2018-10-26 09:38:40,414 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:22592
[INFO] 2018-10-26 09:38:40,416 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:38:40,450 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 22592, None)
[INFO] 2018-10-26 09:38:40,457 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:22592 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 22592, None)
[INFO] 2018-10-26 09:38:40,461 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 22592, None)
[INFO] 2018-10-26 09:38:40,462 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 22592, None)
[INFO] 2018-10-26 09:38:40,742 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:38:40,742 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:38:41,259 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:38:41,515 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:38:41,528 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:38:41,542 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:38:41,551 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:38:41,551 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:38:41,561 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:38:41,566 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:38:41,575 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:38:41,579 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:38:41,581 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-17b6a618-4e7f-44a4-912b-2d45133b6e10/pyspark-98bcf1d2-bcc7-488a-9e84-3afc64e2e14e
[INFO] 2018-10-26 09:38:41,582 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-17b6a618-4e7f-44a4-912b-2d45133b6e10
[INFO] 2018-10-26 09:38:41,583 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bf8399a7-bc1e-43fb-8be8-52efee06f5f8
[WARN] 2018-10-26 09:39:09,626 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:39:10,385 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:39:10,417 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-26 09:39:10,663 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:39:10,663 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:39:10,664 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:39:10,664 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:39:10,665 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:39:10,860 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11414.
[INFO] 2018-10-26 09:39:10,886 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:39:10,905 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:39:10,908 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:39:10,909 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:39:10,918 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-921132f4-26a6-4178-95d9-8508a33fe7b3
[INFO] 2018-10-26 09:39:10,935 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:39:10,949 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:39:11,117 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:39:11,168 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:39:11,262 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540571951261
[INFO] 2018-10-26 09:39:11,264 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-41f7d5af-6a1d-42a7-91db-06bfa4191206/userFiles-7aeff238-2617-4fa0-aac5-b5746279bc6c/etl_config.json
[INFO] 2018-10-26 09:39:11,280 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py with timestamp 1540571951280
[INFO] 2018-10-26 09:39:11,281 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py to /tmp/spark-41f7d5af-6a1d-42a7-91db-06bfa4191206/userFiles-7aeff238-2617-4fa0-aac5-b5746279bc6c/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 09:39:11,286 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540571951286
[INFO] 2018-10-26 09:39:11,287 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-41f7d5af-6a1d-42a7-91db-06bfa4191206/userFiles-7aeff238-2617-4fa0-aac5-b5746279bc6c/packages.zip
[INFO] 2018-10-26 09:39:11,366 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:39:11,396 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18680.
[INFO] 2018-10-26 09:39:11,397 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:18680
[INFO] 2018-10-26 09:39:11,399 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:39:11,434 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18680, None)
[INFO] 2018-10-26 09:39:11,441 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:18680 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 18680, None)
[INFO] 2018-10-26 09:39:11,446 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18680, None)
[INFO] 2018-10-26 09:39:11,448 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 18680, None)
[INFO] 2018-10-26 09:39:11,733 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:39:11,734 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:39:12,255 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:39:15,267 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 214.458675 ms
[INFO] 2018-10-26 09:39:15,438 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 09:39:15,461 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 09:39:15,461 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 09:39:15,462 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:39:15,463 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:39:15,476 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 09:39:15,549 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 09:39:15,582 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 09:39:15,584 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:18680 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:39:15,587 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:39:15,602 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:39:15,603 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 09:39:15,655 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:39:15,669 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 09:39:15,676 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540571951261
[INFO] 2018-10-26 09:39:15,710 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-41f7d5af-6a1d-42a7-91db-06bfa4191206/userFiles-7aeff238-2617-4fa0-aac5-b5746279bc6c/etl_config.json
[INFO] 2018-10-26 09:39:15,715 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540571951286
[INFO] 2018-10-26 09:39:15,716 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-41f7d5af-6a1d-42a7-91db-06bfa4191206/userFiles-7aeff238-2617-4fa0-aac5-b5746279bc6c/packages.zip
[INFO] 2018-10-26 09:39:15,721 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py with timestamp 1540571951280
[INFO] 2018-10-26 09:39:15,723 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-41f7d5af-6a1d-42a7-91db-06bfa4191206/userFiles-7aeff238-2617-4fa0-aac5-b5746279bc6c/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-26 09:39:16,365 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:39:16,406 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 469, boot = 412, init = 56, finish = 1
[INFO] 2018-10-26 09:39:16,429 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 09:39:16,446 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 802 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:39:16,454 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:39:16,467 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.966 s
[INFO] 2018-10-26 09:39:16,473 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.034379 s
[INFO] 2018-10-26 09:39:16,674 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-26 09:39:16,676 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-26 09:39:16,676 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-26 09:39:16,677 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:39:16,677 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:39:16,678 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-26 09:39:16,684 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-26 09:39:16,687 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-26 09:39:16,688 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:18680 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:39:16,689 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:39:16,690 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:39:16,690 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 09:39:16,692 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:39:16,693 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 09:39:16,841 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:39:16,856 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -399, init = 441, finish = 0
[INFO] 2018-10-26 09:39:16,859 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1704 bytes result sent to driver
[INFO] 2018-10-26 09:39:16,862 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 171 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:39:16,862 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:39:16,864 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.184 s
[INFO] 2018-10-26 09:39:16,865 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.190639 s
[INFO] 2018-10-26 09:39:17,157 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-26 09:39:17,159 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-26 09:39:17,159 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-26 09:39:17,159 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:39:17,160 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:39:17,161 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-26 09:39:17,165 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-26 09:39:17,168 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-26 09:39:17,169 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:18680 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:39:17,170 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:39:17,171 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:39:17,171 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-26 09:39:17,173 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:39:17,174 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-26 09:39:17,423 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:39:17,460 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -557, init = 599, finish = 0
[INFO] 2018-10-26 09:39:17,462 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1675 bytes result sent to driver
[INFO] 2018-10-26 09:39:17,464 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 292 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:39:17,465 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:39:17,467 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.303 s
[INFO] 2018-10-26 09:39:17,468 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.310094 s
[INFO] 2018-10-26 09:39:17,661 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:39:17,673 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:39:17,690 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:39:17,709 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:39:17,710 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:39:17,723 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:39:17,727 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:39:17,736 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:39:17,737 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:39:17,739 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-41f7d5af-6a1d-42a7-91db-06bfa4191206
[INFO] 2018-10-26 09:39:17,740 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-41f7d5af-6a1d-42a7-91db-06bfa4191206/pyspark-f8fde202-6944-4236-bdf6-c3a848c67dae
[INFO] 2018-10-26 09:39:17,741 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d1570154-d2da-4b11-84f5-b701fa802d8a
[WARN] 2018-10-26 09:40:07,309 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:40:08,125 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:40:08,152 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER
[INFO] 2018-10-26 09:40:08,343 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:40:08,343 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:40:08,344 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:40:08,344 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:40:08,345 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:40:08,561 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 13130.
[INFO] 2018-10-26 09:40:08,588 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:40:08,607 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:40:08,610 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:40:08,610 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:40:08,619 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-f531db33-fc6d-45e8-9688-cf4e8e59da94
[INFO] 2018-10-26 09:40:08,636 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:40:08,650 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:40:08,843 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:40:08,902 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:40:09,002 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572009002
[INFO] 2018-10-26 09:40:09,004 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-40e1ec5d-fa58-44de-bff2-a3235b9d923c/userFiles-b7f16777-ad19-4543-bc33-8900be2a6ef8/etl_config.json
[INFO] 2018-10-26 09:40:09,017 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540572009017
[INFO] 2018-10-26 09:40:09,018 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py to /tmp/spark-40e1ec5d-fa58-44de-bff2-a3235b9d923c/userFiles-b7f16777-ad19-4543-bc33-8900be2a6ef8/JB_BOOKINGS_TIER.py
[INFO] 2018-10-26 09:40:09,022 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572009022
[INFO] 2018-10-26 09:40:09,023 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-40e1ec5d-fa58-44de-bff2-a3235b9d923c/userFiles-b7f16777-ad19-4543-bc33-8900be2a6ef8/packages.zip
[INFO] 2018-10-26 09:40:09,083 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:40:09,106 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15151.
[INFO] 2018-10-26 09:40:09,107 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:15151
[INFO] 2018-10-26 09:40:09,109 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:40:09,136 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 15151, None)
[INFO] 2018-10-26 09:40:09,141 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:15151 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 15151, None)
[INFO] 2018-10-26 09:40:09,144 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 15151, None)
[INFO] 2018-10-26 09:40:09,144 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 15151, None)
[INFO] 2018-10-26 09:40:09,391 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:40:09,392 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:40:09,958 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:40:09,998 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:40:10,008 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:40:10,022 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:40:10,030 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:40:10,031 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:40:10,038 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:40:10,044 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:40:10,053 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:40:10,055 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:40:10,056 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-40e1ec5d-fa58-44de-bff2-a3235b9d923c/pyspark-5722f658-cc84-4ebe-accd-3803094532fd
[INFO] 2018-10-26 09:40:10,057 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-40e1ec5d-fa58-44de-bff2-a3235b9d923c
[INFO] 2018-10-26 09:40:10,057 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bf46eb57-92d3-4e63-9761-95a5402303e7
[WARN] 2018-10-26 09:42:02,532 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:42:03,385 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:42:03,410 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER
[INFO] 2018-10-26 09:42:03,719 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:42:03,720 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:42:03,720 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:42:03,720 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:42:03,732 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:42:03,941 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 13167.
[INFO] 2018-10-26 09:42:03,968 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:42:03,986 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:42:03,989 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:42:03,990 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:42:03,998 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-74e4ae08-a41b-46b8-84e7-adf64445bb49
[INFO] 2018-10-26 09:42:04,015 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:42:04,029 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:42:04,197 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:42:04,255 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:42:04,353 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572124353
[INFO] 2018-10-26 09:42:04,355 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-c9f6b6f2-3b5f-4414-a510-de56c2b71820/userFiles-b15c94fc-bc4e-4cb2-8beb-29e5653ff5b5/etl_config.json
[INFO] 2018-10-26 09:42:04,371 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540572124371
[INFO] 2018-10-26 09:42:04,372 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py to /tmp/spark-c9f6b6f2-3b5f-4414-a510-de56c2b71820/userFiles-b15c94fc-bc4e-4cb2-8beb-29e5653ff5b5/JB_BOOKINGS_TIER.py
[INFO] 2018-10-26 09:42:04,377 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572124377
[INFO] 2018-10-26 09:42:04,377 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-c9f6b6f2-3b5f-4414-a510-de56c2b71820/userFiles-b15c94fc-bc4e-4cb2-8beb-29e5653ff5b5/packages.zip
[INFO] 2018-10-26 09:42:04,442 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:42:04,467 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15002.
[INFO] 2018-10-26 09:42:04,469 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:15002
[INFO] 2018-10-26 09:42:04,470 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:42:04,504 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 15002, None)
[INFO] 2018-10-26 09:42:04,509 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:15002 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 15002, None)
[INFO] 2018-10-26 09:42:04,513 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 15002, None)
[INFO] 2018-10-26 09:42:04,514 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 15002, None)
[INFO] 2018-10-26 09:42:04,799 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:42:04,800 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:42:05,384 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:42:05,429 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:42:05,440 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:42:05,453 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:42:05,461 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:42:05,465 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:42:05,478 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:42:05,484 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:42:05,492 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:42:05,493 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:42:05,494 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d06945e8-4ec5-4997-b1ab-3279b23c9bfd
[INFO] 2018-10-26 09:42:05,494 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c9f6b6f2-3b5f-4414-a510-de56c2b71820
[INFO] 2018-10-26 09:42:05,494 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c9f6b6f2-3b5f-4414-a510-de56c2b71820/pyspark-1c5dce53-c973-48a7-9e2f-c1eee853190a
[WARN] 2018-10-26 09:43:02,904 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:43:03,681 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:43:03,707 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER
[INFO] 2018-10-26 09:43:03,875 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:43:03,875 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:43:03,876 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:43:03,876 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:43:03,877 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:43:04,108 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 28318.
[INFO] 2018-10-26 09:43:04,138 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:43:04,160 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:43:04,163 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:43:04,164 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:43:04,174 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-0d9eeca7-f3cb-49ad-929b-ac45d7055049
[INFO] 2018-10-26 09:43:04,194 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:43:04,210 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:43:04,399 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:43:04,470 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:43:04,595 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572184594
[INFO] 2018-10-26 09:43:04,597 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-64133a85-9366-44fd-a134-496d4af8c0ec/userFiles-c89b029a-904a-4a27-aa64-a5ee8240a0d7/etl_config.json
[INFO] 2018-10-26 09:43:04,610 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540572184610
[INFO] 2018-10-26 09:43:04,610 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py to /tmp/spark-64133a85-9366-44fd-a134-496d4af8c0ec/userFiles-c89b029a-904a-4a27-aa64-a5ee8240a0d7/JB_BOOKINGS_TIER.py
[INFO] 2018-10-26 09:43:04,614 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572184614
[INFO] 2018-10-26 09:43:04,614 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-64133a85-9366-44fd-a134-496d4af8c0ec/userFiles-c89b029a-904a-4a27-aa64-a5ee8240a0d7/packages.zip
[INFO] 2018-10-26 09:43:04,683 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:43:04,709 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33743.
[INFO] 2018-10-26 09:43:04,710 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:33743
[INFO] 2018-10-26 09:43:04,712 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:43:04,745 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 33743, None)
[INFO] 2018-10-26 09:43:04,749 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:33743 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 33743, None)
[INFO] 2018-10-26 09:43:04,752 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 33743, None)
[INFO] 2018-10-26 09:43:04,753 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 33743, None)
[INFO] 2018-10-26 09:43:05,014 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:43:05,014 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:43:05,530 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:43:08,473 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 201.732728 ms
[INFO] 2018-10-26 09:43:08,617 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 09:43:08,645 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 09:43:08,646 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 09:43:08,646 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:43:08,648 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:43:08,667 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 09:43:08,738 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 09:43:08,773 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 09:43:08,776 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:33743 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:43:08,779 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:43:08,802 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:43:08,803 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 09:43:08,854 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:43:08,870 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 09:43:08,879 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572184594
[INFO] 2018-10-26 09:43:08,911 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-64133a85-9366-44fd-a134-496d4af8c0ec/userFiles-c89b029a-904a-4a27-aa64-a5ee8240a0d7/etl_config.json
[INFO] 2018-10-26 09:43:08,917 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540572184610
[INFO] 2018-10-26 09:43:08,919 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py has been previously copied to /tmp/spark-64133a85-9366-44fd-a134-496d4af8c0ec/userFiles-c89b029a-904a-4a27-aa64-a5ee8240a0d7/JB_BOOKINGS_TIER.py
[INFO] 2018-10-26 09:43:08,923 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540572184614
[INFO] 2018-10-26 09:43:08,924 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-64133a85-9366-44fd-a134-496d4af8c0ec/userFiles-c89b029a-904a-4a27-aa64-a5ee8240a0d7/packages.zip
[INFO] 2018-10-26 09:43:09,691 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:43:09,731 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 442, boot = 369, init = 72, finish = 1
[INFO] 2018-10-26 09:43:09,750 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 09:43:09,764 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 922 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:43:09,769 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:43:09,782 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.088 s
[INFO] 2018-10-26 09:43:09,788 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.170414 s
[INFO] 2018-10-26 09:43:09,824 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:43:09,841 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:43:09,855 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:43:09,867 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:43:09,868 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:43:09,875 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:43:09,879 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:43:09,894 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:43:09,896 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:43:09,898 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-64133a85-9366-44fd-a134-496d4af8c0ec
[INFO] 2018-10-26 09:43:09,899 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-64133a85-9366-44fd-a134-496d4af8c0ec/pyspark-ea3b8c66-6b82-4f17-9332-13b402167081
[INFO] 2018-10-26 09:43:09,899 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a87ab3a9-bf9c-4574-9042-76afaf590ada
[WARN] 2018-10-26 09:43:49,035 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:43:49,849 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:43:49,871 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER
[INFO] 2018-10-26 09:43:50,225 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:43:50,225 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:43:50,225 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:43:50,226 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:43:50,233 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:43:50,420 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 28251.
[INFO] 2018-10-26 09:43:50,444 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:43:50,463 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:43:50,466 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:43:50,466 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:43:50,475 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-08f92a72-a8a2-472f-9090-3f36eb86474c
[INFO] 2018-10-26 09:43:50,493 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:43:50,507 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:43:50,685 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:43:50,751 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:43:50,845 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572230844
[INFO] 2018-10-26 09:43:50,847 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-c1497ef2-761f-41e7-ac81-f3fe787c95ad/userFiles-bc565576-d416-410e-a18b-f09480874f0f/etl_config.json
[INFO] 2018-10-26 09:43:50,859 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540572230859
[INFO] 2018-10-26 09:43:50,859 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py to /tmp/spark-c1497ef2-761f-41e7-ac81-f3fe787c95ad/userFiles-bc565576-d416-410e-a18b-f09480874f0f/JB_BOOKINGS_TIER.py
[INFO] 2018-10-26 09:43:50,864 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572230864
[INFO] 2018-10-26 09:43:50,864 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-c1497ef2-761f-41e7-ac81-f3fe787c95ad/userFiles-bc565576-d416-410e-a18b-f09480874f0f/packages.zip
[INFO] 2018-10-26 09:43:50,935 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:43:50,965 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35041.
[INFO] 2018-10-26 09:43:50,966 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:35041
[INFO] 2018-10-26 09:43:50,968 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:43:51,002 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35041, None)
[INFO] 2018-10-26 09:43:51,013 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:35041 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 35041, None)
[INFO] 2018-10-26 09:43:51,018 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35041, None)
[INFO] 2018-10-26 09:43:51,019 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 35041, None)
[INFO] 2018-10-26 09:43:51,328 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:43:51,329 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:43:51,882 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:43:54,858 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 204.855278 ms
[INFO] 2018-10-26 09:43:55,009 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 09:43:55,030 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 09:43:55,031 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 09:43:55,032 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:43:55,033 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:43:55,048 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 09:43:55,123 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 09:43:55,159 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 09:43:55,163 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:35041 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:43:55,167 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:43:55,181 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:43:55,183 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 09:43:55,235 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:43:55,251 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 09:43:55,260 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572230844
[INFO] 2018-10-26 09:43:55,288 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-c1497ef2-761f-41e7-ac81-f3fe787c95ad/userFiles-bc565576-d416-410e-a18b-f09480874f0f/etl_config.json
[INFO] 2018-10-26 09:43:55,294 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540572230859
[INFO] 2018-10-26 09:43:55,296 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py has been previously copied to /tmp/spark-c1497ef2-761f-41e7-ac81-f3fe787c95ad/userFiles-bc565576-d416-410e-a18b-f09480874f0f/JB_BOOKINGS_TIER.py
[INFO] 2018-10-26 09:43:55,301 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540572230864
[INFO] 2018-10-26 09:43:55,303 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-c1497ef2-761f-41e7-ac81-f3fe787c95ad/userFiles-bc565576-d416-410e-a18b-f09480874f0f/packages.zip
[INFO] 2018-10-26 09:43:55,962 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:43:56,001 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 467, boot = 400, init = 66, finish = 1
[INFO] 2018-10-26 09:43:56,025 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1706 bytes result sent to driver
[INFO] 2018-10-26 09:43:56,036 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 814 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:43:56,040 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:43:56,053 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.983 s
[INFO] 2018-10-26 09:43:56,060 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.050564 s
[INFO] 2018-10-26 09:43:56,096 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:43:56,107 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:43:56,123 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:43:56,134 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:43:56,135 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:43:56,146 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:43:56,150 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:43:56,162 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:43:56,162 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:43:56,164 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c1497ef2-761f-41e7-ac81-f3fe787c95ad/pyspark-e0215035-6bf6-4bcb-9e75-0964c245aa27
[INFO] 2018-10-26 09:43:56,164 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c1497ef2-761f-41e7-ac81-f3fe787c95ad
[INFO] 2018-10-26 09:43:56,165 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6bb3c1c0-4c2f-4c03-9ccd-27b3dab5659f
[WARN] 2018-10-26 09:46:03,154 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:46:03,995 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:46:04,022 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER
[INFO] 2018-10-26 09:46:04,197 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:46:04,197 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:46:04,197 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:46:04,198 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:46:04,208 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:46:04,427 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 18950.
[INFO] 2018-10-26 09:46:04,456 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:46:04,478 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:46:04,481 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:46:04,482 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:46:04,493 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-c10eb62f-0654-4da2-b133-eebb09a2b8e5
[INFO] 2018-10-26 09:46:04,513 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:46:04,529 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:46:04,745 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:46:04,808 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:46:04,916 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572364915
[INFO] 2018-10-26 09:46:04,918 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-3ce44e51-ef25-477c-bb2d-1c450fa92bb7/userFiles-d0da4288-6c18-4a6f-9fbd-b1a1636832df/etl_config.json
[INFO] 2018-10-26 09:46:04,933 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540572364933
[INFO] 2018-10-26 09:46:04,934 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py to /tmp/spark-3ce44e51-ef25-477c-bb2d-1c450fa92bb7/userFiles-d0da4288-6c18-4a6f-9fbd-b1a1636832df/JB_BOOKINGS_TIER.py
[INFO] 2018-10-26 09:46:04,939 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572364939
[INFO] 2018-10-26 09:46:04,940 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-3ce44e51-ef25-477c-bb2d-1c450fa92bb7/userFiles-d0da4288-6c18-4a6f-9fbd-b1a1636832df/packages.zip
[INFO] 2018-10-26 09:46:05,011 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:46:05,036 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 20066.
[INFO] 2018-10-26 09:46:05,037 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:20066
[INFO] 2018-10-26 09:46:05,039 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:46:05,073 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 20066, None)
[INFO] 2018-10-26 09:46:05,080 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:20066 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 20066, None)
[INFO] 2018-10-26 09:46:05,084 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 20066, None)
[INFO] 2018-10-26 09:46:05,085 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 20066, None)
[INFO] 2018-10-26 09:46:05,352 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:46:05,353 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:46:05,886 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:46:08,750 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 200.988257 ms
[INFO] 2018-10-26 09:46:08,898 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 09:46:08,922 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 09:46:08,923 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 09:46:08,923 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 09:46:08,925 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 09:46:08,934 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 09:46:09,012 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 09:46:09,048 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 09:46:09,052 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:20066 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 09:46:09,055 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 09:46:09,070 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 09:46:09,071 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 09:46:09,123 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 09:46:09,138 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 09:46:09,145 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572364915
[INFO] 2018-10-26 09:46:09,176 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-3ce44e51-ef25-477c-bb2d-1c450fa92bb7/userFiles-d0da4288-6c18-4a6f-9fbd-b1a1636832df/etl_config.json
[INFO] 2018-10-26 09:46:09,181 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540572364933
[INFO] 2018-10-26 09:46:09,182 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py has been previously copied to /tmp/spark-3ce44e51-ef25-477c-bb2d-1c450fa92bb7/userFiles-d0da4288-6c18-4a6f-9fbd-b1a1636832df/JB_BOOKINGS_TIER.py
[INFO] 2018-10-26 09:46:09,187 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540572364939
[INFO] 2018-10-26 09:46:09,189 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-3ce44e51-ef25-477c-bb2d-1c450fa92bb7/userFiles-d0da4288-6c18-4a6f-9fbd-b1a1636832df/packages.zip
[INFO] 2018-10-26 09:46:09,887 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 09:46:09,922 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 463, boot = 402, init = 61, finish = 0
[INFO] 2018-10-26 09:46:09,942 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 09:46:09,957 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 846 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 09:46:09,962 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 09:46:09,975 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.015 s
[INFO] 2018-10-26 09:46:09,980 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.081638 s
[INFO] 2018-10-26 09:46:12,927 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:46:12,940 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:46:12,955 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:46:12,967 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:46:12,968 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:46:12,979 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:46:12,982 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:46:12,990 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:46:12,991 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:46:12,993 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3ce44e51-ef25-477c-bb2d-1c450fa92bb7/pyspark-a460d043-d078-4332-adeb-3b61d7700a6b
[INFO] 2018-10-26 09:46:12,993 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3ce44e51-ef25-477c-bb2d-1c450fa92bb7
[INFO] 2018-10-26 09:46:12,993 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5a41059b-41fb-4d83-9f29-10afa32dd790
[WARN] 2018-10-26 09:48:13,364 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:48:14,181 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:48:14,210 org.apache.spark.SparkContext logInfo - Submitted application: JB_DAY_FLAG_BOOKINGS
[INFO] 2018-10-26 09:48:14,382 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:48:14,382 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:48:14,383 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:48:14,383 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:48:14,383 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:48:14,578 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 9067.
[INFO] 2018-10-26 09:48:14,602 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:48:14,621 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:48:14,624 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:48:14,624 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:48:14,633 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-2fab7cb4-3b3c-4ba6-9a6c-d2241800d25f
[INFO] 2018-10-26 09:48:14,651 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:48:14,664 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:48:14,840 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:48:14,893 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:48:14,983 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572494983
[INFO] 2018-10-26 09:48:14,985 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-1996572f-6c2f-4f47-89e8-c9f869929be1/userFiles-7b13b406-c301-48f0-848c-b1ef32bc748d/etl_config.json
[INFO] 2018-10-26 09:48:14,997 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_BOOKINGS.py with timestamp 1540572494997
[INFO] 2018-10-26 09:48:14,998 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_BOOKINGS.py to /tmp/spark-1996572f-6c2f-4f47-89e8-c9f869929be1/userFiles-7b13b406-c301-48f0-848c-b1ef32bc748d/JB_DAY_FLAG_BOOKINGS.py
[INFO] 2018-10-26 09:48:15,003 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572495003
[INFO] 2018-10-26 09:48:15,003 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-1996572f-6c2f-4f47-89e8-c9f869929be1/userFiles-7b13b406-c301-48f0-848c-b1ef32bc748d/packages.zip
[INFO] 2018-10-26 09:48:15,065 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:48:15,087 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23348.
[INFO] 2018-10-26 09:48:15,088 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:23348
[INFO] 2018-10-26 09:48:15,089 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:48:15,116 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 23348, None)
[INFO] 2018-10-26 09:48:15,121 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:23348 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 23348, None)
[INFO] 2018-10-26 09:48:15,123 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 23348, None)
[INFO] 2018-10-26 09:48:15,124 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 23348, None)
[INFO] 2018-10-26 09:48:15,373 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:48:15,374 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:48:15,933 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:48:16,350 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:48:16,361 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:48:16,376 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:48:16,391 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:48:16,393 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:48:16,405 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:48:16,413 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:48:16,423 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:48:16,424 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:48:16,425 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d93b1135-2579-4aef-8ce7-ae9d2ff6cd68
[INFO] 2018-10-26 09:48:16,426 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1996572f-6c2f-4f47-89e8-c9f869929be1
[INFO] 2018-10-26 09:48:16,426 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1996572f-6c2f-4f47-89e8-c9f869929be1/pyspark-43652d3c-6e84-40a2-b654-d3219df5505d
[WARN] 2018-10-26 09:49:31,023 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:49:31,805 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:49:31,830 org.apache.spark.SparkContext logInfo - Submitted application: JB_DAY_FLAG_LOGIC_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 09:49:31,973 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:49:31,973 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:49:31,974 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:49:31,974 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:49:31,974 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:49:32,177 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 21940.
[INFO] 2018-10-26 09:49:32,200 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:49:32,219 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:49:32,222 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:49:32,222 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:49:32,231 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-71d71493-65a7-4e7b-9400-9819ffb85de5
[INFO] 2018-10-26 09:49:32,248 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:49:32,261 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:49:32,435 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:49:32,491 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:49:32,581 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572572581
[INFO] 2018-10-26 09:49:32,583 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-aa725d98-087f-4edd-b4ce-e039e94cc767/userFiles-a8648d90-d04e-4d17-bbb2-4edf3d54edac/etl_config.json
[INFO] 2018-10-26 09:49:32,595 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_LOGIC_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_LOGIC_BOOKINGS_TRUNCATE.py with timestamp 1540572572595
[INFO] 2018-10-26 09:49:32,596 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_LOGIC_BOOKINGS_TRUNCATE.py to /tmp/spark-aa725d98-087f-4edd-b4ce-e039e94cc767/userFiles-a8648d90-d04e-4d17-bbb2-4edf3d54edac/JB_DAY_FLAG_LOGIC_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 09:49:32,600 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572572600
[INFO] 2018-10-26 09:49:32,600 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-aa725d98-087f-4edd-b4ce-e039e94cc767/userFiles-a8648d90-d04e-4d17-bbb2-4edf3d54edac/packages.zip
[INFO] 2018-10-26 09:49:32,665 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:49:32,691 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 19069.
[INFO] 2018-10-26 09:49:32,692 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:19069
[INFO] 2018-10-26 09:49:32,700 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:49:32,766 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 19069, None)
[INFO] 2018-10-26 09:49:32,771 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:19069 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 19069, None)
[INFO] 2018-10-26 09:49:32,774 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 19069, None)
[INFO] 2018-10-26 09:49:32,774 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 19069, None)
[INFO] 2018-10-26 09:49:33,044 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:49:33,045 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:49:33,600 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:49:34,010 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:49:34,023 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:49:34,035 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:49:34,047 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:49:34,048 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:49:34,055 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:49:34,060 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:49:34,069 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:49:34,070 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:49:34,071 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-aa725d98-087f-4edd-b4ce-e039e94cc767/pyspark-3467cc3a-9568-4773-bfdf-6dd6a6740e50
[INFO] 2018-10-26 09:49:34,071 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-42547fe5-d083-457f-a304-752670b84f9b
[INFO] 2018-10-26 09:49:34,072 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-aa725d98-087f-4edd-b4ce-e039e94cc767
[WARN] 2018-10-26 09:49:46,527 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:49:47,278 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:49:47,304 org.apache.spark.SparkContext logInfo - Submitted application: JB_DAY_FLAG_BOOKINGS
[INFO] 2018-10-26 09:49:47,477 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:49:47,477 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:49:47,477 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:49:47,478 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:49:47,478 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:49:47,671 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 12048.
[INFO] 2018-10-26 09:49:47,696 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:49:47,715 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:49:47,718 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:49:47,719 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:49:47,728 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-48721822-a123-4c4f-9197-365142d64286
[INFO] 2018-10-26 09:49:47,745 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:49:47,759 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:49:47,936 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:49:47,984 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:49:48,093 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572588092
[INFO] 2018-10-26 09:49:48,095 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-3c56772b-91ee-47c9-9f14-fd41bc123658/userFiles-cfbb9784-777a-4a0e-8240-2a16d67394f9/etl_config.json
[INFO] 2018-10-26 09:49:48,109 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_BOOKINGS.py with timestamp 1540572588109
[INFO] 2018-10-26 09:49:48,109 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_DAY_FLAG_BOOKINGS.py to /tmp/spark-3c56772b-91ee-47c9-9f14-fd41bc123658/userFiles-cfbb9784-777a-4a0e-8240-2a16d67394f9/JB_DAY_FLAG_BOOKINGS.py
[INFO] 2018-10-26 09:49:48,114 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572588114
[INFO] 2018-10-26 09:49:48,114 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-3c56772b-91ee-47c9-9f14-fd41bc123658/userFiles-cfbb9784-777a-4a0e-8240-2a16d67394f9/packages.zip
[INFO] 2018-10-26 09:49:48,187 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:49:48,215 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25587.
[INFO] 2018-10-26 09:49:48,216 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:25587
[INFO] 2018-10-26 09:49:48,218 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:49:48,252 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25587, None)
[INFO] 2018-10-26 09:49:48,259 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:25587 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 25587, None)
[INFO] 2018-10-26 09:49:48,264 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25587, None)
[INFO] 2018-10-26 09:49:48,264 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 25587, None)
[INFO] 2018-10-26 09:49:48,542 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:49:48,543 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:49:49,080 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:49:49,307 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:49:49,321 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:49:49,334 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:49:49,343 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:49:49,343 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:49:49,351 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:49:49,355 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:49:49,364 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:49:49,365 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:49:49,366 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5c569b47-0e33-4d1c-a3b0-a31b129410a7
[INFO] 2018-10-26 09:49:49,367 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3c56772b-91ee-47c9-9f14-fd41bc123658
[INFO] 2018-10-26 09:49:49,367 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3c56772b-91ee-47c9-9f14-fd41bc123658/pyspark-ea7b19e7-dd7f-489b-87ea-b4e5afcdcd82
[WARN] 2018-10-26 09:50:19,463 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:50:20,225 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:50:20,251 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 09:50:20,449 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:50:20,449 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:50:20,450 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:50:20,450 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:50:20,450 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:50:20,699 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 18373.
[INFO] 2018-10-26 09:50:20,731 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:50:20,754 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:50:20,757 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:50:20,758 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:50:20,769 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-d892e34c-d322-4868-bef0-3efc4daf84e4
[INFO] 2018-10-26 09:50:20,792 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:50:20,811 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:50:21,008 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:50:21,057 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:50:21,149 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572621149
[INFO] 2018-10-26 09:50:21,151 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-b1196a6f-0d38-409f-8d76-7fa4c134d17b/userFiles-b915c2d0-3840-45c3-81fe-90e0dbed1c36/etl_config.json
[INFO] 2018-10-26 09:50:21,166 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540572621166
[INFO] 2018-10-26 09:50:21,167 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-b1196a6f-0d38-409f-8d76-7fa4c134d17b/userFiles-b915c2d0-3840-45c3-81fe-90e0dbed1c36/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 09:50:21,171 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572621171
[INFO] 2018-10-26 09:50:21,172 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-b1196a6f-0d38-409f-8d76-7fa4c134d17b/userFiles-b915c2d0-3840-45c3-81fe-90e0dbed1c36/packages.zip
[INFO] 2018-10-26 09:50:21,235 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:50:21,256 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10054.
[INFO] 2018-10-26 09:50:21,257 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:10054
[INFO] 2018-10-26 09:50:21,258 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:50:21,286 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 10054, None)
[INFO] 2018-10-26 09:50:21,292 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:10054 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 10054, None)
[INFO] 2018-10-26 09:50:21,296 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 10054, None)
[INFO] 2018-10-26 09:50:21,297 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 10054, None)
[INFO] 2018-10-26 09:50:21,563 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:50:21,564 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:50:22,108 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:50:22,370 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:50:22,383 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:50:22,396 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:50:22,405 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:50:22,405 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:50:22,415 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:50:22,420 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:50:22,430 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:50:22,432 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:50:22,434 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b1196a6f-0d38-409f-8d76-7fa4c134d17b
[INFO] 2018-10-26 09:50:22,435 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b1196a6f-0d38-409f-8d76-7fa4c134d17b/pyspark-6e427f0f-6c27-4574-ad36-571e07eaa6c1
[INFO] 2018-10-26 09:50:22,435 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a362cab0-0671-4f49-82a3-d3bd0c2a39cb
[WARN] 2018-10-26 09:51:01,401 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:51:02,178 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:51:02,205 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-26 09:51:02,457 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:51:02,458 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:51:02,458 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:51:02,459 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:51:02,459 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:51:02,656 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 10726.
[INFO] 2018-10-26 09:51:02,683 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:51:02,705 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:51:02,708 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:51:02,709 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:51:02,719 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1e48aa1b-0643-4e5a-8bc5-47d0116a4ec3
[INFO] 2018-10-26 09:51:02,736 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:51:02,749 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 09:51:02,924 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 09:51:02,984 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 09:51:03,123 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572663122
[INFO] 2018-10-26 09:51:03,128 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-c82655a4-9cda-4698-8b51-b189a746e20a/userFiles-0bcfcbc0-5a3c-4962-92de-277a02161367/etl_config.json
[INFO] 2018-10-26 09:51:03,148 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540572663148
[INFO] 2018-10-26 09:51:03,149 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-c82655a4-9cda-4698-8b51-b189a746e20a/userFiles-0bcfcbc0-5a3c-4962-92de-277a02161367/JB_STG_BOOKINGS.py
[INFO] 2018-10-26 09:51:03,155 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572663155
[INFO] 2018-10-26 09:51:03,155 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-c82655a4-9cda-4698-8b51-b189a746e20a/userFiles-0bcfcbc0-5a3c-4962-92de-277a02161367/packages.zip
[INFO] 2018-10-26 09:51:03,222 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:51:03,245 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18460.
[INFO] 2018-10-26 09:51:03,246 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:18460
[INFO] 2018-10-26 09:51:03,248 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:51:03,279 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 09:51:03,285 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:18460 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 09:51:03,289 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 09:51:03,290 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 09:51:03,554 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:51:03,554 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:51:04,085 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[WARN] 2018-10-26 09:54:15,056 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:54:15,811 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:54:15,836 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 09:54:16,070 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:54:16,070 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:54:16,071 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:54:16,071 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:54:16,071 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:54:16,301 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11099.
[INFO] 2018-10-26 09:54:16,328 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:54:16,355 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:54:16,358 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:54:16,359 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:54:16,369 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-5b42808d-ff51-4dad-9394-c39d88fed8b7
[INFO] 2018-10-26 09:54:16,389 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:54:16,404 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 09:54:16,609 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-26 09:54:16,614 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-26 09:54:16,672 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 09:54:16,803 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572856802
[INFO] 2018-10-26 09:54:16,805 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-dcf977b7-809d-4cea-b366-1ffbafb5927c/userFiles-6cf30d72-1709-4ee7-b461-0f5ee43293e1/etl_config.json
[INFO] 2018-10-26 09:54:16,820 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py with timestamp 1540572856820
[INFO] 2018-10-26 09:54:16,821 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py to /tmp/spark-dcf977b7-809d-4cea-b366-1ffbafb5927c/userFiles-6cf30d72-1709-4ee7-b461-0f5ee43293e1/JB_WORK_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 09:54:16,826 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572856826
[INFO] 2018-10-26 09:54:16,827 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-dcf977b7-809d-4cea-b366-1ffbafb5927c/userFiles-6cf30d72-1709-4ee7-b461-0f5ee43293e1/packages.zip
[INFO] 2018-10-26 09:54:16,899 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:54:16,925 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9508.
[INFO] 2018-10-26 09:54:16,927 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:9508
[INFO] 2018-10-26 09:54:16,928 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:54:16,964 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 9508, None)
[INFO] 2018-10-26 09:54:16,972 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:9508 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 9508, None)
[INFO] 2018-10-26 09:54:16,975 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 9508, None)
[INFO] 2018-10-26 09:54:16,976 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 9508, None)
[INFO] 2018-10-26 09:54:17,270 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:54:17,270 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:54:17,845 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:54:19,035 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:54:19,048 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 09:54:19,062 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:54:19,075 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:54:19,076 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:54:19,089 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:54:19,094 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:54:19,106 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:54:19,107 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:54:19,109 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c73458b0-e0e0-408a-9437-ef267be0f9af
[INFO] 2018-10-26 09:54:19,110 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-dcf977b7-809d-4cea-b366-1ffbafb5927c/pyspark-627003df-6a5d-4c72-9579-1ad3da9ff4ae
[INFO] 2018-10-26 09:54:19,111 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-dcf977b7-809d-4cea-b366-1ffbafb5927c
[WARN] 2018-10-26 09:54:43,773 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 09:54:44,625 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 09:54:44,647 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS
[INFO] 2018-10-26 09:54:44,806 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 09:54:44,806 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 09:54:44,806 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 09:54:44,807 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 09:54:44,807 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 09:54:45,004 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 9584.
[INFO] 2018-10-26 09:54:45,027 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 09:54:45,046 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 09:54:45,049 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 09:54:45,050 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 09:54:45,058 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-72d3ca5c-fc4c-4fad-b2e8-2ea092e64a63
[INFO] 2018-10-26 09:54:45,076 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 09:54:45,089 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 09:54:45,256 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-26 09:54:45,261 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-26 09:54:45,314 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 09:54:45,418 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540572885417
[INFO] 2018-10-26 09:54:45,420 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-8a3bc29b-d398-4180-90d8-ca4dcbd35a3b/userFiles-478c9a12-d066-4c7c-89b7-b34c1fc41816/etl_config.json
[INFO] 2018-10-26 09:54:45,435 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py with timestamp 1540572885435
[INFO] 2018-10-26 09:54:45,436 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py to /tmp/spark-8a3bc29b-d398-4180-90d8-ca4dcbd35a3b/userFiles-478c9a12-d066-4c7c-89b7-b34c1fc41816/JB_WORK_BOOKINGS.py
[INFO] 2018-10-26 09:54:45,442 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540572885442
[INFO] 2018-10-26 09:54:45,443 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-8a3bc29b-d398-4180-90d8-ca4dcbd35a3b/userFiles-478c9a12-d066-4c7c-89b7-b34c1fc41816/packages.zip
[INFO] 2018-10-26 09:54:45,517 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 09:54:45,546 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18830.
[INFO] 2018-10-26 09:54:45,547 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:18830
[INFO] 2018-10-26 09:54:45,549 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 09:54:45,582 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18830, None)
[INFO] 2018-10-26 09:54:45,589 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:18830 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 18830, None)
[INFO] 2018-10-26 09:54:45,594 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18830, None)
[INFO] 2018-10-26 09:54:45,596 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 18830, None)
[INFO] 2018-10-26 09:54:45,885 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 09:54:45,886 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 09:54:46,405 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 09:54:57,012 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 09:54:57,025 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 09:54:57,038 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 09:54:57,048 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 09:54:57,049 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 09:54:57,061 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 09:54:57,065 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 09:54:57,073 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 09:54:57,074 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 09:54:57,075 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8a3bc29b-d398-4180-90d8-ca4dcbd35a3b
[INFO] 2018-10-26 09:54:57,076 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8a3bc29b-d398-4180-90d8-ca4dcbd35a3b/pyspark-0b04cd6f-cf78-4a3f-a259-1fb4bb5bbda5
[INFO] 2018-10-26 09:54:57,076 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5dfa165b-bde5-40ad-91c2-92a5e39b1e4d
[WARN] 2018-10-26 10:06:01,535 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 10:06:02,389 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 10:06:02,410 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-26 10:06:02,600 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 10:06:02,600 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 10:06:02,600 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 10:06:02,601 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 10:06:02,601 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 10:06:02,804 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 23132.
[INFO] 2018-10-26 10:06:02,829 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 10:06:02,849 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 10:06:02,852 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 10:06:02,852 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 10:06:02,861 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-4288823b-0ed7-4a04-88c1-b0911645e4ed
[INFO] 2018-10-26 10:06:02,879 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 10:06:02,893 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 10:06:03,074 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-26 10:06:03,082 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-26 10:06:03,140 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 10:06:03,247 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540573563246
[INFO] 2018-10-26 10:06:03,249 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-f58660ce-9d46-4cb0-80dc-aeb730e7ae28/userFiles-10006788-c60a-4bbe-913e-c4ecdf918214/etl_config.json
[INFO] 2018-10-26 10:06:03,262 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540573563262
[INFO] 2018-10-26 10:06:03,263 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-f58660ce-9d46-4cb0-80dc-aeb730e7ae28/userFiles-10006788-c60a-4bbe-913e-c4ecdf918214/JB_STG_BOOKINGS.py
[INFO] 2018-10-26 10:06:03,267 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540573563267
[INFO] 2018-10-26 10:06:03,268 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-f58660ce-9d46-4cb0-80dc-aeb730e7ae28/userFiles-10006788-c60a-4bbe-913e-c4ecdf918214/packages.zip
[INFO] 2018-10-26 10:06:03,341 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 10:06:03,365 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14143.
[INFO] 2018-10-26 10:06:03,366 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:14143
[INFO] 2018-10-26 10:06:03,367 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 10:06:03,402 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 10:06:03,407 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:14143 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 10:06:03,410 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 10:06:03,411 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 10:06:03,697 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 10:06:03,698 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 10:06:04,319 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[WARN] 2018-10-26 10:16:16,021 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 10:16:16,815 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 10:16:16,838 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP_TRUNCATE
[INFO] 2018-10-26 10:16:17,048 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 10:16:17,048 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 10:16:17,049 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 10:16:17,049 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 10:16:17,061 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 10:16:17,276 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 22149.
[INFO] 2018-10-26 10:16:17,301 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 10:16:17,320 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 10:16:17,323 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 10:16:17,324 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 10:16:17,332 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-15feb183-64be-4be2-83e6-fab5b595de32
[INFO] 2018-10-26 10:16:17,349 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 10:16:17,362 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 10:16:17,540 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 10:16:17,541 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-26 10:16:17,547 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-26 10:16:17,603 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:16:17,704 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540574177704
[INFO] 2018-10-26 10:16:17,706 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-67f46368-7450-46b6-8eed-0a013bb3fde6/userFiles-79df45d5-ec48-4388-a726-3abcb5af526c/etl_config.json
[INFO] 2018-10-26 10:16:17,720 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py with timestamp 1540574177720
[INFO] 2018-10-26 10:16:17,721 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py to /tmp/spark-67f46368-7450-46b6-8eed-0a013bb3fde6/userFiles-79df45d5-ec48-4388-a726-3abcb5af526c/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py
[INFO] 2018-10-26 10:16:17,726 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540574177726
[INFO] 2018-10-26 10:16:17,726 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-67f46368-7450-46b6-8eed-0a013bb3fde6/userFiles-79df45d5-ec48-4388-a726-3abcb5af526c/packages.zip
[INFO] 2018-10-26 10:16:17,799 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 10:16:17,825 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 17903.
[INFO] 2018-10-26 10:16:17,826 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:17903
[INFO] 2018-10-26 10:16:17,828 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 10:16:17,864 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17903, None)
[INFO] 2018-10-26 10:16:17,869 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:17903 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 17903, None)
[INFO] 2018-10-26 10:16:17,874 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17903, None)
[INFO] 2018-10-26 10:16:17,875 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 17903, None)
[INFO] 2018-10-26 10:16:18,187 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 10:16:18,188 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 10:16:18,727 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 10:16:21,805 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 10:16:21,816 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:16:21,830 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 10:16:21,839 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 10:16:21,840 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 10:16:21,848 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 10:16:21,854 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 10:16:21,862 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 10:16:21,862 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 10:16:21,863 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-67f46368-7450-46b6-8eed-0a013bb3fde6/pyspark-35494768-0fba-4c44-ab6e-e75ca4ca23cc
[INFO] 2018-10-26 10:16:21,864 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-786ae96c-1cef-4e36-83f3-d31e56731e02
[INFO] 2018-10-26 10:16:21,864 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-67f46368-7450-46b6-8eed-0a013bb3fde6
[WARN] 2018-10-26 10:16:48,250 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 10:16:49,053 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 10:16:49,076 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP
[INFO] 2018-10-26 10:16:49,315 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 10:16:49,316 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 10:16:49,316 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 10:16:49,316 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 10:16:49,317 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 10:16:49,516 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 21497.
[INFO] 2018-10-26 10:16:49,544 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 10:16:49,568 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 10:16:49,571 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 10:16:49,572 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 10:16:49,582 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-138b84e1-cace-45cd-b895-c0388346ccd7
[INFO] 2018-10-26 10:16:49,602 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 10:16:49,618 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 10:16:49,832 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 10:16:49,832 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-26 10:16:49,839 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-26 10:16:49,917 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:16:50,035 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540574210034
[INFO] 2018-10-26 10:16:50,038 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-b2b4777c-6bde-4a58-9072-3e3b18559d28/userFiles-e1cf0270-5deb-4fbe-bf88-194fcac80157/etl_config.json
[INFO] 2018-10-26 10:16:50,053 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540574210053
[INFO] 2018-10-26 10:16:50,054 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py to /tmp/spark-b2b4777c-6bde-4a58-9072-3e3b18559d28/userFiles-e1cf0270-5deb-4fbe-bf88-194fcac80157/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 10:16:50,059 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540574210059
[INFO] 2018-10-26 10:16:50,060 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-b2b4777c-6bde-4a58-9072-3e3b18559d28/userFiles-e1cf0270-5deb-4fbe-bf88-194fcac80157/packages.zip
[INFO] 2018-10-26 10:16:50,133 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 10:16:50,159 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 21717.
[INFO] 2018-10-26 10:16:50,160 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:21717
[INFO] 2018-10-26 10:16:50,161 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 10:16:50,193 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 21717, None)
[INFO] 2018-10-26 10:16:50,199 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:21717 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 21717, None)
[INFO] 2018-10-26 10:16:50,203 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 21717, None)
[INFO] 2018-10-26 10:16:50,204 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 21717, None)
[INFO] 2018-10-26 10:16:50,507 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 10:16:50,508 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 10:16:51,094 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 10:16:51,139 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 10:16:51,152 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:16:51,167 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 10:16:51,183 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 10:16:51,184 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 10:16:51,196 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 10:16:51,202 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 10:16:51,218 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 10:16:51,219 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 10:16:51,220 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-59fe2d41-4d85-4915-9a50-3f2fc47dfa48
[INFO] 2018-10-26 10:16:51,221 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b2b4777c-6bde-4a58-9072-3e3b18559d28
[INFO] 2018-10-26 10:16:51,221 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b2b4777c-6bde-4a58-9072-3e3b18559d28/pyspark-225b9c65-b4b2-4b44-a4b3-a1d19a8fa7f9
[WARN] 2018-10-26 10:19:50,983 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 10:19:51,735 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 10:19:51,757 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP
[INFO] 2018-10-26 10:19:51,897 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 10:19:51,898 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 10:19:51,898 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 10:19:51,898 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 10:19:51,899 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 10:19:52,120 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 32478.
[INFO] 2018-10-26 10:19:52,147 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 10:19:52,168 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 10:19:52,171 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 10:19:52,171 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 10:19:52,180 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-0101a131-99ee-42ee-9384-42518bb28210
[INFO] 2018-10-26 10:19:52,197 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 10:19:52,210 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 10:19:52,402 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 10:19:52,402 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-26 10:19:52,409 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-26 10:19:52,466 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:19:52,567 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540574392567
[INFO] 2018-10-26 10:19:52,570 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-695faacf-898f-46c0-9597-b3b0e7206f08/userFiles-1c0934a3-2c4b-4934-b9be-ee91d16a4c2f/etl_config.json
[INFO] 2018-10-26 10:19:52,583 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540574392583
[INFO] 2018-10-26 10:19:52,584 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py to /tmp/spark-695faacf-898f-46c0-9597-b3b0e7206f08/userFiles-1c0934a3-2c4b-4934-b9be-ee91d16a4c2f/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 10:19:52,589 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540574392589
[INFO] 2018-10-26 10:19:52,589 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-695faacf-898f-46c0-9597-b3b0e7206f08/userFiles-1c0934a3-2c4b-4934-b9be-ee91d16a4c2f/packages.zip
[INFO] 2018-10-26 10:19:52,666 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 10:19:52,694 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27262.
[INFO] 2018-10-26 10:19:52,696 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:27262
[INFO] 2018-10-26 10:19:52,698 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 10:19:52,730 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 27262, None)
[INFO] 2018-10-26 10:19:52,737 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:27262 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 27262, None)
[INFO] 2018-10-26 10:19:52,742 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 27262, None)
[INFO] 2018-10-26 10:19:52,742 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 27262, None)
[INFO] 2018-10-26 10:19:53,041 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 10:19:53,041 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 10:19:53,568 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 10:19:56,595 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 200.843778 ms
[INFO] 2018-10-26 10:19:56,752 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 10:19:56,776 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 10:19:56,777 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 10:19:56,777 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 10:19:56,779 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 10:19:56,785 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 10:19:56,860 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 10:19:56,897 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 10:19:56,901 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:27262 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 10:19:56,904 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 10:19:56,919 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 10:19:56,920 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 10:19:56,972 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 10:19:56,984 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 10:19:56,989 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540574392567
[INFO] 2018-10-26 10:19:57,032 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-695faacf-898f-46c0-9597-b3b0e7206f08/userFiles-1c0934a3-2c4b-4934-b9be-ee91d16a4c2f/etl_config.json
[INFO] 2018-10-26 10:19:57,038 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540574392583
[INFO] 2018-10-26 10:19:57,040 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py has been previously copied to /tmp/spark-695faacf-898f-46c0-9597-b3b0e7206f08/userFiles-1c0934a3-2c4b-4934-b9be-ee91d16a4c2f/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 10:19:57,047 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540574392589
[INFO] 2018-10-26 10:19:57,048 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-695faacf-898f-46c0-9597-b3b0e7206f08/userFiles-1c0934a3-2c4b-4934-b9be-ee91d16a4c2f/packages.zip
[INFO] 2018-10-26 10:19:57,854 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 10:19:57,898 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 545, boot = 424, init = 120, finish = 1
[INFO] 2018-10-26 10:19:57,918 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 10:19:57,934 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 976 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 10:19:57,939 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 10:19:57,952 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.140 s
[INFO] 2018-10-26 10:19:57,958 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.205922 s
[INFO] 2018-10-26 10:19:58,332 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340
[INFO] 2018-10-26 10:19:58,334 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) with 1 output partitions
[INFO] 2018-10-26 10:19:58,334 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340)
[INFO] 2018-10-26 10:19:58,334 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 10:19:58,335 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 10:19:58,336 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340), which has no missing parents
[INFO] 2018-10-26 10:19:58,342 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 10:19:58,344 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 10:19:58,345 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:27262 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 10:19:58,347 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 10:19:58,348 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 10:19:58,348 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 10:19:58,350 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 10:19:58,350 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 10:19:58,664 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 10:19:58,667 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 129, boot = -633, init = 761, finish = 1
[INFO] 2018-10-26 10:19:58,670 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1668 bytes result sent to driver
[INFO] 2018-10-26 10:19:58,673 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 324 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 10:19:58,674 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 10:19:58,675 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) finished in 0.337 s
[INFO] 2018-10-26 10:19:58,676 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340, took 0.343477 s
[INFO] 2018-10-26 10:19:58,714 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 10:19:58,727 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:19:58,746 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 10:19:58,765 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 10:19:58,766 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 10:19:58,775 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 10:19:58,780 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 10:19:58,796 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 10:19:58,797 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 10:19:58,799 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f586255b-478a-4e2c-93df-a5261667f601
[INFO] 2018-10-26 10:19:58,800 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-695faacf-898f-46c0-9597-b3b0e7206f08/pyspark-253f041e-f345-4ac4-bebd-9be3583ec560
[INFO] 2018-10-26 10:19:58,801 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-695faacf-898f-46c0-9597-b3b0e7206f08
[WARN] 2018-10-26 10:20:51,128 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 10:20:51,891 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 10:20:51,918 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP
[INFO] 2018-10-26 10:20:52,129 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 10:20:52,130 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 10:20:52,130 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 10:20:52,131 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 10:20:52,131 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 10:20:52,341 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 26655.
[INFO] 2018-10-26 10:20:52,366 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 10:20:52,388 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 10:20:52,391 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 10:20:52,392 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 10:20:52,402 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-f4fe6e37-78ef-4500-920e-0ee704ad84cb
[INFO] 2018-10-26 10:20:52,423 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 10:20:52,441 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 10:20:52,642 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 10:20:52,642 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-26 10:20:52,648 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-26 10:20:52,711 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:20:52,815 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540574452815
[INFO] 2018-10-26 10:20:52,817 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-6c44cfce-3c5f-4469-b51b-19921ebd6b2c/userFiles-15a64035-76f5-4834-96e9-95bcde5555ed/etl_config.json
[INFO] 2018-10-26 10:20:52,831 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540574452831
[INFO] 2018-10-26 10:20:52,831 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py to /tmp/spark-6c44cfce-3c5f-4469-b51b-19921ebd6b2c/userFiles-15a64035-76f5-4834-96e9-95bcde5555ed/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 10:20:52,835 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540574452835
[INFO] 2018-10-26 10:20:52,836 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-6c44cfce-3c5f-4469-b51b-19921ebd6b2c/userFiles-15a64035-76f5-4834-96e9-95bcde5555ed/packages.zip
[INFO] 2018-10-26 10:20:52,908 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 10:20:52,935 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12637.
[INFO] 2018-10-26 10:20:52,936 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:12637
[INFO] 2018-10-26 10:20:52,938 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 10:20:52,971 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 12637, None)
[INFO] 2018-10-26 10:20:52,978 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:12637 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 12637, None)
[INFO] 2018-10-26 10:20:52,982 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 12637, None)
[INFO] 2018-10-26 10:20:52,983 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 12637, None)
[INFO] 2018-10-26 10:20:53,255 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 10:20:53,255 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 10:20:53,811 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 10:20:56,781 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 211.403854 ms
[INFO] 2018-10-26 10:20:56,934 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 10:20:56,954 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 10:20:56,955 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 10:20:56,956 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 10:20:56,957 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 10:20:56,966 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 10:20:57,035 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 10:20:57,070 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 10:20:57,074 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:12637 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 10:20:57,077 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 10:20:57,091 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 10:20:57,092 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 10:20:57,144 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 10:20:57,157 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 10:20:57,163 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540574452815
[INFO] 2018-10-26 10:20:57,196 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-6c44cfce-3c5f-4469-b51b-19921ebd6b2c/userFiles-15a64035-76f5-4834-96e9-95bcde5555ed/etl_config.json
[INFO] 2018-10-26 10:20:57,202 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540574452831
[INFO] 2018-10-26 10:20:57,203 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py has been previously copied to /tmp/spark-6c44cfce-3c5f-4469-b51b-19921ebd6b2c/userFiles-15a64035-76f5-4834-96e9-95bcde5555ed/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 10:20:57,208 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540574452835
[INFO] 2018-10-26 10:20:57,210 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-6c44cfce-3c5f-4469-b51b-19921ebd6b2c/userFiles-15a64035-76f5-4834-96e9-95bcde5555ed/packages.zip
[INFO] 2018-10-26 10:20:58,065 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 10:20:58,099 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 525, boot = 402, init = 122, finish = 1
[INFO] 2018-10-26 10:20:58,115 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 10:20:58,134 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1001 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 10:20:58,138 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 10:20:58,146 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.160 s
[INFO] 2018-10-26 10:20:58,151 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.215838 s
[INFO] 2018-10-26 10:20:58,377 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340
[INFO] 2018-10-26 10:20:58,378 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) with 1 output partitions
[INFO] 2018-10-26 10:20:58,379 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340)
[INFO] 2018-10-26 10:20:58,379 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 10:20:58,379 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 10:20:58,380 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340), which has no missing parents
[INFO] 2018-10-26 10:20:58,384 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 10:20:58,386 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 10:20:58,387 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:12637 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 10:20:58,389 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 10:20:58,390 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 10:20:58,390 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 10:20:58,392 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 10:20:58,392 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 10:20:58,563 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 10:20:58,591 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -444, init = 486, finish = 0
[INFO] 2018-10-26 10:20:58,594 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1668 bytes result sent to driver
[INFO] 2018-10-26 10:20:58,597 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 206 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 10:20:58,598 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 10:20:58,600 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) finished in 0.219 s
[INFO] 2018-10-26 10:20:58,601 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340, took 0.224057 s
[INFO] 2018-10-26 10:20:58,638 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 10:20:58,650 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:20:58,666 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 10:20:58,681 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 10:20:58,682 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 10:20:58,695 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 10:20:58,699 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 10:20:58,712 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 10:20:58,713 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 10:20:58,715 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6c44cfce-3c5f-4469-b51b-19921ebd6b2c
[INFO] 2018-10-26 10:20:58,715 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6c44cfce-3c5f-4469-b51b-19921ebd6b2c/pyspark-1434dfd4-ce84-4b32-b760-2611f016c854
[INFO] 2018-10-26 10:20:58,716 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-45f79757-a6eb-4140-aac1-54a8fc2fbc0b
[WARN] 2018-10-26 10:21:42,236 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 10:21:42,992 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 10:21:43,020 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP
[INFO] 2018-10-26 10:21:43,196 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 10:21:43,197 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 10:21:43,197 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 10:21:43,197 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 10:21:43,197 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 10:21:43,434 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 25990.
[INFO] 2018-10-26 10:21:43,464 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 10:21:43,487 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 10:21:43,491 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 10:21:43,491 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 10:21:43,502 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-0c3ccdf9-332a-421d-96fb-5b4992b06a0c
[INFO] 2018-10-26 10:21:43,524 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 10:21:43,540 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 10:21:43,724 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 10:21:43,725 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-26 10:21:43,731 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-26 10:21:43,794 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:21:43,902 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540574503901
[INFO] 2018-10-26 10:21:43,904 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-0f95ffde-84b3-4e09-93b6-492bcdf24a5b/userFiles-c5541c0a-ccd4-49d4-b2d4-dddc24acb6fc/etl_config.json
[INFO] 2018-10-26 10:21:43,926 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540574503926
[INFO] 2018-10-26 10:21:43,926 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py to /tmp/spark-0f95ffde-84b3-4e09-93b6-492bcdf24a5b/userFiles-c5541c0a-ccd4-49d4-b2d4-dddc24acb6fc/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 10:21:43,931 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540574503931
[INFO] 2018-10-26 10:21:43,931 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-0f95ffde-84b3-4e09-93b6-492bcdf24a5b/userFiles-c5541c0a-ccd4-49d4-b2d4-dddc24acb6fc/packages.zip
[INFO] 2018-10-26 10:21:44,004 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 10:21:44,026 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27798.
[INFO] 2018-10-26 10:21:44,028 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:27798
[INFO] 2018-10-26 10:21:44,029 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 10:21:44,060 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 27798, None)
[INFO] 2018-10-26 10:21:44,067 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:27798 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 27798, None)
[INFO] 2018-10-26 10:21:44,071 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 27798, None)
[INFO] 2018-10-26 10:21:44,072 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 27798, None)
[INFO] 2018-10-26 10:21:44,342 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 10:21:44,343 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 10:21:44,909 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 10:21:47,986 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 210.055067 ms
[INFO] 2018-10-26 10:21:48,134 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 10:21:48,154 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 10:21:48,154 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 10:21:48,155 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 10:21:48,157 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 10:21:48,165 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 10:21:48,240 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 10:21:48,273 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 10:21:48,278 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:27798 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 10:21:48,281 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 10:21:48,298 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 10:21:48,300 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 10:21:48,352 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 10:21:48,366 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 10:21:48,373 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540574503901
[INFO] 2018-10-26 10:21:48,407 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-0f95ffde-84b3-4e09-93b6-492bcdf24a5b/userFiles-c5541c0a-ccd4-49d4-b2d4-dddc24acb6fc/etl_config.json
[INFO] 2018-10-26 10:21:48,412 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540574503926
[INFO] 2018-10-26 10:21:48,413 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py has been previously copied to /tmp/spark-0f95ffde-84b3-4e09-93b6-492bcdf24a5b/userFiles-c5541c0a-ccd4-49d4-b2d4-dddc24acb6fc/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 10:21:48,418 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540574503931
[INFO] 2018-10-26 10:21:48,420 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-0f95ffde-84b3-4e09-93b6-492bcdf24a5b/userFiles-c5541c0a-ccd4-49d4-b2d4-dddc24acb6fc/packages.zip
[INFO] 2018-10-26 10:21:49,171 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 10:21:49,213 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 469, boot = 395, init = 74, finish = 0
[INFO] 2018-10-26 10:21:49,236 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-26 10:21:49,248 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 910 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 10:21:49,253 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 10:21:49,266 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.080 s
[INFO] 2018-10-26 10:21:49,273 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.138872 s
[INFO] 2018-10-26 10:21:49,467 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340
[INFO] 2018-10-26 10:21:49,468 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) with 1 output partitions
[INFO] 2018-10-26 10:21:49,468 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340)
[INFO] 2018-10-26 10:21:49,469 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 10:21:49,469 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 10:21:49,469 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340), which has no missing parents
[INFO] 2018-10-26 10:21:49,473 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 10:21:49,475 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 10:21:49,476 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:27798 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 10:21:49,478 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 10:21:49,479 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 10:21:49,479 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 10:21:49,481 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 10:21:49,482 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 10:21:49,614 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 10:21:49,649 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -384, init = 426, finish = 0
[INFO] 2018-10-26 10:21:49,652 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1668 bytes result sent to driver
[INFO] 2018-10-26 10:21:49,656 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 175 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 10:21:49,656 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 10:21:49,657 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) finished in 0.186 s
[INFO] 2018-10-26 10:21:49,658 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340, took 0.191303 s
[INFO] 2018-10-26 10:27:13,735 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 10:27:13,750 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:27:13,772 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 10:27:13,795 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 10:27:13,795 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 10:27:13,796 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 10:27:13,800 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 10:27:13,810 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 10:27:13,811 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 10:27:13,812 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0f95ffde-84b3-4e09-93b6-492bcdf24a5b/pyspark-3636a278-a52a-4582-afe5-00919da568c8
[INFO] 2018-10-26 10:27:13,813 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0f95ffde-84b3-4e09-93b6-492bcdf24a5b
[INFO] 2018-10-26 10:27:13,813 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5be5c016-8834-4be3-8ecd-a86799058e8e
[WARN] 2018-10-26 10:30:32,650 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 10:30:33,479 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 10:30:33,504 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-26 10:30:33,683 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 10:30:33,684 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 10:30:33,684 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 10:30:33,684 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 10:30:33,685 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 10:30:33,886 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 28546.
[INFO] 2018-10-26 10:30:33,913 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 10:30:33,932 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 10:30:33,935 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 10:30:33,935 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 10:30:33,944 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-4b801d82-16d6-4e4a-8b65-9efb033cde81
[INFO] 2018-10-26 10:30:33,961 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 10:30:33,975 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 10:30:34,142 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 10:30:34,143 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-26 10:30:34,148 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-26 10:30:34,203 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 10:30:34,301 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540575034300
[INFO] 2018-10-26 10:30:34,303 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-829beb32-0661-4aeb-a9d2-eae596bc3be6/userFiles-5e154d8d-8bce-49f9-83b3-3f0f6cf0fddc/etl_config.json
[INFO] 2018-10-26 10:30:34,316 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540575034316
[INFO] 2018-10-26 10:30:34,316 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-829beb32-0661-4aeb-a9d2-eae596bc3be6/userFiles-5e154d8d-8bce-49f9-83b3-3f0f6cf0fddc/JB_STG_BOOKINGS.py
[INFO] 2018-10-26 10:30:34,320 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540575034320
[INFO] 2018-10-26 10:30:34,321 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-829beb32-0661-4aeb-a9d2-eae596bc3be6/userFiles-5e154d8d-8bce-49f9-83b3-3f0f6cf0fddc/packages.zip
[INFO] 2018-10-26 10:30:34,388 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 10:30:34,412 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35283.
[INFO] 2018-10-26 10:30:34,413 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:35283
[INFO] 2018-10-26 10:30:34,415 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 10:30:34,447 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 10:30:34,454 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:35283 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 10:30:34,458 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 10:30:34,459 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 10:30:34,729 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 10:30:34,729 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 10:30:35,203 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[WARN] 2018-10-26 11:02:35,589 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 11:02:36,422 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 11:02:36,456 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-26 11:02:36,649 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 11:02:36,649 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 11:02:36,650 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 11:02:36,650 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 11:02:36,650 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 11:02:36,838 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 25788.
[INFO] 2018-10-26 11:02:36,862 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 11:02:36,880 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 11:02:36,883 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 11:02:36,883 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 11:02:36,892 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-3a542a70-5b65-477b-a0b7-52c83fee7c38
[INFO] 2018-10-26 11:02:36,912 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 11:02:36,928 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 11:02:37,115 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 11:02:37,115 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-26 11:02:37,116 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[INFO] 2018-10-26 11:02:37,122 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4043.
[INFO] 2018-10-26 11:02:37,171 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4043
[INFO] 2018-10-26 11:02:37,259 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540576957258
[INFO] 2018-10-26 11:02:37,261 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-dffe247f-f421-4ae7-83f7-ffaf52268143/userFiles-943de5ba-a388-4605-9665-3849f9a4af8d/etl_config.json
[INFO] 2018-10-26 11:02:37,274 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540576957274
[INFO] 2018-10-26 11:02:37,275 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-dffe247f-f421-4ae7-83f7-ffaf52268143/userFiles-943de5ba-a388-4605-9665-3849f9a4af8d/JB_STG_BOOKINGS.py
[INFO] 2018-10-26 11:02:37,279 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540576957279
[INFO] 2018-10-26 11:02:37,279 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-dffe247f-f421-4ae7-83f7-ffaf52268143/userFiles-943de5ba-a388-4605-9665-3849f9a4af8d/packages.zip
[INFO] 2018-10-26 11:02:37,354 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 11:02:37,385 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18181.
[INFO] 2018-10-26 11:02:37,386 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:18181
[INFO] 2018-10-26 11:02:37,388 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 11:02:37,425 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18181, None)
[INFO] 2018-10-26 11:02:37,432 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:18181 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 18181, None)
[INFO] 2018-10-26 11:02:37,435 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18181, None)
[INFO] 2018-10-26 11:02:37,437 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 18181, None)
[INFO] 2018-10-26 11:02:37,730 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 11:02:37,731 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 11:02:38,264 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[WARN] 2018-10-26 11:25:01,487 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 11:25:02,249 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 11:25:02,272 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 11:25:02,542 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 11:25:02,542 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 11:25:02,543 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 11:25:02,543 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 11:25:02,543 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 11:25:02,784 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 36126.
[INFO] 2018-10-26 11:25:02,814 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 11:25:02,836 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 11:25:02,839 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 11:25:02,840 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 11:25:02,851 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-8ddd5562-b95c-439e-bbcc-898e03eeb641
[INFO] 2018-10-26 11:25:02,875 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 11:25:02,891 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 11:25:03,088 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 11:25:03,088 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-26 11:25:03,089 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-26 11:25:03,089 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[INFO] 2018-10-26 11:25:03,096 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4044.
[INFO] 2018-10-26 11:25:03,170 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4044
[INFO] 2018-10-26 11:25:03,281 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540578303281
[INFO] 2018-10-26 11:25:03,284 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-12934e85-9ffa-4f25-b1c7-137cd662158f/userFiles-829ac349-0b33-475d-9212-8c67d9206083/etl_config.json
[INFO] 2018-10-26 11:25:03,296 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540578303296
[INFO] 2018-10-26 11:25:03,296 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-12934e85-9ffa-4f25-b1c7-137cd662158f/userFiles-829ac349-0b33-475d-9212-8c67d9206083/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 11:25:03,301 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540578303301
[INFO] 2018-10-26 11:25:03,301 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-12934e85-9ffa-4f25-b1c7-137cd662158f/userFiles-829ac349-0b33-475d-9212-8c67d9206083/packages.zip
[INFO] 2018-10-26 11:25:03,379 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 11:25:03,404 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 16431.
[INFO] 2018-10-26 11:25:03,405 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:16431
[INFO] 2018-10-26 11:25:03,406 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 11:25:03,438 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 16431, None)
[INFO] 2018-10-26 11:25:03,446 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:16431 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 16431, None)
[INFO] 2018-10-26 11:25:03,451 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 16431, None)
[INFO] 2018-10-26 11:25:03,452 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 16431, None)
[INFO] 2018-10-26 11:25:03,729 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 11:25:03,729 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 11:25:04,319 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 11:25:05,185 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 11:25:05,196 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4044
[INFO] 2018-10-26 11:25:05,208 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 11:25:05,217 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 11:25:05,217 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 11:25:05,225 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 11:25:05,232 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 11:25:05,245 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 11:25:05,246 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 11:25:05,248 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-12934e85-9ffa-4f25-b1c7-137cd662158f/pyspark-729b2378-4507-41b4-af48-fc15853e1395
[INFO] 2018-10-26 11:25:05,249 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-73e2b854-f784-4fe2-981d-659bba3ca5a8
[INFO] 2018-10-26 11:25:05,250 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-12934e85-9ffa-4f25-b1c7-137cd662158f
[WARN] 2018-10-26 11:25:10,728 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 11:25:11,469 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 11:25:11,493 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 11:25:11,660 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 11:25:11,660 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 11:25:11,661 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 11:25:11,661 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 11:25:11,672 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 11:25:11,925 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 35039.
[INFO] 2018-10-26 11:25:11,956 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 11:25:11,982 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 11:25:11,985 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 11:25:11,986 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 11:25:11,998 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1f28a26d-c1ca-43f0-981b-da4c4c819190
[INFO] 2018-10-26 11:25:12,019 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 11:25:12,037 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 11:25:12,219 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 11:25:12,220 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-26 11:25:12,220 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-26 11:25:12,221 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[INFO] 2018-10-26 11:25:12,227 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4044.
[INFO] 2018-10-26 11:25:12,277 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4044
[INFO] 2018-10-26 11:25:12,369 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540578312368
[INFO] 2018-10-26 11:25:12,371 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-712836af-a43a-4349-8363-8aa29ca0d0aa/userFiles-92e656c4-5a28-4fa4-86f4-9a634a029ca0/etl_config.json
[INFO] 2018-10-26 11:25:12,385 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540578312384
[INFO] 2018-10-26 11:25:12,385 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-712836af-a43a-4349-8363-8aa29ca0d0aa/userFiles-92e656c4-5a28-4fa4-86f4-9a634a029ca0/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 11:25:12,390 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540578312390
[INFO] 2018-10-26 11:25:12,391 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-712836af-a43a-4349-8363-8aa29ca0d0aa/userFiles-92e656c4-5a28-4fa4-86f4-9a634a029ca0/packages.zip
[INFO] 2018-10-26 11:25:12,453 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 11:25:12,478 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 19541.
[INFO] 2018-10-26 11:25:12,479 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:19541
[INFO] 2018-10-26 11:25:12,480 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 11:25:12,510 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 19541, None)
[INFO] 2018-10-26 11:25:12,516 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:19541 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 19541, None)
[INFO] 2018-10-26 11:25:12,521 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 19541, None)
[INFO] 2018-10-26 11:25:12,523 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 19541, None)
[INFO] 2018-10-26 11:25:12,792 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 11:25:12,793 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 11:25:13,367 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 11:25:14,425 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 11:25:14,437 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4044
[INFO] 2018-10-26 11:25:14,456 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 11:25:14,467 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 11:25:14,467 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 11:25:14,480 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 11:25:14,487 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 11:25:14,505 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 11:25:14,506 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 11:25:14,508 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-712836af-a43a-4349-8363-8aa29ca0d0aa
[INFO] 2018-10-26 11:25:14,509 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-712836af-a43a-4349-8363-8aa29ca0d0aa/pyspark-c5a4e068-cf57-462e-8ebd-93bd384fdacf
[INFO] 2018-10-26 11:25:14,509 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-cb59fa64-cf9d-4d01-aeda-d78a8b71b0a3
[WARN] 2018-10-26 11:41:37,766 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 11:41:38,607 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 11:41:38,633 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 11:41:38,807 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 11:41:38,807 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 11:41:38,807 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 11:41:38,808 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 11:41:38,808 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 11:41:39,003 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 19690.
[INFO] 2018-10-26 11:41:39,027 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 11:41:39,047 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 11:41:39,050 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 11:41:39,050 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 11:41:39,059 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-2710b41e-d684-4d75-b422-1712968225b2
[INFO] 2018-10-26 11:41:39,076 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 11:41:39,090 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-26 11:41:39,260 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 11:41:39,261 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-26 11:41:39,261 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-26 11:41:39,262 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[INFO] 2018-10-26 11:41:39,268 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4044.
[INFO] 2018-10-26 11:41:39,317 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4044
[INFO] 2018-10-26 11:41:39,425 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540579299424
[INFO] 2018-10-26 11:41:39,427 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-de4b8cc1-6900-4881-acbd-8307c9d9795a/userFiles-86c1fcec-f521-4827-96ab-89e56588aab9/etl_config.json
[INFO] 2018-10-26 11:41:39,440 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540579299440
[INFO] 2018-10-26 11:41:39,440 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-de4b8cc1-6900-4881-acbd-8307c9d9795a/userFiles-86c1fcec-f521-4827-96ab-89e56588aab9/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 11:41:39,445 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540579299445
[INFO] 2018-10-26 11:41:39,446 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-de4b8cc1-6900-4881-acbd-8307c9d9795a/userFiles-86c1fcec-f521-4827-96ab-89e56588aab9/packages.zip
[INFO] 2018-10-26 11:41:39,519 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 11:41:39,550 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10408.
[INFO] 2018-10-26 11:41:39,551 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:10408
[INFO] 2018-10-26 11:41:39,553 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 11:41:39,587 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 10408, None)
[INFO] 2018-10-26 11:41:39,594 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:10408 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 10408, None)
[INFO] 2018-10-26 11:41:39,600 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 10408, None)
[INFO] 2018-10-26 11:41:39,600 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 10408, None)
[INFO] 2018-10-26 11:41:39,879 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 11:41:39,880 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 11:41:40,409 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 11:41:41,050 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 11:41:41,063 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4044
[INFO] 2018-10-26 11:41:41,076 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 11:41:41,085 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 11:41:41,085 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 11:41:41,097 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 11:41:41,102 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 11:41:41,112 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 11:41:41,113 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 11:41:41,114 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-de4b8cc1-6900-4881-acbd-8307c9d9795a
[INFO] 2018-10-26 11:41:41,115 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b57f3917-8e5b-45f4-8443-133d05e9de45
[INFO] 2018-10-26 11:41:41,115 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-de4b8cc1-6900-4881-acbd-8307c9d9795a/pyspark-fdbb1116-e779-4521-ac9f-3053cfa762d7
[WARN] 2018-10-26 11:42:20,361 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2018-10-26 11:42:21,597 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-26 11:42:21,598 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-26 11:42:21,598 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-26 11:42:21,599 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[INFO] 2018-10-26 11:52:36,108 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 11:52:36,122 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4043
[INFO] 2018-10-26 11:52:36,134 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 11:52:36,146 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 11:52:36,146 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 11:52:36,148 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 11:52:36,153 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 11:52:36,178 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 11:52:36,178 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 11:52:36,179 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fba4722d-23cb-4c08-beb7-04dae59f4532
[WARN] 2018-10-26 11:52:36,180 org.apache.spark.HeartbeatReceiver logWarning - Removing executor driver with no recent heartbeats: 3474585 ms exceeds timeout 120000 ms
[INFO] 2018-10-26 11:52:36,180 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-dffe247f-f421-4ae7-83f7-ffaf52268143
[INFO] 2018-10-26 11:52:36,180 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-dffe247f-f421-4ae7-83f7-ffaf52268143/pyspark-cd1b56d2-96c8-498d-bb72-048d68dff803
[ERROR] 2018-10-26 11:52:36,182 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost an executor driver (already removed): Executor heartbeat timed out after 3474585 ms
[INFO] 2018-10-26 11:52:36,182 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 11:52:36,188 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,189 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None) re-registering with master
[INFO] 2018-10-26 11:52:36,189 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,190 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[WARN] 2018-10-26 11:52:36,191 org.apache.spark.SparkContext logWarning - Killing executors is not supported by current scheduler.
[INFO] 2018-10-26 11:52:36,191 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,193 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,193 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None) re-registering with master
[INFO] 2018-10-26 11:52:36,194 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,194 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,194 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,195 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,195 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None) re-registering with master
[INFO] 2018-10-26 11:52:36,195 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,195 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,196 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,196 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,196 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None) re-registering with master
[INFO] 2018-10-26 11:52:36,197 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,197 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-26 11:52:36,198 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,198 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,203 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,204 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None) re-registering with master
[INFO] 2018-10-26 11:52:36,204 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,205 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35283, None)
[INFO] 2018-10-26 11:52:36,205 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,211 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[WARN] 2018-10-26 11:52:36,221 org.apache.spark.HeartbeatReceiver logWarning - Removing executor driver with no recent heartbeats: 4948169 ms exceeds timeout 120000 ms
[ERROR] 2018-10-26 11:52:36,224 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost an executor driver (already removed): Executor heartbeat timed out after 4948169 ms
[INFO] 2018-10-26 11:52:36,225 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-26 11:52:36,226 org.apache.spark.SparkContext logWarning - Killing executors is not supported by current scheduler.
[INFO] 2018-10-26 11:52:36,226 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,229 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None) re-registering with master
[INFO] 2018-10-26 11:52:36,229 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,230 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,231 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,233 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,233 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None) re-registering with master
[INFO] 2018-10-26 11:52:36,234 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,234 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,235 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,235 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,236 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None) re-registering with master
[INFO] 2018-10-26 11:52:36,236 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,236 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 11:52:36,237 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,238 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,238 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 11:52:36,238 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,239 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None) re-registering with master
[INFO] 2018-10-26 11:52:36,239 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 11:52:36,239 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,240 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,242 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-26 11:52:36,242 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,243 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,245 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None) re-registering with master
[INFO] 2018-10-26 11:52:36,245 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[WARN] 2018-10-26 11:52:36,245 org.apache.spark.HeartbeatReceiver logWarning - Removing executor driver with no recent heartbeats: 6406309 ms exceeds timeout 120000 ms
[INFO] 2018-10-26 11:52:36,245 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 11:52:36,246 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,246 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,247 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,248 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 11:52:36,247 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None) re-registering with master
[ERROR] 2018-10-26 11:52:36,248 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost an executor driver (already removed): Executor heartbeat timed out after 6406309 ms
[INFO] 2018-10-26 11:52:36,249 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,249 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,249 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,250 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,250 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None) re-registering with master
[INFO] 2018-10-26 11:52:36,250 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,251 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,251 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,251 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,251 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None) re-registering with master
[INFO] 2018-10-26 11:52:36,252 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,252 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14143, None)
[INFO] 2018-10-26 11:52:36,252 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[WARN] 2018-10-26 11:52:36,253 org.apache.spark.SparkContext logWarning - Killing executors is not supported by current scheduler.
[INFO] 2018-10-26 11:52:36,253 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,255 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None) re-registering with master
[INFO] 2018-10-26 11:52:36,255 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,256 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,256 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,257 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 11:52:36,258 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 11:52:36,258 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,258 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None) re-registering with master
[INFO] 2018-10-26 11:52:36,259 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,259 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-829beb32-0661-4aeb-a9d2-eae596bc3be6/pyspark-fa184669-52db-4f18-9878-7b64f4d057cd
[INFO] 2018-10-26 11:52:36,259 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,259 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ab9bf50c-af7b-45e0-ab0a-b4153326eb2b
[INFO] 2018-10-26 11:52:36,259 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,260 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-829beb32-0661-4aeb-a9d2-eae596bc3be6
[INFO] 2018-10-26 11:52:36,260 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,260 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None) re-registering with master
[INFO] 2018-10-26 11:52:36,260 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[WARN] 2018-10-26 11:52:36,256 org.apache.spark.executor.Executor logWarning - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:785)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:814)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:814)
	at org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:814)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:814)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Could not find HeartbeatReceiver.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:135)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:229)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:523)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:91)
	... 13 more
[INFO] 2018-10-26 11:52:36,261 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 11:52:36,261 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,257 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 11:52:36,262 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,263 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,263 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None) re-registering with master
[INFO] 2018-10-26 11:52:36,263 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,264 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,264 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,264 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,264 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None) re-registering with master
[INFO] 2018-10-26 11:52:36,265 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,265 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,265 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,266 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-26 11:52:36,266 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None) re-registering with master
[INFO] 2018-10-26 11:52:36,266 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,267 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18460, None)
[INFO] 2018-10-26 11:52:36,268 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-26 11:52:36,272 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 11:52:36,278 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 11:52:36,279 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 11:52:36,280 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 11:52:36,280 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 11:52:36,280 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 11:52:36,281 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 11:52:36,285 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 11:52:36,286 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 11:52:36,291 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 11:52:36,291 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 11:52:36,292 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c82655a4-9cda-4698-8b51-b189a746e20a
[INFO] 2018-10-26 11:52:36,293 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-806d9832-8dc1-4e95-a53f-aecbbc49b548
[INFO] 2018-10-26 11:52:36,293 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c82655a4-9cda-4698-8b51-b189a746e20a/pyspark-278ce7f8-13d9-4d9c-8b11-14a0fc3cd908
[INFO] 2018-10-26 11:52:36,299 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 11:52:36,300 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 11:52:36,302 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-9f297880-d2e0-4add-b395-ebe2ab17c968
[INFO] 2018-10-26 11:52:36,303 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f58660ce-9d46-4cb0-80dc-aeb730e7ae28/pyspark-128452c7-4f6e-4e4b-b21f-c386654a162f
[INFO] 2018-10-26 11:52:36,303 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f58660ce-9d46-4cb0-80dc-aeb730e7ae28
[WARN] 2018-10-26 21:12:53,651 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 21:12:54,519 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 21:12:54,547 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 21:12:54,734 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 21:12:54,735 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 21:12:54,735 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 21:12:54,735 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 21:12:54,736 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 21:12:54,965 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 18900.
[INFO] 2018-10-26 21:12:54,993 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 21:12:55,012 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 21:12:55,015 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 21:12:55,015 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 21:12:55,024 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-7538bd36-e355-4676-9695-91ee7bbf66e8
[INFO] 2018-10-26 21:12:55,041 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 21:12:55,054 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 21:12:55,235 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 21:12:55,285 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:12:55,380 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540613575379
[INFO] 2018-10-26 21:12:55,382 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-0ef72859-f078-42c9-b160-346c08ef2335/userFiles-3b7923cd-03fb-4d88-8d55-8fb34f365558/etl_config.json
[INFO] 2018-10-26 21:12:55,396 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540613575396
[INFO] 2018-10-26 21:12:55,396 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-0ef72859-f078-42c9-b160-346c08ef2335/userFiles-3b7923cd-03fb-4d88-8d55-8fb34f365558/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 21:12:55,401 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540613575401
[INFO] 2018-10-26 21:12:55,402 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-0ef72859-f078-42c9-b160-346c08ef2335/userFiles-3b7923cd-03fb-4d88-8d55-8fb34f365558/packages.zip
[INFO] 2018-10-26 21:12:55,462 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 21:12:55,486 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25267.
[INFO] 2018-10-26 21:12:55,487 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:25267
[INFO] 2018-10-26 21:12:55,489 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 21:12:55,517 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25267, None)
[INFO] 2018-10-26 21:12:55,521 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:25267 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 25267, None)
[INFO] 2018-10-26 21:12:55,524 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25267, None)
[INFO] 2018-10-26 21:12:55,525 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 25267, None)
[INFO] 2018-10-26 21:12:55,828 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 21:12:55,829 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 21:12:56,411 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 21:12:57,059 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 21:12:57,067 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:12:57,077 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 21:12:57,084 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 21:12:57,084 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 21:12:57,092 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 21:12:57,095 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 21:12:57,101 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 21:12:57,101 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 21:12:57,102 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0ef72859-f078-42c9-b160-346c08ef2335
[INFO] 2018-10-26 21:12:57,103 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1b54e0cf-b2ee-4a70-839d-778edfdb8f42
[INFO] 2018-10-26 21:12:57,103 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0ef72859-f078-42c9-b160-346c08ef2335/pyspark-10b55993-d556-471e-b028-41c6be7c774c
[WARN] 2018-10-26 21:15:17,797 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 21:15:18,600 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 21:15:18,627 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 21:15:18,824 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 21:15:18,825 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 21:15:18,825 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 21:15:18,825 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 21:15:18,826 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 21:15:19,066 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 27524.
[INFO] 2018-10-26 21:15:19,096 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 21:15:19,119 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 21:15:19,123 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 21:15:19,123 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 21:15:19,134 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-f18d3c27-382e-4184-be44-921deb1589bd
[INFO] 2018-10-26 21:15:19,155 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 21:15:19,171 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 21:15:19,349 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 21:15:19,409 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:15:19,516 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540613719515
[INFO] 2018-10-26 21:15:19,518 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-3fd2ceea-8fed-47a1-b86f-adf4dafd8ae3/userFiles-d03ed7e6-e845-4d58-a54d-c451c93ee4f9/etl_config.json
[INFO] 2018-10-26 21:15:19,531 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540613719531
[INFO] 2018-10-26 21:15:19,532 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-3fd2ceea-8fed-47a1-b86f-adf4dafd8ae3/userFiles-d03ed7e6-e845-4d58-a54d-c451c93ee4f9/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 21:15:19,537 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540613719537
[INFO] 2018-10-26 21:15:19,538 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-3fd2ceea-8fed-47a1-b86f-adf4dafd8ae3/userFiles-d03ed7e6-e845-4d58-a54d-c451c93ee4f9/packages.zip
[INFO] 2018-10-26 21:15:19,606 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 21:15:19,645 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26302.
[INFO] 2018-10-26 21:15:19,647 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:26302
[INFO] 2018-10-26 21:15:19,648 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 21:15:19,684 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 26302, None)
[INFO] 2018-10-26 21:15:19,690 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:26302 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 26302, None)
[INFO] 2018-10-26 21:15:19,695 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 26302, None)
[INFO] 2018-10-26 21:15:19,695 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 26302, None)
[INFO] 2018-10-26 21:15:19,999 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 21:15:19,999 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 21:15:20,530 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 21:15:20,976 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 21:15:20,990 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:15:21,002 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 21:15:21,013 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 21:15:21,014 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 21:15:21,026 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 21:15:21,033 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 21:15:21,045 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 21:15:21,046 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 21:15:21,047 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3fd2ceea-8fed-47a1-b86f-adf4dafd8ae3
[INFO] 2018-10-26 21:15:21,049 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3fd2ceea-8fed-47a1-b86f-adf4dafd8ae3/pyspark-457d9cc2-1890-4d8f-b321-70185b90959c
[INFO] 2018-10-26 21:15:21,049 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7f67e620-6c96-4e6e-a867-afc3e273c6d8
[WARN] 2018-10-26 21:15:33,310 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 21:15:34,076 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 21:15:34,101 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 21:15:34,396 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 21:15:34,396 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 21:15:34,397 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 21:15:34,397 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 21:15:34,397 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 21:15:34,629 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 9470.
[INFO] 2018-10-26 21:15:34,662 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 21:15:34,683 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 21:15:34,686 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 21:15:34,686 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 21:15:34,696 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-6e995547-b191-4bba-8099-d43378d338d3
[INFO] 2018-10-26 21:15:34,716 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 21:15:34,730 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 21:15:34,916 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 21:15:34,972 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:15:35,086 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540613735085
[INFO] 2018-10-26 21:15:35,090 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-a33f4021-f97d-4880-8b14-ef48495c1c50/userFiles-84a3f457-5288-4c84-9277-fdf8603ed11d/etl_config.json
[INFO] 2018-10-26 21:15:35,106 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540613735106
[INFO] 2018-10-26 21:15:35,106 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-a33f4021-f97d-4880-8b14-ef48495c1c50/userFiles-84a3f457-5288-4c84-9277-fdf8603ed11d/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 21:15:35,110 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540613735110
[INFO] 2018-10-26 21:15:35,111 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-a33f4021-f97d-4880-8b14-ef48495c1c50/userFiles-84a3f457-5288-4c84-9277-fdf8603ed11d/packages.zip
[INFO] 2018-10-26 21:15:35,190 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 21:15:35,218 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29152.
[INFO] 2018-10-26 21:15:35,219 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:29152
[INFO] 2018-10-26 21:15:35,221 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 21:15:35,255 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 29152, None)
[INFO] 2018-10-26 21:15:35,262 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:29152 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 29152, None)
[INFO] 2018-10-26 21:15:35,267 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 29152, None)
[INFO] 2018-10-26 21:15:35,268 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 29152, None)
[INFO] 2018-10-26 21:15:35,577 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 21:15:35,578 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 21:15:36,128 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 21:15:36,514 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 21:15:36,525 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:15:36,536 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 21:15:36,550 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 21:15:36,550 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 21:15:36,563 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 21:15:36,569 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 21:15:36,577 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 21:15:36,578 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 21:15:36,579 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a33f4021-f97d-4880-8b14-ef48495c1c50
[INFO] 2018-10-26 21:15:36,579 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fa1960dc-70af-4907-b947-1755685f0b71
[INFO] 2018-10-26 21:15:36,580 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a33f4021-f97d-4880-8b14-ef48495c1c50/pyspark-c258cce2-d747-4f92-93a3-388c434ef974
[WARN] 2018-10-26 21:33:00,083 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 21:33:00,880 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 21:33:00,902 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-26 21:33:01,046 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 21:33:01,046 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 21:33:01,046 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 21:33:01,047 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 21:33:01,047 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 21:33:01,239 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 22046.
[INFO] 2018-10-26 21:33:01,263 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 21:33:01,281 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 21:33:01,284 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 21:33:01,285 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 21:33:01,294 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1c0c4212-6c49-41fe-ab75-505322db91f7
[INFO] 2018-10-26 21:33:01,311 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 21:33:01,325 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 21:33:01,498 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 21:33:01,552 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:33:01,638 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540614781638
[INFO] 2018-10-26 21:33:01,640 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-5355f3c4-caf4-4b67-b4b1-bdccb5a23f2e/userFiles-93560bce-8f68-4e4a-90b8-efda9bda60fb/etl_config.json
[INFO] 2018-10-26 21:33:01,653 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540614781653
[INFO] 2018-10-26 21:33:01,654 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-5355f3c4-caf4-4b67-b4b1-bdccb5a23f2e/userFiles-93560bce-8f68-4e4a-90b8-efda9bda60fb/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-26 21:33:01,657 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540614781657
[INFO] 2018-10-26 21:33:01,658 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-5355f3c4-caf4-4b67-b4b1-bdccb5a23f2e/userFiles-93560bce-8f68-4e4a-90b8-efda9bda60fb/packages.zip
[INFO] 2018-10-26 21:33:01,732 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 21:33:01,760 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33287.
[INFO] 2018-10-26 21:33:01,762 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:33287
[INFO] 2018-10-26 21:33:01,764 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 21:33:01,798 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 33287, None)
[INFO] 2018-10-26 21:33:01,806 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:33287 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 33287, None)
[INFO] 2018-10-26 21:33:01,810 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 33287, None)
[INFO] 2018-10-26 21:33:01,812 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 33287, None)
[INFO] 2018-10-26 21:33:02,136 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 21:33:02,137 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 21:33:02,762 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 21:33:03,026 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 21:33:03,040 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:33:03,053 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 21:33:03,068 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 21:33:03,068 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 21:33:03,079 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 21:33:03,085 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 21:33:03,099 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 21:33:03,101 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 21:33:03,103 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1ea1b283-1786-4dea-adc9-b4b28b756ce8
[INFO] 2018-10-26 21:33:03,105 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5355f3c4-caf4-4b67-b4b1-bdccb5a23f2e/pyspark-dae6e304-5aee-4b1d-beb8-4291bfc4f2ed
[INFO] 2018-10-26 21:33:03,106 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5355f3c4-caf4-4b67-b4b1-bdccb5a23f2e
[WARN] 2018-10-26 21:37:45,119 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 21:37:45,892 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 21:37:45,918 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP_TRUNCATE
[INFO] 2018-10-26 21:37:46,154 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 21:37:46,154 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 21:37:46,155 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 21:37:46,155 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 21:37:46,155 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 21:37:46,357 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 15174.
[INFO] 2018-10-26 21:37:46,382 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 21:37:46,401 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 21:37:46,404 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 21:37:46,404 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 21:37:46,413 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-96212bf2-9106-4441-ba3f-1b8152470e6f
[INFO] 2018-10-26 21:37:46,430 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 21:37:46,443 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 21:37:46,618 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 21:37:46,670 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:37:46,777 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540615066776
[INFO] 2018-10-26 21:37:46,779 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-fe6dd656-39e4-4060-89e8-c4b7a6fd744f/userFiles-4620fd95-f854-488c-8b02-0e1ec59e9e79/etl_config.json
[INFO] 2018-10-26 21:37:46,794 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py with timestamp 1540615066794
[INFO] 2018-10-26 21:37:46,795 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py to /tmp/spark-fe6dd656-39e4-4060-89e8-c4b7a6fd744f/userFiles-4620fd95-f854-488c-8b02-0e1ec59e9e79/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py
[INFO] 2018-10-26 21:37:46,800 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540615066800
[INFO] 2018-10-26 21:37:46,801 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-fe6dd656-39e4-4060-89e8-c4b7a6fd744f/userFiles-4620fd95-f854-488c-8b02-0e1ec59e9e79/packages.zip
[INFO] 2018-10-26 21:37:46,864 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 21:37:46,888 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29215.
[INFO] 2018-10-26 21:37:46,888 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:29215
[INFO] 2018-10-26 21:37:46,890 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 21:37:46,922 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 29215, None)
[INFO] 2018-10-26 21:37:46,927 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:29215 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 29215, None)
[INFO] 2018-10-26 21:37:46,931 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 29215, None)
[INFO] 2018-10-26 21:37:46,932 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 29215, None)
[INFO] 2018-10-26 21:37:47,231 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 21:37:47,232 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 21:37:47,788 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 21:37:48,944 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 21:37:48,957 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:37:48,970 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 21:37:48,980 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 21:37:48,981 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 21:37:48,990 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 21:37:48,998 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 21:37:49,007 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 21:37:49,007 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 21:37:49,008 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fe6dd656-39e4-4060-89e8-c4b7a6fd744f
[INFO] 2018-10-26 21:37:49,009 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3124689d-da7e-4951-9167-178f61b54deb
[INFO] 2018-10-26 21:37:49,009 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-fe6dd656-39e4-4060-89e8-c4b7a6fd744f/pyspark-5fabf7d2-e690-48fe-877d-326447d72e7a
[WARN] 2018-10-26 21:38:17,065 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-26 21:38:17,823 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-26 21:38:17,850 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP
[INFO] 2018-10-26 21:38:18,012 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-26 21:38:18,012 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-26 21:38:18,012 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-26 21:38:18,013 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-26 21:38:18,013 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-26 21:38:18,215 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 29192.
[INFO] 2018-10-26 21:38:18,242 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-26 21:38:18,260 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-26 21:38:18,263 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-26 21:38:18,264 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-26 21:38:18,273 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-2c0d6f32-02eb-4d7e-a3c4-fab346b526a0
[INFO] 2018-10-26 21:38:18,289 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-26 21:38:18,303 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-26 21:38:18,478 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-26 21:38:18,530 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:38:18,651 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540615098650
[INFO] 2018-10-26 21:38:18,653 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-89db7ddc-7de4-4232-8cfa-18d055091d3c/userFiles-7938f6f8-8d62-4792-aa30-16beacc4deda/etl_config.json
[INFO] 2018-10-26 21:38:18,667 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540615098666
[INFO] 2018-10-26 21:38:18,667 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py to /tmp/spark-89db7ddc-7de4-4232-8cfa-18d055091d3c/userFiles-7938f6f8-8d62-4792-aa30-16beacc4deda/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 21:38:18,671 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540615098671
[INFO] 2018-10-26 21:38:18,672 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-89db7ddc-7de4-4232-8cfa-18d055091d3c/userFiles-7938f6f8-8d62-4792-aa30-16beacc4deda/packages.zip
[INFO] 2018-10-26 21:38:18,742 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-26 21:38:18,769 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27990.
[INFO] 2018-10-26 21:38:18,770 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:27990
[INFO] 2018-10-26 21:38:18,772 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-26 21:38:18,805 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 27990, None)
[INFO] 2018-10-26 21:38:18,812 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:27990 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 27990, None)
[INFO] 2018-10-26 21:38:18,816 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 27990, None)
[INFO] 2018-10-26 21:38:18,817 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 27990, None)
[INFO] 2018-10-26 21:38:19,074 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-26 21:38:19,075 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-26 21:38:19,577 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-26 21:38:22,574 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 200.085757 ms
[INFO] 2018-10-26 21:38:22,735 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-26 21:38:22,760 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-26 21:38:22,761 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-26 21:38:22,761 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 21:38:22,763 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 21:38:22,771 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-26 21:38:22,856 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 21:38:22,903 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 21:38:22,906 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:27990 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 21:38:22,909 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 21:38:22,924 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 21:38:22,925 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-26 21:38:22,975 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 21:38:22,989 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-26 21:38:22,995 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540615098650
[INFO] 2018-10-26 21:38:23,027 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-89db7ddc-7de4-4232-8cfa-18d055091d3c/userFiles-7938f6f8-8d62-4792-aa30-16beacc4deda/etl_config.json
[INFO] 2018-10-26 21:38:23,034 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540615098666
[INFO] 2018-10-26 21:38:23,036 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py has been previously copied to /tmp/spark-89db7ddc-7de4-4232-8cfa-18d055091d3c/userFiles-7938f6f8-8d62-4792-aa30-16beacc4deda/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-26 21:38:23,043 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540615098671
[INFO] 2018-10-26 21:38:23,045 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-89db7ddc-7de4-4232-8cfa-18d055091d3c/userFiles-7938f6f8-8d62-4792-aa30-16beacc4deda/packages.zip
[INFO] 2018-10-26 21:38:24,462 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 21:38:24,501 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 506, boot = 416, init = 90, finish = 0
[INFO] 2018-10-26 21:38:24,520 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1706 bytes result sent to driver
[INFO] 2018-10-26 21:38:24,536 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1574 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 21:38:24,541 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 21:38:24,555 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.760 s
[INFO] 2018-10-26 21:38:24,561 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.825239 s
[INFO] 2018-10-26 21:38:24,781 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340
[INFO] 2018-10-26 21:38:24,783 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) with 1 output partitions
[INFO] 2018-10-26 21:38:24,783 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340)
[INFO] 2018-10-26 21:38:24,783 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-26 21:38:24,784 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-26 21:38:24,785 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340), which has no missing parents
[INFO] 2018-10-26 21:38:24,791 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-26 21:38:24,794 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-26 21:38:24,795 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:27990 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-26 21:38:24,797 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-26 21:38:24,798 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-26 21:38:24,798 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-26 21:38:24,799 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-26 21:38:24,800 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-26 21:38:24,964 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-26 21:38:24,996 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -442, init = 483, finish = 0
[INFO] 2018-10-26 21:38:25,001 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1668 bytes result sent to driver
[INFO] 2018-10-26 21:38:25,004 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 205 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-26 21:38:25,004 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-26 21:38:25,007 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) finished in 0.219 s
[INFO] 2018-10-26 21:38:25,008 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340, took 0.226501 s
[INFO] 2018-10-26 21:42:10,669 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-26 21:42:10,681 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-26 21:42:10,703 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-26 21:42:10,730 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-26 21:42:10,730 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-26 21:42:10,731 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-26 21:42:10,735 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-26 21:42:10,749 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-26 21:42:10,750 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-26 21:42:10,752 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-89db7ddc-7de4-4232-8cfa-18d055091d3c
[INFO] 2018-10-26 21:42:10,753 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-89db7ddc-7de4-4232-8cfa-18d055091d3c/pyspark-2e30de88-0b19-448c-b50c-8a8076d80188
[INFO] 2018-10-26 21:42:10,754 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-9c396ceb-38fc-4262-9ca8-5361b0268339
