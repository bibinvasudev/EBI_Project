[WARN] 2018-10-25 01:22:46,922 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 01:22:47,340 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 01:22:47,342 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-df3e1775-156e-4384-ad32-9f8ccc9a4fd7
[WARN] 2018-10-25 01:23:04,927 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 01:23:05,715 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 01:23:05,741 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_PARTITION.TRUNCATE
[INFO] 2018-10-25 01:23:05,893 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 01:23:05,893 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 01:23:05,894 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 01:23:05,894 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 01:23:05,894 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 01:23:06,100 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 37132.
[INFO] 2018-10-25 01:23:06,126 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 01:23:06,145 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 01:23:06,148 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 01:23:06,149 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 01:23:06,158 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-9e271fe4-f699-4fce-abad-67ba36f41de6
[INFO] 2018-10-25 01:23:06,175 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 01:23:06,189 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 01:23:06,366 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 01:23:06,420 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:23:06,511 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540455786510
[INFO] 2018-10-25 01:23:06,513 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-2cd2f656-65cf-43b2-be1b-ff62a3404330/userFiles-e1979785-dc04-498b-99e6-81314f405dd6/etl_config.json
[INFO] 2018-10-25 01:23:06,525 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py with timestamp 1540455786525
[INFO] 2018-10-25 01:23:06,525 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py to /tmp/spark-2cd2f656-65cf-43b2-be1b-ff62a3404330/userFiles-e1979785-dc04-498b-99e6-81314f405dd6/JB_BOOKINGS_PARTITION.TRUNCATE.py
[INFO] 2018-10-25 01:23:06,529 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540455786529
[INFO] 2018-10-25 01:23:06,530 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-2cd2f656-65cf-43b2-be1b-ff62a3404330/userFiles-e1979785-dc04-498b-99e6-81314f405dd6/packages.zip
[INFO] 2018-10-25 01:23:06,598 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 01:23:06,624 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30145.
[INFO] 2018-10-25 01:23:06,625 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:30145
[INFO] 2018-10-25 01:23:06,627 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 01:23:06,660 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 30145, None)
[INFO] 2018-10-25 01:23:06,666 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:30145 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 30145, None)
[INFO] 2018-10-25 01:23:06,669 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 30145, None)
[INFO] 2018-10-25 01:23:06,669 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 30145, None)
[INFO] 2018-10-25 01:23:06,961 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 01:23:06,962 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 01:23:07,460 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 01:23:10,491 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 209.503502 ms
[INFO] 2018-10-25 01:23:10,657 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 01:23:10,680 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 01:23:10,681 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 01:23:10,681 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:23:10,683 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:23:10,707 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 01:23:10,788 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:23:10,828 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 01:23:10,831 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:30145 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:23:10,834 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:23:10,848 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:23:10,849 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 01:23:10,895 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:23:10,917 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 01:23:10,925 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540455786510
[INFO] 2018-10-25 01:23:10,957 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-2cd2f656-65cf-43b2-be1b-ff62a3404330/userFiles-e1979785-dc04-498b-99e6-81314f405dd6/etl_config.json
[INFO] 2018-10-25 01:23:10,963 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540455786529
[INFO] 2018-10-25 01:23:10,963 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-2cd2f656-65cf-43b2-be1b-ff62a3404330/userFiles-e1979785-dc04-498b-99e6-81314f405dd6/packages.zip
[INFO] 2018-10-25 01:23:10,969 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py with timestamp 1540455786525
[INFO] 2018-10-25 01:23:10,970 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py has been previously copied to /tmp/spark-2cd2f656-65cf-43b2-be1b-ff62a3404330/userFiles-e1979785-dc04-498b-99e6-81314f405dd6/JB_BOOKINGS_PARTITION.TRUNCATE.py
[INFO] 2018-10-25 01:23:11,818 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:23:11,857 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 505, boot = 391, init = 114, finish = 0
[INFO] 2018-10-25 01:23:11,878 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 01:23:11,895 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1012 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:23:11,899 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:23:11,913 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.177 s
[INFO] 2018-10-25 01:23:11,919 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.261394 s
[INFO] 2018-10-25 01:23:12,326 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 01:23:12,328 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 01:23:12,328 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 01:23:12,329 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:23:12,329 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:23:12,330 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 01:23:12,335 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:23:12,337 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 01:23:12,338 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:30145 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:23:12,340 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:23:12,341 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:23:12,341 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 01:23:12,342 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:23:12,343 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 01:23:12,686 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:23:12,690 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 64, boot = -760, init = 824, finish = 0
[INFO] 2018-10-25 01:23:12,694 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 01:23:12,698 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 356 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:23:12,698 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:23:12,699 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.367 s
[INFO] 2018-10-25 01:23:12,700 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.373518 s
[INFO] 2018-10-25 01:23:13,019 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 01:23:13,020 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 01:23:13,020 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 01:23:13,021 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:23:13,021 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:23:13,021 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 01:23:13,024 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:23:13,026 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 01:23:13,027 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:30145 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:23:13,028 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:23:13,029 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:23:13,029 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 01:23:13,030 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:23:13,031 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 01:23:13,336 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:23:13,339 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 51, boot = -592, init = 643, finish = 0
[INFO] 2018-10-25 01:23:13,342 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1663 bytes result sent to driver
[INFO] 2018-10-25 01:23:13,345 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 315 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:23:13,345 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:23:13,347 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.325 s
[INFO] 2018-10-25 01:23:13,348 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 0.328567 s
[INFO] 2018-10-25 01:23:13,678 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 01:23:13,680 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 01:23:13,680 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 01:23:13,680 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:23:13,681 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:23:13,682 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 3 (PythonRDD[19] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 01:23:13,687 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_3 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:23:13,690 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 01:23:13,691 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_3_piece0 in memory on vmwebietl02-dev:30145 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:23:13,692 org.apache.spark.SparkContext logInfo - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:23:13,694 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 3 (PythonRDD[19] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:23:13,694 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 3.0 with 1 tasks
[INFO] 2018-10-25 01:23:13,696 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:23:13,696 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 3.0 (TID 3)
[INFO] 2018-10-25 01:23:14,013 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:23:14,022 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 49, boot = -621, init = 670, finish = 0
[INFO] 2018-10-25 01:23:14,026 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 3.0 (TID 3). 1661 bytes result sent to driver
[INFO] 2018-10-25 01:23:14,031 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 3.0 (TID 3) in 336 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:23:14,032 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:23:14,034 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.349 s
[INFO] 2018-10-25 01:23:14,035 org.apache.spark.scheduler.DAGScheduler logInfo - Job 3 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.356247 s
[INFO] 2018-10-25 01:23:14,376 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 01:23:14,390 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:23:14,407 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 01:23:14,419 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 01:23:14,420 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 01:23:14,427 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 01:23:14,433 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 01:23:14,443 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 01:23:14,445 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 01:23:14,446 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2cd2f656-65cf-43b2-be1b-ff62a3404330
[INFO] 2018-10-25 01:23:14,446 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2d2b4d26-1a4d-4d3a-a421-20a40bf34799
[INFO] 2018-10-25 01:23:14,447 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2cd2f656-65cf-43b2-be1b-ff62a3404330/pyspark-02e8dee7-d1e6-4f5b-ae8d-7a621807f647
[WARN] 2018-10-25 01:24:11,279 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 01:24:12,051 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 01:24:12,076 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_PARTITION.TRUNCATE
[INFO] 2018-10-25 01:24:12,253 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 01:24:12,254 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 01:24:12,254 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 01:24:12,254 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 01:24:12,255 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 01:24:12,450 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 22962.
[INFO] 2018-10-25 01:24:12,475 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 01:24:12,494 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 01:24:12,497 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 01:24:12,498 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 01:24:12,506 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-7a299641-0a46-4a9c-8138-3b4ac456b810
[INFO] 2018-10-25 01:24:12,523 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 01:24:12,537 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 01:24:12,728 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 01:24:12,781 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:24:12,871 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540455852871
[INFO] 2018-10-25 01:24:12,873 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-40e63d73-3435-4bf9-8204-b370e879b629/userFiles-d5f39d39-41e8-4466-8301-41fc3bf9f52a/etl_config.json
[INFO] 2018-10-25 01:24:12,886 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py with timestamp 1540455852886
[INFO] 2018-10-25 01:24:12,887 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py to /tmp/spark-40e63d73-3435-4bf9-8204-b370e879b629/userFiles-d5f39d39-41e8-4466-8301-41fc3bf9f52a/JB_BOOKINGS_PARTITION.TRUNCATE.py
[INFO] 2018-10-25 01:24:12,891 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540455852891
[INFO] 2018-10-25 01:24:12,892 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-40e63d73-3435-4bf9-8204-b370e879b629/userFiles-d5f39d39-41e8-4466-8301-41fc3bf9f52a/packages.zip
[INFO] 2018-10-25 01:24:12,964 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 01:24:12,991 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26242.
[INFO] 2018-10-25 01:24:12,992 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:26242
[INFO] 2018-10-25 01:24:12,994 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 01:24:13,028 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 26242, None)
[INFO] 2018-10-25 01:24:13,033 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:26242 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 26242, None)
[INFO] 2018-10-25 01:24:13,036 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 26242, None)
[INFO] 2018-10-25 01:24:13,037 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 26242, None)
[INFO] 2018-10-25 01:24:13,316 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 01:24:13,317 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 01:24:13,835 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 01:24:17,086 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 215.346597 ms
[INFO] 2018-10-25 01:24:17,235 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 01:24:17,260 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 01:24:17,261 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 01:24:17,262 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:24:17,263 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:24:17,271 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 01:24:17,345 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:17,378 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:17,382 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:26242 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:24:17,385 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:24:17,401 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:24:17,402 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 01:24:17,455 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:24:17,470 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 01:24:17,477 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540455852871
[INFO] 2018-10-25 01:24:17,504 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-40e63d73-3435-4bf9-8204-b370e879b629/userFiles-d5f39d39-41e8-4466-8301-41fc3bf9f52a/etl_config.json
[INFO] 2018-10-25 01:24:17,509 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540455852891
[INFO] 2018-10-25 01:24:17,510 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-40e63d73-3435-4bf9-8204-b370e879b629/userFiles-d5f39d39-41e8-4466-8301-41fc3bf9f52a/packages.zip
[INFO] 2018-10-25 01:24:17,517 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py with timestamp 1540455852886
[INFO] 2018-10-25 01:24:17,518 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py has been previously copied to /tmp/spark-40e63d73-3435-4bf9-8204-b370e879b629/userFiles-d5f39d39-41e8-4466-8301-41fc3bf9f52a/JB_BOOKINGS_PARTITION.TRUNCATE.py
[INFO] 2018-10-25 01:24:18,316 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:24:18,355 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 483, boot = 406, init = 76, finish = 1
[INFO] 2018-10-25 01:24:18,379 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 01:24:18,397 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 953 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:24:18,401 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:24:18,409 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.117 s
[INFO] 2018-10-25 01:24:18,416 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.180587 s
[INFO] 2018-10-25 01:24:18,776 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 01:24:18,778 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 01:24:18,778 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 01:24:18,779 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:24:18,779 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:24:18,780 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 01:24:18,786 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:18,789 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:18,790 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:26242 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:24:18,791 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:24:18,792 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:24:18,793 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 01:24:18,794 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:24:18,795 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 01:24:19,088 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:24:19,097 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -690, init = 731, finish = 0
[INFO] 2018-10-25 01:24:19,101 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 01:24:19,105 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 310 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:24:19,105 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:24:19,108 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.325 s
[INFO] 2018-10-25 01:24:19,109 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.332778 s
[INFO] 2018-10-25 01:24:19,439 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 01:24:19,441 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 01:24:19,441 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 01:24:19,442 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:24:19,442 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:24:19,443 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 01:24:19,448 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:19,450 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:19,452 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:26242 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:24:19,453 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:24:19,454 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:24:19,454 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 01:24:19,456 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:24:19,457 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 01:24:19,776 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:24:19,779 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 46, boot = -635, init = 681, finish = 0
[INFO] 2018-10-25 01:24:19,783 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1663 bytes result sent to driver
[INFO] 2018-10-25 01:24:19,785 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 330 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:24:19,786 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:24:19,788 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.343 s
[INFO] 2018-10-25 01:24:19,790 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 0.349918 s
[INFO] 2018-10-25 01:24:20,137 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 01:24:20,138 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 01:24:20,139 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 01:24:20,140 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:24:20,140 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:24:20,141 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 3 (PythonRDD[19] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 01:24:20,147 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_3 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:20,149 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:20,150 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_3_piece0 in memory on vmwebietl02-dev:26242 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:24:20,150 org.apache.spark.SparkContext logInfo - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:24:20,151 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 3 (PythonRDD[19] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:24:20,151 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 3.0 with 1 tasks
[INFO] 2018-10-25 01:24:20,153 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:24:20,154 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 3.0 (TID 3)
[INFO] 2018-10-25 01:24:20,449 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:24:20,457 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -629, init = 670, finish = 0
[INFO] 2018-10-25 01:24:20,461 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 3.0 (TID 3). 1661 bytes result sent to driver
[INFO] 2018-10-25 01:24:20,465 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 3.0 (TID 3) in 312 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:24:20,465 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:24:20,466 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.323 s
[INFO] 2018-10-25 01:24:20,467 org.apache.spark.scheduler.DAGScheduler logInfo - Job 3 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.329482 s
[INFO] 2018-10-25 01:24:21,190 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 01:24:21,192 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 4 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 01:24:21,193 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 4 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 01:24:21,193 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:24:21,193 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:24:21,194 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 4 (PythonRDD[24] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 01:24:21,199 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_4 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:21,202 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 01:24:21,203 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_4_piece0 in memory on vmwebietl02-dev:26242 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:24:21,205 org.apache.spark.SparkContext logInfo - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:24:21,207 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 4 (PythonRDD[24] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:24:21,207 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 4.0 with 1 tasks
[INFO] 2018-10-25 01:24:21,208 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:24:21,209 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 4.0 (TID 4)
[INFO] 2018-10-25 01:24:22,601 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:24:22,606 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 51, boot = -2095, init = 2146, finish = 0
[INFO] 2018-10-25 01:24:22,609 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 4.0 (TID 4). 1670 bytes result sent to driver
[INFO] 2018-10-25 01:24:22,611 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 4.0 (TID 4) in 1403 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:24:22,612 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:24:22,613 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 4 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 1.417 s
[INFO] 2018-10-25 01:24:22,614 org.apache.spark.scheduler.DAGScheduler logInfo - Job 4 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 1.423583 s
[INFO] 2018-10-25 01:24:22,653 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 01:24:22,664 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:24:22,678 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 01:24:22,697 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 01:24:22,698 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 01:24:22,709 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 01:24:22,714 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 01:24:22,723 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 01:24:22,724 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 01:24:22,726 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-40e63d73-3435-4bf9-8204-b370e879b629
[INFO] 2018-10-25 01:24:22,726 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-475da19c-e30a-47c3-ae3c-c9823619deb7
[INFO] 2018-10-25 01:24:22,727 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-40e63d73-3435-4bf9-8204-b370e879b629/pyspark-a5ae2942-6809-43b5-bbf0-faa031110a8e
[WARN] 2018-10-25 01:30:08,203 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 01:30:08,993 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 01:30:09,021 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_PARTITION.TRUNCATE
[INFO] 2018-10-25 01:30:09,162 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 01:30:09,163 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 01:30:09,163 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 01:30:09,163 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 01:30:09,164 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 01:30:09,390 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 22349.
[INFO] 2018-10-25 01:30:09,417 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 01:30:09,437 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 01:30:09,440 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 01:30:09,441 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 01:30:09,451 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-a42761ce-2aa8-4657-9cbb-e6da9a64d41d
[INFO] 2018-10-25 01:30:09,468 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 01:30:09,484 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 01:30:09,681 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 01:30:09,743 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:30:09,862 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540456209861
[INFO] 2018-10-25 01:30:09,864 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-2188a405-d495-4113-9288-fb79fac5ec8d/userFiles-026e3a04-6724-479c-82fe-bb4bdbb4ac50/etl_config.json
[INFO] 2018-10-25 01:30:09,879 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py with timestamp 1540456209878
[INFO] 2018-10-25 01:30:09,879 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py to /tmp/spark-2188a405-d495-4113-9288-fb79fac5ec8d/userFiles-026e3a04-6724-479c-82fe-bb4bdbb4ac50/JB_BOOKINGS_PARTITION.TRUNCATE.py
[INFO] 2018-10-25 01:30:09,883 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540456209883
[INFO] 2018-10-25 01:30:09,883 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-2188a405-d495-4113-9288-fb79fac5ec8d/userFiles-026e3a04-6724-479c-82fe-bb4bdbb4ac50/packages.zip
[INFO] 2018-10-25 01:30:09,954 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 01:30:09,978 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34692.
[INFO] 2018-10-25 01:30:09,979 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:34692
[INFO] 2018-10-25 01:30:09,981 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 01:30:10,015 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34692, None)
[INFO] 2018-10-25 01:30:10,022 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:34692 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 34692, None)
[INFO] 2018-10-25 01:30:10,026 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34692, None)
[INFO] 2018-10-25 01:30:10,027 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 34692, None)
[INFO] 2018-10-25 01:30:10,317 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 01:30:10,317 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 01:30:10,923 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 01:30:14,000 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 199.069083 ms
[INFO] 2018-10-25 01:30:14,147 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 01:30:14,167 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 01:30:14,167 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 01:30:14,168 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:30:14,170 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:30:14,177 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 01:30:14,241 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:14,271 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:14,275 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:34692 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:30:14,277 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:30:14,291 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:30:14,292 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 01:30:14,342 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:30:14,357 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 01:30:14,363 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540456209861
[INFO] 2018-10-25 01:30:14,394 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-2188a405-d495-4113-9288-fb79fac5ec8d/userFiles-026e3a04-6724-479c-82fe-bb4bdbb4ac50/etl_config.json
[INFO] 2018-10-25 01:30:14,400 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540456209883
[INFO] 2018-10-25 01:30:14,401 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-2188a405-d495-4113-9288-fb79fac5ec8d/userFiles-026e3a04-6724-479c-82fe-bb4bdbb4ac50/packages.zip
[INFO] 2018-10-25 01:30:14,406 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py with timestamp 1540456209878
[INFO] 2018-10-25 01:30:14,410 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION.TRUNCATE.py has been previously copied to /tmp/spark-2188a405-d495-4113-9288-fb79fac5ec8d/userFiles-026e3a04-6724-479c-82fe-bb4bdbb4ac50/JB_BOOKINGS_PARTITION.TRUNCATE.py
[INFO] 2018-10-25 01:30:15,382 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:30:15,419 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 633, boot = 368, init = 264, finish = 1
[INFO] 2018-10-25 01:30:15,441 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 01:30:15,459 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1129 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:30:15,464 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:30:15,475 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.279 s
[INFO] 2018-10-25 01:30:15,482 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.334084 s
[INFO] 2018-10-25 01:30:15,948 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 01:30:15,958 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 01:30:15,958 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 01:30:15,958 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:30:15,959 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:30:15,960 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 01:30:15,966 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:15,969 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:15,971 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:34692 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:30:15,972 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:30:15,973 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:30:15,974 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 01:30:15,975 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:30:15,976 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 01:30:16,288 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:30:16,291 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 45, boot = -821, init = 865, finish = 1
[INFO] 2018-10-25 01:30:16,294 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 01:30:16,299 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 324 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:30:16,300 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:30:16,302 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.340 s
[INFO] 2018-10-25 01:30:16,303 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.347028 s
[INFO] 2018-10-25 01:30:16,634 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 01:30:16,635 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 01:30:16,636 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 01:30:16,636 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:30:16,637 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:30:16,637 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 01:30:16,642 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:16,644 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:16,646 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:34692 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:30:16,647 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:30:16,648 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:30:16,648 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 01:30:16,649 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:30:16,650 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 01:30:16,968 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:30:16,970 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 53, boot = -620, init = 672, finish = 1
[INFO] 2018-10-25 01:30:16,973 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1663 bytes result sent to driver
[INFO] 2018-10-25 01:30:16,976 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 327 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:30:16,977 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:30:16,978 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 0.339 s
[INFO] 2018-10-25 01:30:16,978 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 0.344301 s
[INFO] 2018-10-25 01:30:17,335 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 01:30:17,336 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 01:30:17,337 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 01:30:17,337 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:30:17,338 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:30:17,339 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 3 (PythonRDD[19] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 01:30:17,343 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_3 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:17,345 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:17,346 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_3_piece0 in memory on vmwebietl02-dev:34692 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:30:17,347 org.apache.spark.SparkContext logInfo - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:30:17,348 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 3 (PythonRDD[19] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:30:17,348 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 3.0 with 1 tasks
[INFO] 2018-10-25 01:30:17,350 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:30:17,351 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 3.0 (TID 3)
[INFO] 2018-10-25 01:30:17,656 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:30:17,681 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -658, init = 700, finish = 0
[INFO] 2018-10-25 01:30:17,685 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 3.0 (TID 3). 1661 bytes result sent to driver
[INFO] 2018-10-25 01:30:17,688 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 3.0 (TID 3) in 338 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:30:17,688 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:30:17,690 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 3 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.349 s
[INFO] 2018-10-25 01:30:17,691 org.apache.spark.scheduler.DAGScheduler logInfo - Job 3 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.355759 s
[INFO] 2018-10-25 01:30:18,044 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 01:30:18,045 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 4 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 01:30:18,046 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 4 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 01:30:18,046 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:30:18,047 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:30:18,048 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 4 (PythonRDD[24] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 01:30:18,053 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_4 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:18,057 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 01:30:18,058 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_4_piece0 in memory on vmwebietl02-dev:34692 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:30:18,059 org.apache.spark.SparkContext logInfo - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:30:18,061 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 4 (PythonRDD[24] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:30:18,061 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 4.0 with 1 tasks
[INFO] 2018-10-25 01:30:18,063 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:30:18,064 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 4.0 (TID 4)
[INFO] 2018-10-25 01:30:18,764 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:30:18,766 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 66, boot = -1019, init = 1084, finish = 1
[INFO] 2018-10-25 01:30:18,768 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 4.0 (TID 4). 1670 bytes result sent to driver
[INFO] 2018-10-25 01:30:18,770 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 4.0 (TID 4) in 708 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:30:18,771 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:30:18,772 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 4 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.722 s
[INFO] 2018-10-25 01:30:18,774 org.apache.spark.scheduler.DAGScheduler logInfo - Job 4 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.730168 s
[INFO] 2018-10-25 01:30:18,816 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 01:30:18,828 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:30:18,847 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 01:30:18,864 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 01:30:18,864 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 01:30:18,876 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 01:30:18,880 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 01:30:18,889 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 01:30:18,891 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 01:30:18,892 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2188a405-d495-4113-9288-fb79fac5ec8d/pyspark-518c0e2f-80d1-4637-b3ff-422a3760cf47
[INFO] 2018-10-25 01:30:18,892 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-2188a405-d495-4113-9288-fb79fac5ec8d
[INFO] 2018-10-25 01:30:18,892 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3a98096a-e714-411b-89db-c49a2fd71fa2
[WARN] 2018-10-25 01:33:34,414 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 01:33:35,221 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 01:33:35,248 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE
[INFO] 2018-10-25 01:33:35,470 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 01:33:35,470 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 01:33:35,470 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 01:33:35,471 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 01:33:35,471 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 01:33:35,704 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 29832.
[INFO] 2018-10-25 01:33:35,732 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 01:33:35,751 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 01:33:35,754 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 01:33:35,754 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 01:33:35,764 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-e686f747-43d1-401c-b217-566f4225d538
[INFO] 2018-10-25 01:33:35,781 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 01:33:35,795 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 01:33:35,966 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 01:33:36,035 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:33:36,164 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:29832/files/etl_config.json with timestamp 1540456416163
[INFO] 2018-10-25 01:33:36,166 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-5872e1e7-ccee-488f-b772-80e73800f683/userFiles-27681e90-8f49-41b3-8a63-9e01491f55cd/etl_config.json
[INFO] 2018-10-25 01:33:36,181 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE.py at spark://vmwebietl02-dev:29832/files/JB_WORK_INVOICE.py with timestamp 1540456416181
[INFO] 2018-10-25 01:33:36,182 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE.py to /tmp/spark-5872e1e7-ccee-488f-b772-80e73800f683/userFiles-27681e90-8f49-41b3-8a63-9e01491f55cd/JB_WORK_INVOICE.py
[INFO] 2018-10-25 01:33:36,187 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:29832/files/packages.zip with timestamp 1540456416186
[INFO] 2018-10-25 01:33:36,187 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-5872e1e7-ccee-488f-b772-80e73800f683/userFiles-27681e90-8f49-41b3-8a63-9e01491f55cd/packages.zip
[INFO] 2018-10-25 01:33:36,280 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 01:33:36,367 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 53 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 01:33:36,491 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025013336-0002
[INFO] 2018-10-25 01:33:36,502 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37099.
[INFO] 2018-10-25 01:33:36,502 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:37099
[INFO] 2018-10-25 01:33:36,504 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 01:33:36,532 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 37099, None)
[INFO] 2018-10-25 01:33:36,536 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:37099 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 37099, None)
[INFO] 2018-10-25 01:33:36,538 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 37099, None)
[INFO] 2018-10-25 01:33:36,539 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 37099, None)
[INFO] 2018-10-25 01:33:36,681 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 01:33:36,888 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 01:33:36,888 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 01:33:37,314 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 01:33:49,483 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 01:33:49,493 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:33:49,499 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 01:33:49,505 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 01:33:49,519 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 01:33:49,533 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 01:33:49,533 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 01:33:49,545 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 01:33:49,550 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 01:33:49,563 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 01:33:49,563 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 01:33:49,565 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5872e1e7-ccee-488f-b772-80e73800f683
[INFO] 2018-10-25 01:33:49,565 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-159886cf-2f3f-46d4-9e53-76c9321d4128
[INFO] 2018-10-25 01:33:49,566 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5872e1e7-ccee-488f-b772-80e73800f683/pyspark-6c8ecbf6-47e9-428b-980a-f7ed53ed28dc
[WARN] 2018-10-25 01:49:05,541 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 01:49:05,927 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 01:49:05,929 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-eab0d781-cf62-4b97-a778-ddcf5f488ddf
[WARN] 2018-10-25 01:49:45,748 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 01:49:46,123 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 01:49:46,126 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c3417d1d-3541-46f9-82e5-ea77d0ef2d63
[WARN] 2018-10-25 01:50:08,183 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 01:50:08,934 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 01:50:08,961 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_PARTITION.TRUNCATE
[INFO] 2018-10-25 01:50:09,139 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 01:50:09,140 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 01:50:09,140 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 01:50:09,140 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 01:50:09,141 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 01:50:09,356 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 29566.
[INFO] 2018-10-25 01:50:09,381 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 01:50:09,399 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 01:50:09,402 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 01:50:09,402 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 01:50:09,411 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-d4c5a1e9-f657-4619-9467-293e1a6bfeba
[INFO] 2018-10-25 01:50:09,428 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 01:50:09,441 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 01:50:09,619 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 01:50:09,672 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:50:09,761 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540457409760
[INFO] 2018-10-25 01:50:09,762 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-0964c5f7-f319-4cb7-a8f0-9c3e194317b9/userFiles-642c9c90-46a5-49fb-bc40-89c7735181f4/etl_config.json
[INFO] 2018-10-25 01:50:09,776 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540457409776
[INFO] 2018-10-25 01:50:09,777 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py to /tmp/spark-0964c5f7-f319-4cb7-a8f0-9c3e194317b9/userFiles-642c9c90-46a5-49fb-bc40-89c7735181f4/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 01:50:09,781 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540457409781
[INFO] 2018-10-25 01:50:09,782 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-0964c5f7-f319-4cb7-a8f0-9c3e194317b9/userFiles-642c9c90-46a5-49fb-bc40-89c7735181f4/packages.zip
[INFO] 2018-10-25 01:50:09,853 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 01:50:09,881 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29752.
[INFO] 2018-10-25 01:50:09,882 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:29752
[INFO] 2018-10-25 01:50:09,883 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 01:50:09,917 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 29752, None)
[INFO] 2018-10-25 01:50:09,922 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:29752 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 29752, None)
[INFO] 2018-10-25 01:50:09,926 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 29752, None)
[INFO] 2018-10-25 01:50:09,927 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 29752, None)
[INFO] 2018-10-25 01:50:10,233 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 01:50:10,234 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 01:50:10,802 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 01:50:13,947 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 198.48335 ms
[INFO] 2018-10-25 01:50:14,094 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 01:50:14,115 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 01:50:14,116 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 01:50:14,117 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:50:14,119 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:50:14,126 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 01:50:14,192 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:50:14,225 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 01:50:14,227 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:29752 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:50:14,229 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:50:14,242 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:50:14,243 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 01:50:14,285 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:50:14,301 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 01:50:14,305 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540457409760
[INFO] 2018-10-25 01:50:14,335 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-0964c5f7-f319-4cb7-a8f0-9c3e194317b9/userFiles-642c9c90-46a5-49fb-bc40-89c7735181f4/etl_config.json
[INFO] 2018-10-25 01:50:14,341 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540457409781
[INFO] 2018-10-25 01:50:14,342 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-0964c5f7-f319-4cb7-a8f0-9c3e194317b9/userFiles-642c9c90-46a5-49fb-bc40-89c7735181f4/packages.zip
[INFO] 2018-10-25 01:50:14,347 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540457409776
[INFO] 2018-10-25 01:50:14,349 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-0964c5f7-f319-4cb7-a8f0-9c3e194317b9/userFiles-642c9c90-46a5-49fb-bc40-89c7735181f4/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 01:50:15,951 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:50:15,991 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 485, boot = 389, init = 95, finish = 1
[INFO] 2018-10-25 01:50:16,011 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 01:50:16,028 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1752 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:50:16,033 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:50:16,047 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.901 s
[INFO] 2018-10-25 01:50:16,052 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.957808 s
[INFO] 2018-10-25 01:50:16,406 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 01:50:16,408 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 01:50:16,409 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 01:50:16,409 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:50:16,409 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:50:16,411 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 01:50:16,416 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 01:50:16,419 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 01:50:16,420 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:29752 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:50:16,421 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:50:16,423 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:50:16,423 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 01:50:16,425 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:50:16,426 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 01:50:16,710 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:50:16,714 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 48, boot = -666, init = 714, finish = 0
[INFO] 2018-10-25 01:50:16,717 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 01:50:16,720 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 295 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:50:16,721 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:50:16,723 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.310 s
[INFO] 2018-10-25 01:50:16,725 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.318548 s
[INFO] 2018-10-25 01:50:17,075 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 01:50:17,077 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 01:50:17,078 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 01:50:17,078 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 01:50:17,078 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 01:50:17,079 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 01:50:17,084 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 01:50:17,087 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 01:50:17,090 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:29752 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 01:50:17,090 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 01:50:17,091 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 01:50:17,091 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 01:50:17,093 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 01:50:17,094 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 01:50:17,836 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 01:50:17,839 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 70, boot = -1049, init = 1119, finish = 0
[INFO] 2018-10-25 01:50:17,842 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-25 01:50:17,844 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 752 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 01:50:17,845 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 01:50:17,847 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.765 s
[INFO] 2018-10-25 01:50:17,848 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.771886 s
[INFO] 2018-10-25 01:50:18,650 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 01:50:18,665 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 01:50:18,681 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 01:50:18,692 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 01:50:18,693 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 01:50:18,702 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 01:50:18,705 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 01:50:18,714 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 01:50:18,716 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 01:50:18,718 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f869a0b9-3708-477d-804c-2be3df3c01bb
[INFO] 2018-10-25 01:50:18,718 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0964c5f7-f319-4cb7-a8f0-9c3e194317b9/pyspark-29d07f01-1d6d-4111-b862-ebc753032c69
[INFO] 2018-10-25 01:50:18,719 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-0964c5f7-f319-4cb7-a8f0-9c3e194317b9
[WARN] 2018-10-25 02:03:00,416 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:03:00,797 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:03:00,800 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1b342b72-b3cd-4f50-b703-3a4a569cd35d
[WARN] 2018-10-25 02:03:24,278 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:03:25,073 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:03:25,099 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_PARTITION.TRUNCATE
[INFO] 2018-10-25 02:03:25,228 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:03:25,228 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:03:25,228 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:03:25,229 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:03:25,229 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:03:25,438 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 22196.
[INFO] 2018-10-25 02:03:25,464 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:03:25,484 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:03:25,487 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:03:25,487 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:03:25,496 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-93a77592-943a-4816-9845-8cff97790342
[INFO] 2018-10-25 02:03:25,514 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:03:25,529 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 02:03:25,715 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 02:03:25,776 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 02:03:25,880 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540458205880
[INFO] 2018-10-25 02:03:25,882 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-e624aefb-ff38-4dc5-aeca-2a8455eca573/userFiles-4591b6fb-96df-4204-999d-52a35277196d/etl_config.json
[INFO] 2018-10-25 02:03:25,897 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540458205897
[INFO] 2018-10-25 02:03:25,897 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py to /tmp/spark-e624aefb-ff38-4dc5-aeca-2a8455eca573/userFiles-4591b6fb-96df-4204-999d-52a35277196d/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:03:25,902 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540458205902
[INFO] 2018-10-25 02:03:25,903 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-e624aefb-ff38-4dc5-aeca-2a8455eca573/userFiles-4591b6fb-96df-4204-999d-52a35277196d/packages.zip
[INFO] 2018-10-25 02:03:25,979 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 02:03:26,005 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11656.
[INFO] 2018-10-25 02:03:26,006 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:11656
[INFO] 2018-10-25 02:03:26,008 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:03:26,043 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 11656, None)
[INFO] 2018-10-25 02:03:26,049 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:11656 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 11656, None)
[INFO] 2018-10-25 02:03:26,054 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 11656, None)
[INFO] 2018-10-25 02:03:26,055 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 11656, None)
[INFO] 2018-10-25 02:03:26,351 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:03:26,351 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:03:26,873 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:03:29,809 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 196.165931 ms
[INFO] 2018-10-25 02:03:29,961 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:03:29,984 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:03:29,984 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:03:29,985 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:03:29,987 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:03:29,995 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:03:30,065 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:03:30,101 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:03:30,104 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:11656 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:03:30,107 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:03:30,127 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:03:30,128 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 02:03:30,177 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:03:30,188 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 02:03:30,193 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540458205880
[INFO] 2018-10-25 02:03:30,235 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-e624aefb-ff38-4dc5-aeca-2a8455eca573/userFiles-4591b6fb-96df-4204-999d-52a35277196d/etl_config.json
[INFO] 2018-10-25 02:03:30,240 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540458205902
[INFO] 2018-10-25 02:03:30,241 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-e624aefb-ff38-4dc5-aeca-2a8455eca573/userFiles-4591b6fb-96df-4204-999d-52a35277196d/packages.zip
[INFO] 2018-10-25 02:03:30,246 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540458205897
[INFO] 2018-10-25 02:03:30,248 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-e624aefb-ff38-4dc5-aeca-2a8455eca573/userFiles-4591b6fb-96df-4204-999d-52a35277196d/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:03:31,045 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:03:31,086 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 473, boot = 371, init = 101, finish = 1
[INFO] 2018-10-25 02:03:31,110 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 02:03:31,128 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 961 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:03:31,132 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:03:31,147 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.129 s
[INFO] 2018-10-25 02:03:31,152 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.191007 s
[INFO] 2018-10-25 02:03:31,494 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 02:03:31,497 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 02:03:31,497 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 02:03:31,497 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:03:31,498 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:03:31,498 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 02:03:31,502 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:03:31,504 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 02:03:31,505 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:11656 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:03:31,506 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:03:31,507 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:03:31,507 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 02:03:31,509 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:03:31,510 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 02:03:31,819 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:03:31,822 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 45, boot = -685, init = 729, finish = 1
[INFO] 2018-10-25 02:03:31,825 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 02:03:31,829 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 320 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:03:31,829 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:03:31,831 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.331 s
[INFO] 2018-10-25 02:03:31,833 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.338242 s
[INFO] 2018-10-25 02:03:32,158 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 02:03:32,160 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 02:03:32,160 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 02:03:32,160 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:03:32,161 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:03:32,162 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 02:03:32,167 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 02:03:32,169 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 02:03:32,171 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:11656 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:03:32,174 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:03:32,186 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:03:32,186 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 02:03:32,188 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:03:32,189 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 02:03:32,846 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:03:32,849 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 49, boot = -971, init = 1020, finish = 0
[INFO] 2018-10-25 02:03:32,853 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-25 02:03:32,856 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 669 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:03:32,856 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:03:32,858 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.694 s
[INFO] 2018-10-25 02:03:32,860 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.701062 s
[INFO] 2018-10-25 02:03:33,252 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 02:03:33,265 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 02:03:33,282 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 02:03:33,294 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 02:03:33,294 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 02:03:33,307 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 02:03:33,310 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 02:03:33,322 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 02:03:33,322 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:03:33,324 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4477b4c0-859c-4622-8b77-559b48f15012
[INFO] 2018-10-25 02:03:33,324 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e624aefb-ff38-4dc5-aeca-2a8455eca573
[INFO] 2018-10-25 02:03:33,324 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e624aefb-ff38-4dc5-aeca-2a8455eca573/pyspark-0d443aba-3cc5-4491-90ae-4e0453bb6a71
[WARN] 2018-10-25 02:05:06,040 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:05:06,828 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:05:06,854 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_PARTITION.TRUNCATE
[INFO] 2018-10-25 02:05:07,009 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:05:07,010 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:05:07,010 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:05:07,010 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:05:07,011 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:05:07,218 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 36654.
[INFO] 2018-10-25 02:05:07,244 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:05:07,264 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:05:07,267 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:05:07,268 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:05:07,277 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-25eaba81-7437-45a2-ae2f-8c24ced6264a
[INFO] 2018-10-25 02:05:07,295 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:05:07,309 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 02:05:07,490 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 02:05:07,542 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 02:05:07,634 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540458307633
[INFO] 2018-10-25 02:05:07,636 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-b5f98e24-db06-4c6f-a276-595a9ea7b639/userFiles-31e55901-8079-4286-b6d8-598b3fc4ab1c/etl_config.json
[INFO] 2018-10-25 02:05:07,649 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540458307649
[INFO] 2018-10-25 02:05:07,650 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py to /tmp/spark-b5f98e24-db06-4c6f-a276-595a9ea7b639/userFiles-31e55901-8079-4286-b6d8-598b3fc4ab1c/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:05:07,655 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540458307655
[INFO] 2018-10-25 02:05:07,655 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-b5f98e24-db06-4c6f-a276-595a9ea7b639/userFiles-31e55901-8079-4286-b6d8-598b3fc4ab1c/packages.zip
[INFO] 2018-10-25 02:05:07,728 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 02:05:07,762 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9362.
[INFO] 2018-10-25 02:05:07,763 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:9362
[INFO] 2018-10-25 02:05:07,765 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:05:07,800 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 9362, None)
[INFO] 2018-10-25 02:05:07,808 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:9362 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 9362, None)
[INFO] 2018-10-25 02:05:07,812 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 9362, None)
[INFO] 2018-10-25 02:05:07,814 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 9362, None)
[INFO] 2018-10-25 02:05:08,105 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:05:08,106 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:05:08,644 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:05:11,676 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 201.565037 ms
[INFO] 2018-10-25 02:05:11,824 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:05:11,849 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:05:11,850 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:05:11,856 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:05:11,858 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:05:11,869 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:05:11,942 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:11,981 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:11,985 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:9362 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:05:11,987 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:05:11,999 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:05:12,000 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 02:05:12,051 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:05:12,063 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 02:05:12,067 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540458307633
[INFO] 2018-10-25 02:05:12,094 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-b5f98e24-db06-4c6f-a276-595a9ea7b639/userFiles-31e55901-8079-4286-b6d8-598b3fc4ab1c/etl_config.json
[INFO] 2018-10-25 02:05:12,099 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540458307655
[INFO] 2018-10-25 02:05:12,100 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-b5f98e24-db06-4c6f-a276-595a9ea7b639/userFiles-31e55901-8079-4286-b6d8-598b3fc4ab1c/packages.zip
[INFO] 2018-10-25 02:05:12,104 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540458307649
[INFO] 2018-10-25 02:05:12,105 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-b5f98e24-db06-4c6f-a276-595a9ea7b639/userFiles-31e55901-8079-4286-b6d8-598b3fc4ab1c/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:05:12,894 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:05:12,934 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 485, boot = 374, init = 110, finish = 1
[INFO] 2018-10-25 02:05:12,955 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 02:05:12,973 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 934 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:05:12,977 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:05:12,991 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.101 s
[INFO] 2018-10-25 02:05:12,998 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.173086 s
[INFO] 2018-10-25 02:05:13,367 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 02:05:13,370 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 02:05:13,371 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 02:05:13,371 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:05:13,372 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:05:13,373 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 02:05:13,379 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:13,382 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:13,383 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:9362 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:05:13,385 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:05:13,386 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:05:13,386 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 02:05:13,388 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:05:13,388 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 02:05:13,681 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:05:13,692 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -701, init = 743, finish = 0
[INFO] 2018-10-25 02:05:13,696 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 02:05:13,701 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 314 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:05:13,701 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:05:13,704 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.329 s
[INFO] 2018-10-25 02:05:13,706 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.337823 s
[INFO] 2018-10-25 02:05:14,018 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 02:05:14,020 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 02:05:14,020 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 02:05:14,020 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:05:14,020 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:05:14,021 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 02:05:14,024 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:14,026 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:14,028 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:9362 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:05:14,029 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:05:14,030 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:05:14,031 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 02:05:14,032 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:05:14,033 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 02:05:14,363 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:05:14,365 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 47, boot = -626, init = 673, finish = 0
[INFO] 2018-10-25 02:05:14,368 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-25 02:05:14,371 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 339 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:05:14,372 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:05:14,373 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.351 s
[INFO] 2018-10-25 02:05:14,374 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.354985 s
[INFO] 2018-10-25 02:05:14,758 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 02:05:14,770 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 02:05:14,785 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 02:05:14,797 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 02:05:14,797 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 02:05:14,810 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 02:05:14,814 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 02:05:14,823 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 02:05:14,824 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:05:14,825 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b5f98e24-db06-4c6f-a276-595a9ea7b639
[INFO] 2018-10-25 02:05:14,826 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b5f98e24-db06-4c6f-a276-595a9ea7b639/pyspark-b98874ca-7709-4bc9-855c-2393cd0885a8
[INFO] 2018-10-25 02:05:14,826 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c759f78f-452e-4c50-befe-080454e1e631
[WARN] 2018-10-25 02:05:50,383 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:05:51,163 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:05:51,188 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_PARTITION.TRUNCATE
[INFO] 2018-10-25 02:05:51,403 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:05:51,403 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:05:51,404 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:05:51,404 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:05:51,404 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:05:51,597 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 20649.
[INFO] 2018-10-25 02:05:51,621 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:05:51,640 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:05:51,643 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:05:51,643 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:05:51,652 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-3352495a-8fe8-49c2-a20f-c53106483aa5
[INFO] 2018-10-25 02:05:51,669 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:05:51,682 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 02:05:51,878 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 02:05:51,937 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 02:05:52,045 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540458352044
[INFO] 2018-10-25 02:05:52,047 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-c0edfe68-d1c6-4fe0-8273-fb3555ba3fde/userFiles-4a39d681-63f8-463f-a17a-35ab68ae1504/etl_config.json
[INFO] 2018-10-25 02:05:52,061 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540458352061
[INFO] 2018-10-25 02:05:52,062 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py to /tmp/spark-c0edfe68-d1c6-4fe0-8273-fb3555ba3fde/userFiles-4a39d681-63f8-463f-a17a-35ab68ae1504/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:05:52,067 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540458352067
[INFO] 2018-10-25 02:05:52,068 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-c0edfe68-d1c6-4fe0-8273-fb3555ba3fde/userFiles-4a39d681-63f8-463f-a17a-35ab68ae1504/packages.zip
[INFO] 2018-10-25 02:05:52,129 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 02:05:52,156 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10117.
[INFO] 2018-10-25 02:05:52,156 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:10117
[INFO] 2018-10-25 02:05:52,158 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:05:52,188 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 10117, None)
[INFO] 2018-10-25 02:05:52,196 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:10117 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 10117, None)
[INFO] 2018-10-25 02:05:52,201 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 10117, None)
[INFO] 2018-10-25 02:05:52,202 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 10117, None)
[INFO] 2018-10-25 02:05:52,493 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:05:52,493 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:05:53,004 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:05:56,028 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 205.796045 ms
[INFO] 2018-10-25 02:05:56,181 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:05:56,209 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:05:56,210 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:05:56,210 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:05:56,212 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:05:56,222 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:05:56,304 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:56,342 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:56,346 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:10117 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:05:56,350 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:05:56,366 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:05:56,368 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 02:05:56,419 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:05:56,434 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 02:05:56,442 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540458352044
[INFO] 2018-10-25 02:05:56,470 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-c0edfe68-d1c6-4fe0-8273-fb3555ba3fde/userFiles-4a39d681-63f8-463f-a17a-35ab68ae1504/etl_config.json
[INFO] 2018-10-25 02:05:56,475 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540458352067
[INFO] 2018-10-25 02:05:56,475 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-c0edfe68-d1c6-4fe0-8273-fb3555ba3fde/userFiles-4a39d681-63f8-463f-a17a-35ab68ae1504/packages.zip
[INFO] 2018-10-25 02:05:56,480 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py with timestamp 1540458352061
[INFO] 2018-10-25 02:05:56,481 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-c0edfe68-d1c6-4fe0-8273-fb3555ba3fde/userFiles-4a39d681-63f8-463f-a17a-35ab68ae1504/JB_BOOKINGS_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:05:57,216 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:05:57,251 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 444, boot = 369, init = 74, finish = 1
[INFO] 2018-10-25 02:05:57,272 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1706 bytes result sent to driver
[INFO] 2018-10-25 02:05:57,288 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 881 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:05:57,292 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:05:57,306 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.060 s
[INFO] 2018-10-25 02:05:57,313 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.130946 s
[INFO] 2018-10-25 02:05:57,620 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 02:05:57,623 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 02:05:57,623 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 02:05:57,623 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:05:57,624 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:05:57,625 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 02:05:57,631 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:57,634 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:57,636 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:10117 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:05:57,638 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:05:57,639 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:05:57,639 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 02:05:57,640 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:05:57,641 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 02:05:57,909 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:05:57,929 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -623, init = 665, finish = 0
[INFO] 2018-10-25 02:05:57,933 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 02:05:57,936 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 296 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:05:57,937 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:05:57,939 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.311 s
[INFO] 2018-10-25 02:05:57,940 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.318565 s
[INFO] 2018-10-25 02:05:58,227 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 02:05:58,229 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 02:05:58,229 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 02:05:58,230 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:05:58,230 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:05:58,231 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 02:05:58,236 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:58,240 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 02:05:58,241 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:10117 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:05:58,243 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:05:58,244 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:05:58,244 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 02:05:58,246 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:05:58,247 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 02:05:58,516 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:05:58,535 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -559, init = 600, finish = 0
[INFO] 2018-10-25 02:05:58,538 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-25 02:05:58,542 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 297 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:05:58,543 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:05:58,545 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.311 s
[INFO] 2018-10-25 02:05:58,546 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.318120 s
[INFO] 2018-10-25 02:05:58,939 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 02:05:58,954 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 02:05:58,973 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 02:05:58,995 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 02:05:58,996 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 02:05:59,008 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 02:05:59,013 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 02:05:59,023 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 02:05:59,024 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:05:59,026 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7265888a-ae34-45d3-a663-b5901a8cd2d7
[INFO] 2018-10-25 02:05:59,027 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c0edfe68-d1c6-4fe0-8273-fb3555ba3fde/pyspark-f0036679-fb7d-4e3a-816b-45404b0ba0c7
[INFO] 2018-10-25 02:05:59,028 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c0edfe68-d1c6-4fe0-8273-fb3555ba3fde
[WARN] 2018-10-25 02:18:02,082 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:18:02,851 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:18:02,876 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:18:03,105 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:18:03,106 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:18:03,106 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:18:03,106 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:18:03,106 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:18:03,340 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 36612.
[INFO] 2018-10-25 02:18:03,368 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:18:03,390 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:18:03,393 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:18:03,394 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:18:03,405 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-b2e54e14-c759-4d92-82bc-7ac2c4b48ad2
[INFO] 2018-10-25 02:18:03,424 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:18:03,437 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 02:18:03,646 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 02:18:03,707 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 02:18:03,854 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:36612/files/etl_config.json with timestamp 1540459083853
[INFO] 2018-10-25 02:18:03,856 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-7ceda005-5daa-4b1b-9264-164c510d6da2/userFiles-beb92850-fc2f-4c7a-b4d7-8b96fed0ccae/etl_config.json
[INFO] 2018-10-25 02:18:03,882 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:36612/files/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540459083882
[INFO] 2018-10-25 02:18:03,882 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py to /tmp/spark-7ceda005-5daa-4b1b-9264-164c510d6da2/userFiles-beb92850-fc2f-4c7a-b4d7-8b96fed0ccae/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:18:03,887 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:36612/files/packages.zip with timestamp 1540459083887
[INFO] 2018-10-25 02:18:03,888 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-7ceda005-5daa-4b1b-9264-164c510d6da2/userFiles-beb92850-fc2f-4c7a-b4d7-8b96fed0ccae/packages.zip
[INFO] 2018-10-25 02:18:03,976 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 02:18:04,045 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 40 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 02:18:04,147 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025021804-0003
[INFO] 2018-10-25 02:18:04,159 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 21682.
[INFO] 2018-10-25 02:18:04,161 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:21682
[INFO] 2018-10-25 02:18:04,164 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:18:04,199 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 21682, None)
[INFO] 2018-10-25 02:18:04,205 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:21682 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 21682, None)
[INFO] 2018-10-25 02:18:04,210 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 21682, None)
[INFO] 2018-10-25 02:18:04,212 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 21682, None)
[INFO] 2018-10-25 02:18:04,387 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 02:18:04,606 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:18:04,607 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:18:05,092 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:18:08,253 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 228.030186 ms
[INFO] 2018-10-25 02:18:08,410 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:18:08,435 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:18:08,436 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:18:08,436 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:18:08,438 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:18:08,448 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:18:08,528 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:18:08,565 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:18:08,569 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:21682 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:18:08,573 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:18:08,590 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:18:08,591 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[WARN] 2018-10-25 02:18:23,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:18:38,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:18:53,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:19:08,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:19:23,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:19:38,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:19:53,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:20:08,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:20:23,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:20:38,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:20:53,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:21:08,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:21:23,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:21:38,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:21:53,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:22:08,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:22:23,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:22:38,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:22:53,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:23:08,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:23:13,156 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:23:14,044 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:23:14,067 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:23:14,240 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:23:14,240 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:23:14,240 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:23:14,241 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:23:14,241 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:23:14,450 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 13542.
[INFO] 2018-10-25 02:23:14,478 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:23:14,498 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:23:14,501 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:23:14,502 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:23:14,510 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-29beb698-35ab-4cf5-8c3f-9edab4731d1a
[INFO] 2018-10-25 02:23:14,529 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:23:14,547 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:23:14,747 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 02:23:14,753 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 02:23:14,805 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 02:23:14,895 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540459394895
[INFO] 2018-10-25 02:23:14,897 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-aed613cf-1c38-4877-a772-54eff264e15c/userFiles-f55634e6-2bb2-4538-bac6-0fdcbe075ef9/etl_config.json
[INFO] 2018-10-25 02:23:14,910 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540459394910
[INFO] 2018-10-25 02:23:14,911 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py to /tmp/spark-aed613cf-1c38-4877-a772-54eff264e15c/userFiles-f55634e6-2bb2-4538-bac6-0fdcbe075ef9/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:23:14,916 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540459394916
[INFO] 2018-10-25 02:23:14,916 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-aed613cf-1c38-4877-a772-54eff264e15c/userFiles-f55634e6-2bb2-4538-bac6-0fdcbe075ef9/packages.zip
[INFO] 2018-10-25 02:23:14,987 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 02:23:15,014 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30648.
[INFO] 2018-10-25 02:23:15,014 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:30648
[INFO] 2018-10-25 02:23:15,016 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:23:15,049 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 30648, None)
[INFO] 2018-10-25 02:23:15,055 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:30648 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 30648, None)
[INFO] 2018-10-25 02:23:15,059 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 30648, None)
[INFO] 2018-10-25 02:23:15,060 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 30648, None)
[INFO] 2018-10-25 02:23:15,363 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:23:15,363 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:23:15,928 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:23:19,042 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 192.780878 ms
[INFO] 2018-10-25 02:23:19,203 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:23:19,228 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:23:19,229 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:23:19,230 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:23:19,232 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:23:19,242 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:23:19,316 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:23:19,350 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:23:19,352 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:30648 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:23:19,359 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:23:19,370 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:23:19,371 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 02:23:19,424 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:23:19,438 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 02:23:19,442 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540459394895
[INFO] 2018-10-25 02:23:19,471 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-aed613cf-1c38-4877-a772-54eff264e15c/userFiles-f55634e6-2bb2-4538-bac6-0fdcbe075ef9/etl_config.json
[INFO] 2018-10-25 02:23:19,475 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540459394916
[INFO] 2018-10-25 02:23:19,476 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-aed613cf-1c38-4877-a772-54eff264e15c/userFiles-f55634e6-2bb2-4538-bac6-0fdcbe075ef9/packages.zip
[INFO] 2018-10-25 02:23:19,479 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540459394910
[INFO] 2018-10-25 02:23:19,481 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-aed613cf-1c38-4877-a772-54eff264e15c/userFiles-f55634e6-2bb2-4538-bac6-0fdcbe075ef9/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:23:20,256 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:23:20,298 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 499, boot = 386, init = 112, finish = 1
[INFO] 2018-10-25 02:23:20,319 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 02:23:20,335 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 926 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:23:20,340 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:23:20,353 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.088 s
[INFO] 2018-10-25 02:23:20,359 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.155283 s
[INFO] 2018-10-25 02:23:20,696 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 02:23:20,698 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 02:23:20,699 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 02:23:20,699 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:23:20,699 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:23:20,701 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 02:23:20,707 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:23:20,711 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 02:23:20,712 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:30648 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:23:20,714 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:23:20,715 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:23:20,715 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 02:23:20,716 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:23:20,717 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 02:23:21,001 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:23:21,016 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -665, init = 707, finish = 0
[INFO] 2018-10-25 02:23:21,020 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 02:23:21,022 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 306 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:23:21,023 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:23:21,025 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.322 s
[INFO] 2018-10-25 02:23:21,027 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.330179 s
[INFO] 2018-10-25 02:23:21,679 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 02:23:21,681 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 02:23:21,681 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 02:23:21,681 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:23:21,682 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:23:21,683 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 02:23:21,688 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 02:23:21,691 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 02:23:21,692 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:30648 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:23:21,693 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:23:21,694 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:23:21,695 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 02:23:21,696 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:23:21,697 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 02:23:23,188 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:23:23,190 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 45, boot = -2127, init = 2171, finish = 1
[INFO] 2018-10-25 02:23:23,193 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1665 bytes result sent to driver
[INFO] 2018-10-25 02:23:23,196 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 1500 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:23:23,197 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:23:23,199 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 1.513 s
[INFO] 2018-10-25 02:23:23,200 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 1.520470 s
[INFO] 2018-10-25 02:23:23,239 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 02:23:23,253 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 02:23:23,267 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 02:23:23,277 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 02:23:23,277 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 02:23:23,289 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 02:23:23,293 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 02:23:23,306 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 02:23:23,308 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:23:23,309 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-aed613cf-1c38-4877-a772-54eff264e15c/pyspark-32c89979-155d-4596-9ba9-6c0c3044dcad
[INFO] 2018-10-25 02:23:23,310 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-aed613cf-1c38-4877-a772-54eff264e15c
[INFO] 2018-10-25 02:23:23,310 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4d24f170-1f2f-4eff-a547-0fd1563d8879
[WARN] 2018-10-25 02:23:23,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:23:38,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:23:53,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:24:08,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:24:23,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:24:38,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:24:50,366 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:24:51,193 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:24:51,218 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE
[INFO] 2018-10-25 02:24:51,390 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:24:51,391 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:24:51,391 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:24:51,392 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:24:51,404 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:24:51,597 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 17903.
[INFO] 2018-10-25 02:24:51,622 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:24:51,641 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:24:51,644 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:24:51,644 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:24:51,653 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-ca7c34af-9378-4263-b15b-e5e1b661faff
[INFO] 2018-10-25 02:24:51,671 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:24:51,684 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:24:51,854 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 02:24:51,860 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 02:24:51,914 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 02:24:52,019 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:17903/files/etl_config.json with timestamp 1540459492018
[INFO] 2018-10-25 02:24:52,021 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-68361a9c-0b42-4d94-b3c1-255ed72a65c3/userFiles-a33bab6d-a17b-4f63-aec7-206f84bc873b/etl_config.json
[INFO] 2018-10-25 02:24:52,040 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE.py at spark://vmwebietl02-dev:17903/files/JB_WORK_INVOICE.py with timestamp 1540459492040
[INFO] 2018-10-25 02:24:52,041 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE.py to /tmp/spark-68361a9c-0b42-4d94-b3c1-255ed72a65c3/userFiles-a33bab6d-a17b-4f63-aec7-206f84bc873b/JB_WORK_INVOICE.py
[INFO] 2018-10-25 02:24:52,045 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:17903/files/packages.zip with timestamp 1540459492045
[INFO] 2018-10-25 02:24:52,046 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-68361a9c-0b42-4d94-b3c1-255ed72a65c3/userFiles-a33bab6d-a17b-4f63-aec7-206f84bc873b/packages.zip
[INFO] 2018-10-25 02:24:52,138 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 02:24:52,210 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 40 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 02:24:52,325 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025022452-0004
[INFO] 2018-10-25 02:24:52,337 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 28951.
[INFO] 2018-10-25 02:24:52,338 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:28951
[INFO] 2018-10-25 02:24:52,340 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:24:52,373 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 28951, None)
[INFO] 2018-10-25 02:24:52,379 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:28951 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 28951, None)
[INFO] 2018-10-25 02:24:52,385 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 28951, None)
[INFO] 2018-10-25 02:24:52,386 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 28951, None)
[INFO] 2018-10-25 02:24:52,558 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 02:24:52,782 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:24:52,783 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:24:53,215 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:25:05,941 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 02:25:05,952 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 02:25:05,958 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 02:25:05,962 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 02:25:05,976 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 02:25:05,992 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 02:25:05,993 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 02:25:06,000 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 02:25:06,006 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 02:25:06,019 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 02:25:06,020 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:25:06,022 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-68361a9c-0b42-4d94-b3c1-255ed72a65c3/pyspark-45e1f4bb-204f-41d7-a7dd-c58b8a962f76
[INFO] 2018-10-25 02:25:06,023 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-68361a9c-0b42-4d94-b3c1-255ed72a65c3
[INFO] 2018-10-25 02:25:06,024 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f967332e-7119-4d39-8f27-73e7c7a460b3
[WARN] 2018-10-25 02:25:17,088 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:25:17,841 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:25:17,866 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:25:18,141 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:25:18,142 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:25:18,142 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:25:18,142 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:25:18,143 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:25:18,344 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 32728.
[INFO] 2018-10-25 02:25:18,371 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:25:18,389 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:25:18,392 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:25:18,393 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:25:18,402 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-5c6d3ca8-d5c1-4443-b98b-5cc556db4dbe
[INFO] 2018-10-25 02:25:18,419 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:25:18,432 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:25:18,596 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 02:25:18,602 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 02:25:18,653 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 02:25:18,743 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:32728/files/etl_config.json with timestamp 1540459518742
[INFO] 2018-10-25 02:25:18,745 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-634ceeee-901b-4356-93f3-353404eede77/userFiles-36f355d3-3514-44c6-965b-b42a7366d52a/etl_config.json
[INFO] 2018-10-25 02:25:18,757 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:32728/files/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540459518757
[INFO] 2018-10-25 02:25:18,758 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py to /tmp/spark-634ceeee-901b-4356-93f3-353404eede77/userFiles-36f355d3-3514-44c6-965b-b42a7366d52a/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:25:18,764 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:32728/files/packages.zip with timestamp 1540459518763
[INFO] 2018-10-25 02:25:18,764 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-634ceeee-901b-4356-93f3-353404eede77/userFiles-36f355d3-3514-44c6-965b-b42a7366d52a/packages.zip
[INFO] 2018-10-25 02:25:18,844 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 02:25:18,917 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 41 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 02:25:19,043 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025022519-0005
[INFO] 2018-10-25 02:25:19,055 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 20721.
[INFO] 2018-10-25 02:25:19,056 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:20721
[INFO] 2018-10-25 02:25:19,058 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:25:19,086 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 20721, None)
[INFO] 2018-10-25 02:25:19,092 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:20721 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 20721, None)
[INFO] 2018-10-25 02:25:19,097 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 20721, None)
[INFO] 2018-10-25 02:25:19,098 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 20721, None)
[INFO] 2018-10-25 02:25:19,247 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 02:25:19,441 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:25:19,442 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:25:19,878 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:25:23,000 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 210.803457 ms
[INFO] 2018-10-25 02:25:23,133 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:25:23,154 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:25:23,154 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:25:23,155 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:25:23,156 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:25:23,165 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:25:23,239 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:25:23,277 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:25:23,281 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:20721 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:25:23,283 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:25:23,300 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:25:23,301 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[WARN] 2018-10-25 02:25:38,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:25:53,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:26:08,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:26:23,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:26:38,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:26:53,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:27:08,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:27:23,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:27:38,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:27:53,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:28:08,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:28:23,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:29:11,904 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:29:12,892 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:29:12,920 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE_DATAPREP_TRUNCATE
[INFO] 2018-10-25 02:29:13,178 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:29:13,178 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:29:13,179 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:29:13,179 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:29:13,198 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:29:13,425 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 14018.
[INFO] 2018-10-25 02:29:13,453 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:29:13,474 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:29:13,477 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:29:13,478 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:29:13,490 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-f08833d4-79f9-44fb-83cf-ba8e6fdb81ed
[INFO] 2018-10-25 02:29:13,508 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:29:13,521 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:29:13,713 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 02:29:13,714 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-25 02:29:13,719 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-25 02:29:13,768 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 02:29:13,857 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:14018/files/etl_config.json with timestamp 1540459753856
[INFO] 2018-10-25 02:29:13,859 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-bc8c28a4-c4e6-44cf-a96a-5babd5866a3f/userFiles-863d87f9-3645-4ca2-9856-ccd5c32029c8/etl_config.json
[INFO] 2018-10-25 02:29:13,870 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py at spark://vmwebietl02-dev:14018/files/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py with timestamp 1540459753870
[INFO] 2018-10-25 02:29:13,870 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py to /tmp/spark-bc8c28a4-c4e6-44cf-a96a-5babd5866a3f/userFiles-863d87f9-3645-4ca2-9856-ccd5c32029c8/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py
[INFO] 2018-10-25 02:29:13,874 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:14018/files/packages.zip with timestamp 1540459753874
[INFO] 2018-10-25 02:29:13,875 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-bc8c28a4-c4e6-44cf-a96a-5babd5866a3f/userFiles-863d87f9-3645-4ca2-9856-ccd5c32029c8/packages.zip
[INFO] 2018-10-25 02:29:13,980 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 02:29:14,041 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 36 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 02:29:14,167 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025022914-0006
[INFO] 2018-10-25 02:29:14,173 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 13118.
[INFO] 2018-10-25 02:29:14,174 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:13118
[INFO] 2018-10-25 02:29:14,176 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:29:14,203 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 13118, None)
[INFO] 2018-10-25 02:29:14,208 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:13118 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 13118, None)
[INFO] 2018-10-25 02:29:14,212 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 13118, None)
[INFO] 2018-10-25 02:29:14,212 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 13118, None)
[INFO] 2018-10-25 02:29:14,377 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 02:29:14,655 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:29:14,656 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:29:15,110 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:29:16,005 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 02:29:16,018 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 02:29:16,025 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 02:29:16,031 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 02:29:16,045 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 02:29:16,055 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 02:29:16,057 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 02:29:16,068 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 02:29:16,071 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 02:29:16,102 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 02:29:16,103 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:29:16,104 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8bf3bb65-fd1f-4385-99b9-1fd524353c8e
[INFO] 2018-10-25 02:29:16,105 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bc8c28a4-c4e6-44cf-a96a-5babd5866a3f
[INFO] 2018-10-25 02:29:16,105 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-bc8c28a4-c4e6-44cf-a96a-5babd5866a3f/pyspark-26b8caea-8a63-41f4-9337-06600da7914c
[WARN] 2018-10-25 02:29:31,856 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:29:32,598 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:29:32,623 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:29:32,806 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:29:32,807 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:29:32,807 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:29:32,808 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:29:32,808 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:29:33,042 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 24275.
[INFO] 2018-10-25 02:29:33,070 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:29:33,094 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:29:33,097 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:29:33,098 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:29:33,110 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-87c51754-83e1-4789-b358-2e0097a18db2
[INFO] 2018-10-25 02:29:33,130 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:29:33,146 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:29:33,327 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 02:29:33,328 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-25 02:29:33,335 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-25 02:29:33,383 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 02:29:33,474 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:24275/files/etl_config.json with timestamp 1540459773473
[INFO] 2018-10-25 02:29:33,476 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-de15f453-e9ea-4943-8e01-e3cae5a5061f/userFiles-c1d16820-c76e-4935-a7ce-acc1fb4f4972/etl_config.json
[INFO] 2018-10-25 02:29:33,490 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:24275/files/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540459773490
[INFO] 2018-10-25 02:29:33,491 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py to /tmp/spark-de15f453-e9ea-4943-8e01-e3cae5a5061f/userFiles-c1d16820-c76e-4935-a7ce-acc1fb4f4972/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:29:33,496 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:24275/files/packages.zip with timestamp 1540459773496
[INFO] 2018-10-25 02:29:33,497 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-de15f453-e9ea-4943-8e01-e3cae5a5061f/userFiles-c1d16820-c76e-4935-a7ce-acc1fb4f4972/packages.zip
[INFO] 2018-10-25 02:29:33,596 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 02:29:33,668 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 40 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 02:29:33,775 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025022933-0007
[INFO] 2018-10-25 02:29:33,787 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26459.
[INFO] 2018-10-25 02:29:33,788 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:26459
[INFO] 2018-10-25 02:29:33,790 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:29:33,824 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 26459, None)
[INFO] 2018-10-25 02:29:33,830 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:26459 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 26459, None)
[INFO] 2018-10-25 02:29:33,835 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 26459, None)
[INFO] 2018-10-25 02:29:33,836 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 26459, None)
[INFO] 2018-10-25 02:29:34,015 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 02:29:34,227 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:29:34,228 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:29:34,733 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:29:37,926 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 195.525698 ms
[INFO] 2018-10-25 02:29:38,057 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:29:38,075 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:29:38,076 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:29:38,076 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:29:38,078 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:29:38,088 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:29:38,162 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:29:38,192 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:29:38,194 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:26459 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:29:38,198 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:29:38,215 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:29:38,216 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[WARN] 2018-10-25 02:29:53,234 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:30:08,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:30:23,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:30:38,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:30:53,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:31:08,233 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:31:23,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:31:38,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:31:53,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:32:08,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:32:23,233 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:32:38,233 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:32:53,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:33:08,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:33:23,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:33:38,233 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:33:53,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:34:08,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:34:23,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:34:38,232 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:35:17,455 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:35:18,311 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:35:18,336 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:35:18,590 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:35:18,590 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:35:18,590 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:35:18,591 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:35:18,601 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:35:18,804 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 10316.
[INFO] 2018-10-25 02:35:18,830 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:35:18,850 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:35:18,853 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:35:18,854 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:35:18,870 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-a0865015-ca81-4a85-ba4a-aeaffe613d2a
[INFO] 2018-10-25 02:35:18,891 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:35:18,905 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:35:19,080 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 02:35:19,081 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 02:35:19,081 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[INFO] 2018-10-25 02:35:19,087 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4043.
[INFO] 2018-10-25 02:35:19,138 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4043
[INFO] 2018-10-25 02:35:19,230 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:10316/files/etl_config.json with timestamp 1540460119229
[INFO] 2018-10-25 02:35:19,232 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-6794a24f-77c5-4887-a24a-19dbf245b0b7/userFiles-08538e37-5a5e-406c-adc6-c1d0d1dcb66b/etl_config.json
[INFO] 2018-10-25 02:35:19,245 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:10316/files/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540460119245
[INFO] 2018-10-25 02:35:19,245 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py to /tmp/spark-6794a24f-77c5-4887-a24a-19dbf245b0b7/userFiles-08538e37-5a5e-406c-adc6-c1d0d1dcb66b/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:35:19,250 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:10316/files/packages.zip with timestamp 1540460119250
[INFO] 2018-10-25 02:35:19,251 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-6794a24f-77c5-4887-a24a-19dbf245b0b7/userFiles-08538e37-5a5e-406c-adc6-c1d0d1dcb66b/packages.zip
[INFO] 2018-10-25 02:35:19,331 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 02:35:19,403 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 42 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 02:35:19,506 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025023519-0008
[INFO] 2018-10-25 02:35:19,517 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10687.
[INFO] 2018-10-25 02:35:19,518 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:10687
[INFO] 2018-10-25 02:35:19,519 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:35:19,547 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 10687, None)
[INFO] 2018-10-25 02:35:19,551 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:10687 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 10687, None)
[INFO] 2018-10-25 02:35:19,554 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 10687, None)
[INFO] 2018-10-25 02:35:19,554 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 10687, None)
[INFO] 2018-10-25 02:35:19,699 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 02:35:19,989 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:35:19,990 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:35:20,405 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:35:23,643 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 203.176291 ms
[INFO] 2018-10-25 02:35:23,785 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:35:23,810 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:35:23,810 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:35:23,811 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:35:23,812 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:35:23,826 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:35:23,901 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:35:23,932 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:35:23,936 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:10687 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:35:23,938 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:35:23,957 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:35:23,958 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[WARN] 2018-10-25 02:35:38,976 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:35:53,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:36:08,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:36:23,975 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:36:38,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:36:53,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:37:08,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:37:23,975 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:37:38,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:37:53,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:38:08,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:38:23,975 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:38:38,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:38:53,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:39:08,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:39:52,493 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:39:53,323 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:39:53,351 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:39:53,677 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:39:53,677 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:39:53,678 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:39:53,678 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:39:53,692 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:39:53,895 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11112.
[INFO] 2018-10-25 02:39:53,927 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:39:53,947 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:39:53,950 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:39:53,951 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:39:53,960 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-977f501e-38a5-44fb-aa47-a33fabb161aa
[INFO] 2018-10-25 02:39:53,978 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:39:53,992 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:39:54,177 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 02:39:54,178 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 02:39:54,178 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 02:39:54,179 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[INFO] 2018-10-25 02:39:54,186 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4044.
[INFO] 2018-10-25 02:39:54,253 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4044
[INFO] 2018-10-25 02:39:54,371 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540460394370
[INFO] 2018-10-25 02:39:54,373 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-04ef9c2a-02be-40f0-88a3-8a302db9a13d/userFiles-583bac0b-0c84-43e9-a292-c6e6afb3a4f0/etl_config.json
[INFO] 2018-10-25 02:39:54,387 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540460394387
[INFO] 2018-10-25 02:39:54,387 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py to /tmp/spark-04ef9c2a-02be-40f0-88a3-8a302db9a13d/userFiles-583bac0b-0c84-43e9-a292-c6e6afb3a4f0/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:39:54,391 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540460394390
[INFO] 2018-10-25 02:39:54,391 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-04ef9c2a-02be-40f0-88a3-8a302db9a13d/userFiles-583bac0b-0c84-43e9-a292-c6e6afb3a4f0/packages.zip
[INFO] 2018-10-25 02:39:54,491 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 02:39:54,519 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23134.
[INFO] 2018-10-25 02:39:54,519 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:23134
[INFO] 2018-10-25 02:39:54,521 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:39:54,553 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 23134, None)
[INFO] 2018-10-25 02:39:54,564 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:23134 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 23134, None)
[INFO] 2018-10-25 02:39:54,567 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 23134, None)
[INFO] 2018-10-25 02:39:54,568 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 23134, None)
[INFO] 2018-10-25 02:39:54,864 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:39:54,864 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:39:55,411 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:39:58,487 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 212.357574 ms
[INFO] 2018-10-25 02:39:58,630 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:39:58,651 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:39:58,652 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:39:58,653 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:39:58,655 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:39:58,662 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:39:58,734 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:39:58,766 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:39:58,769 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:23134 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:39:58,772 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:39:58,786 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:39:58,786 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 02:39:58,829 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:39:58,841 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 02:39:58,845 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540460394370
[INFO] 2018-10-25 02:39:58,877 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-04ef9c2a-02be-40f0-88a3-8a302db9a13d/userFiles-583bac0b-0c84-43e9-a292-c6e6afb3a4f0/etl_config.json
[INFO] 2018-10-25 02:39:58,882 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540460394390
[INFO] 2018-10-25 02:39:58,883 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-04ef9c2a-02be-40f0-88a3-8a302db9a13d/userFiles-583bac0b-0c84-43e9-a292-c6e6afb3a4f0/packages.zip
[INFO] 2018-10-25 02:39:58,891 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py with timestamp 1540460394387
[INFO] 2018-10-25 02:39:58,892 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-04ef9c2a-02be-40f0-88a3-8a302db9a13d/userFiles-583bac0b-0c84-43e9-a292-c6e6afb3a4f0/JB_INVOICE_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:39:59,696 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:39:59,735 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 489, boot = 381, init = 107, finish = 1
[INFO] 2018-10-25 02:39:59,752 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 02:39:59,770 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 950 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:39:59,776 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:39:59,788 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.099 s
[INFO] 2018-10-25 02:39:59,793 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.162704 s
[INFO] 2018-10-25 02:40:00,156 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 02:40:00,159 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 02:40:00,159 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 02:40:00,160 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:40:00,160 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:40:00,161 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 02:40:00,166 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:40:00,168 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 02:40:00,170 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:23134 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:40:00,172 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:40:00,172 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:40:00,173 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 02:40:00,174 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:40:00,175 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 02:40:00,479 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:40:00,482 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 48, boot = -694, init = 742, finish = 0
[INFO] 2018-10-25 02:40:00,485 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 02:40:00,488 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 314 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:40:00,489 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:40:00,490 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.326 s
[INFO] 2018-10-25 02:40:00,490 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.333506 s
[INFO] 2018-10-25 02:40:00,812 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 02:40:00,813 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 02:40:00,813 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 02:40:00,814 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:40:00,814 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:40:00,814 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 02:40:00,817 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 02:40:00,819 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 02:40:00,820 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:23134 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:40:00,821 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:40:00,822 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:40:00,822 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 02:40:00,824 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:40:00,825 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 02:40:01,912 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:40:01,918 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -1388, init = 1429, finish = 0
[INFO] 2018-10-25 02:40:01,920 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1665 bytes result sent to driver
[INFO] 2018-10-25 02:40:01,923 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 1099 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:40:01,923 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:40:01,924 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 1.109 s
[INFO] 2018-10-25 02:40:01,925 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 1.113249 s
[INFO] 2018-10-25 02:40:02,307 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 02:40:02,327 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4044
[INFO] 2018-10-25 02:40:02,359 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 02:40:02,385 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 02:40:02,386 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 02:40:02,398 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 02:40:02,404 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 02:40:02,422 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 02:40:02,422 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:40:02,423 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-04ef9c2a-02be-40f0-88a3-8a302db9a13d
[INFO] 2018-10-25 02:40:02,424 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-946d10bc-7f4d-4d76-9584-5d3220e2dee4
[INFO] 2018-10-25 02:40:02,424 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-04ef9c2a-02be-40f0-88a3-8a302db9a13d/pyspark-87027d4e-01c3-418f-b836-152feada678f
[WARN] 2018-10-25 02:51:52,837 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:51:53,608 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:51:53,637 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:51:53,806 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:51:53,806 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:51:53,807 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:51:53,807 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:51:53,807 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:51:54,007 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 21967.
[INFO] 2018-10-25 02:51:54,033 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:51:54,052 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:51:54,055 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:51:54,055 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:51:54,064 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-a42c61b3-7aa2-47a4-a4e7-7171f95a4d91
[INFO] 2018-10-25 02:51:54,082 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:51:54,095 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:51:54,278 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 02:51:54,279 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 02:51:54,279 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 02:51:54,280 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[INFO] 2018-10-25 02:51:54,286 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4044.
[INFO] 2018-10-25 02:51:54,350 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4044
[INFO] 2018-10-25 02:51:54,457 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:21967/files/etl_config.json with timestamp 1540461114457
[INFO] 2018-10-25 02:51:54,459 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-502bbc63-9ac3-479a-a124-4f9cee8c481f/userFiles-c23c4931-43ae-46a4-8376-599d1d431446/etl_config.json
[INFO] 2018-10-25 02:51:54,476 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:21967/files/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540461114476
[INFO] 2018-10-25 02:51:54,476 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-502bbc63-9ac3-479a-a124-4f9cee8c481f/userFiles-c23c4931-43ae-46a4-8376-599d1d431446/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:51:54,481 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:21967/files/packages.zip with timestamp 1540461114481
[INFO] 2018-10-25 02:51:54,482 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-502bbc63-9ac3-479a-a124-4f9cee8c481f/userFiles-c23c4931-43ae-46a4-8376-599d1d431446/packages.zip
[INFO] 2018-10-25 02:51:54,567 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 02:51:54,632 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 33 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 02:51:54,742 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025025154-0009
[INFO] 2018-10-25 02:51:54,753 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30568.
[INFO] 2018-10-25 02:51:54,754 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:30568
[INFO] 2018-10-25 02:51:54,756 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:51:54,792 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 30568, None)
[INFO] 2018-10-25 02:51:54,799 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:30568 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 30568, None)
[INFO] 2018-10-25 02:51:54,802 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 30568, None)
[INFO] 2018-10-25 02:51:54,803 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 30568, None)
[INFO] 2018-10-25 02:51:54,980 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 02:51:55,183 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:51:55,184 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:51:55,645 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:51:58,580 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 195.684064 ms
[INFO] 2018-10-25 02:51:58,708 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:51:58,733 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:51:58,734 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:51:58,735 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:51:58,737 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:51:58,746 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:51:58,827 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:51:58,862 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:51:58,866 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:30568 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:51:58,870 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:51:58,892 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:51:58,893 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[WARN] 2018-10-25 02:52:13,912 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:52:31,396 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:52:32,297 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:52:32,325 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:52:32,633 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:52:32,634 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:52:32,634 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:52:32,634 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:52:32,643 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:52:32,869 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 35710.
[INFO] 2018-10-25 02:52:32,900 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:52:32,921 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:52:32,925 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:52:32,925 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:52:32,937 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-75b6c851-b911-4398-bc27-6f87df6d4c30
[INFO] 2018-10-25 02:52:32,957 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:52:32,972 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:52:33,168 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 02:52:33,169 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 02:52:33,169 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 02:52:33,170 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 02:52:33,170 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[INFO] 2018-10-25 02:52:33,177 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4045.
[INFO] 2018-10-25 02:52:33,238 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4045
[INFO] 2018-10-25 02:52:33,347 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:35710/files/etl_config.json with timestamp 1540461153347
[INFO] 2018-10-25 02:52:33,349 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-b1a11759-1ec6-47f8-ab7d-e4da62835f26/userFiles-a6ad0624-0b03-4b2e-af36-e76263928592/etl_config.json
[INFO] 2018-10-25 02:52:33,364 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:35710/files/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540461153364
[INFO] 2018-10-25 02:52:33,364 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-b1a11759-1ec6-47f8-ab7d-e4da62835f26/userFiles-a6ad0624-0b03-4b2e-af36-e76263928592/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:52:33,369 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:35710/files/packages.zip with timestamp 1540461153369
[INFO] 2018-10-25 02:52:33,370 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-b1a11759-1ec6-47f8-ab7d-e4da62835f26/userFiles-a6ad0624-0b03-4b2e-af36-e76263928592/packages.zip
[INFO] 2018-10-25 02:52:33,463 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 02:52:33,532 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 39 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 02:52:33,635 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025025233-0010
[INFO] 2018-10-25 02:52:33,646 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 13318.
[INFO] 2018-10-25 02:52:33,647 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:13318
[INFO] 2018-10-25 02:52:33,651 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:52:33,685 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 13318, None)
[INFO] 2018-10-25 02:52:33,693 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:13318 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 13318, None)
[INFO] 2018-10-25 02:52:33,698 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 13318, None)
[INFO] 2018-10-25 02:52:33,699 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 13318, None)
[INFO] 2018-10-25 02:52:33,886 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 02:52:34,159 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:52:34,160 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:52:34,593 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:52:37,638 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 194.348691 ms
[INFO] 2018-10-25 02:52:37,779 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:52:37,805 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:52:37,806 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:52:37,807 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:52:37,809 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:52:37,818 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:52:37,894 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:52:37,934 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:52:37,937 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:13318 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:52:37,940 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:52:37,954 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:52:37,955 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[WARN] 2018-10-25 02:52:52,974 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 02:53:10,930 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 02:53:11,734 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 02:53:11,759 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-25 02:53:11,960 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 02:53:11,961 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 02:53:11,961 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 02:53:11,961 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 02:53:11,968 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 02:53:12,164 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 9295.
[INFO] 2018-10-25 02:53:12,191 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 02:53:12,210 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 02:53:12,213 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 02:53:12,214 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 02:53:12,223 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1ce1ffa3-1df6-45a4-976c-c2ac52577046
[INFO] 2018-10-25 02:53:12,243 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 02:53:12,258 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 02:53:12,430 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 02:53:12,431 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 02:53:12,432 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 02:53:12,432 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 02:53:12,433 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 02:53:12,433 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[INFO] 2018-10-25 02:53:12,440 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4046.
[INFO] 2018-10-25 02:53:12,493 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 02:53:12,587 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540461192586
[INFO] 2018-10-25 02:53:12,589 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-1fe09294-eaef-4632-b383-9bba55d9e2f6/userFiles-dabfc680-60ff-4769-abff-3978ba5c8b44/etl_config.json
[INFO] 2018-10-25 02:53:12,601 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540461192601
[INFO] 2018-10-25 02:53:12,601 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-1fe09294-eaef-4632-b383-9bba55d9e2f6/userFiles-dabfc680-60ff-4769-abff-3978ba5c8b44/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:53:12,606 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540461192606
[INFO] 2018-10-25 02:53:12,607 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-1fe09294-eaef-4632-b383-9bba55d9e2f6/userFiles-dabfc680-60ff-4769-abff-3978ba5c8b44/packages.zip
[INFO] 2018-10-25 02:53:12,677 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 02:53:12,702 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27893.
[INFO] 2018-10-25 02:53:12,703 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:27893
[INFO] 2018-10-25 02:53:12,705 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 02:53:12,741 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 27893, None)
[INFO] 2018-10-25 02:53:12,752 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:27893 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 27893, None)
[INFO] 2018-10-25 02:53:12,755 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 27893, None)
[INFO] 2018-10-25 02:53:12,756 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 27893, None)
[INFO] 2018-10-25 02:53:13,065 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 02:53:13,065 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 02:53:13,606 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 02:53:16,695 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 195.980945 ms
[INFO] 2018-10-25 02:53:16,839 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 02:53:16,858 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 02:53:16,859 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 02:53:16,859 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:53:16,861 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:53:16,870 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 02:53:16,934 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:53:16,961 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 02:53:16,964 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:27893 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:53:16,967 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:53:16,979 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:53:16,980 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 02:53:17,033 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:53:17,045 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 02:53:17,052 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540461192601
[INFO] 2018-10-25 02:53:17,080 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-1fe09294-eaef-4632-b383-9bba55d9e2f6/userFiles-dabfc680-60ff-4769-abff-3978ba5c8b44/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 02:53:17,084 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540461192586
[INFO] 2018-10-25 02:53:17,085 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-1fe09294-eaef-4632-b383-9bba55d9e2f6/userFiles-dabfc680-60ff-4769-abff-3978ba5c8b44/etl_config.json
[INFO] 2018-10-25 02:53:17,089 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540461192606
[INFO] 2018-10-25 02:53:17,090 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-1fe09294-eaef-4632-b383-9bba55d9e2f6/userFiles-dabfc680-60ff-4769-abff-3978ba5c8b44/packages.zip
[INFO] 2018-10-25 02:53:17,842 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:53:17,874 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 474, boot = 385, init = 89, finish = 0
[INFO] 2018-10-25 02:53:17,894 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 02:53:17,916 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 895 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:53:17,923 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:53:17,928 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.037 s
[INFO] 2018-10-25 02:53:17,933 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.093755 s
[INFO] 2018-10-25 02:53:18,286 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 02:53:18,288 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 02:53:18,288 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 02:53:18,289 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:53:18,289 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:53:18,289 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 02:53:18,292 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 02:53:18,294 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 02:53:18,295 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:27893 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:53:18,297 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:53:18,298 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:53:18,298 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 02:53:18,300 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:53:18,301 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 02:53:18,560 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:53:18,574 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -649, init = 691, finish = 0
[INFO] 2018-10-25 02:53:18,576 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 02:53:18,580 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 281 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:53:18,580 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:53:18,582 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.292 s
[INFO] 2018-10-25 02:53:18,584 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.296744 s
[INFO] 2018-10-25 02:53:19,241 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 02:53:19,243 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 02:53:19,243 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 02:53:19,243 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 02:53:19,243 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 02:53:19,244 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 02:53:19,247 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 02:53:19,249 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 02:53:19,251 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:27893 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 02:53:19,251 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 02:53:19,252 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 02:53:19,252 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 02:53:19,255 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 02:53:19,256 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 02:53:19,990 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 02:53:20,001 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -1379, init = 1421, finish = 0
[INFO] 2018-10-25 02:53:20,004 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-25 02:53:20,006 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 752 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 02:53:20,007 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 02:53:20,009 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.763 s
[INFO] 2018-10-25 02:53:20,010 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.768043 s
[INFO] 2018-10-25 02:53:20,647 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 02:53:20,665 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 02:53:20,692 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 02:53:20,730 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 02:53:20,731 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 02:53:20,743 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 02:53:20,747 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 02:53:20,762 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 02:53:20,762 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 02:53:20,763 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1fe09294-eaef-4632-b383-9bba55d9e2f6
[INFO] 2018-10-25 02:53:20,764 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1fe09294-eaef-4632-b383-9bba55d9e2f6/pyspark-d9f85d9c-ff8f-4399-bbaf-8173fb7e236b
[INFO] 2018-10-25 02:53:20,764 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7681bb60-17a1-4353-82b7-5551e65d7fb2
[WARN] 2018-10-25 03:05:23,855 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 03:05:24,608 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 03:05:24,633 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-25 03:05:24,808 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 03:05:24,808 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 03:05:24,809 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 03:05:24,809 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 03:05:24,816 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 03:05:25,005 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 31105.
[INFO] 2018-10-25 03:05:25,030 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 03:05:25,050 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 03:05:25,052 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 03:05:25,053 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 03:05:25,063 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-cca2799f-bb0b-443f-bcf5-31da8a40d1bf
[INFO] 2018-10-25 03:05:25,080 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 03:05:25,094 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 03:05:25,263 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 03:05:25,264 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 03:05:25,264 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 03:05:25,265 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 03:05:25,265 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 03:05:25,266 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[INFO] 2018-10-25 03:05:25,275 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4046.
[INFO] 2018-10-25 03:05:25,326 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:05:25,418 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:31105/files/etl_config.json with timestamp 1540461925418
[INFO] 2018-10-25 03:05:25,420 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-d8c2f9e2-4649-4f28-8031-baaabec0625d/userFiles-42ad9006-9b71-4abb-9532-ae853d359de1/etl_config.json
[INFO] 2018-10-25 03:05:25,433 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:31105/files/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540461925433
[INFO] 2018-10-25 03:05:25,434 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-d8c2f9e2-4649-4f28-8031-baaabec0625d/userFiles-42ad9006-9b71-4abb-9532-ae853d359de1/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 03:05:25,440 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:31105/files/packages.zip with timestamp 1540461925440
[INFO] 2018-10-25 03:05:25,440 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-d8c2f9e2-4649-4f28-8031-baaabec0625d/userFiles-42ad9006-9b71-4abb-9532-ae853d359de1/packages.zip
[INFO] 2018-10-25 03:05:25,532 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 03:05:25,603 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 43 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 03:05:25,751 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025030525-0000
[INFO] 2018-10-25 03:05:25,763 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33260.
[INFO] 2018-10-25 03:05:25,765 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:33260
[INFO] 2018-10-25 03:05:25,768 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 03:05:25,770 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030525-0000/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:05:25,772 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030525-0000/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:05:25,801 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 33260, None)
[INFO] 2018-10-25 03:05:25,807 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:33260 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 33260, None)
[INFO] 2018-10-25 03:05:25,811 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 33260, None)
[INFO] 2018-10-25 03:05:25,812 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 33260, None)
[INFO] 2018-10-25 03:05:25,855 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/0 is now RUNNING
[INFO] 2018-10-25 03:05:25,986 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 03:05:26,211 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 03:05:26,212 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 03:05:26,683 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 03:05:27,810 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:46248) with ID 0
[ERROR] 2018-10-25 03:05:27,865 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7884268734878305368
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:05:27,899 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:05:27,906 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 0)
[INFO] 2018-10-25 03:05:27,919 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:27,920 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-25 03:05:27,922 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 0)
[INFO] 2018-10-25 03:05:28,244 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:05:28,249 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030525-0000/0 removed: Command exited with code 1
[INFO] 2018-10-25 03:05:28,250 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030525-0000/1 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:05:28,250 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:28,251 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030525-0000/1 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:05:28,251 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-25 03:05:28,253 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-25 03:05:28,262 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/1 is now RUNNING
[INFO] 2018-10-25 03:05:29,999 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 215.435308 ms
[INFO] 2018-10-25 03:05:30,091 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:46254) with ID 1
[INFO] 2018-10-25 03:05:30,133 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[ERROR] 2018-10-25 03:05:30,143 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9085056936701789313
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 03:05:30,151 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 03:05:30,152 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 03:05:30,152 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 03:05:30,154 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[ERROR] 2018-10-25 03:05:30,157 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:05:30,162 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 03:05:30,233 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 03:05:30,269 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 03:05:30,273 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:33260 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 03:05:30,276 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 03:05:30,290 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 03:05:30,291 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 03:05:30,310 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 1)
[INFO] 2018-10-25 03:05:30,311 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:30,311 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-25 03:05:30,311 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 1)
[INFO] 2018-10-25 03:05:30,513 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:05:30,514 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030525-0000/1 removed: Command exited with code 1
[INFO] 2018-10-25 03:05:30,514 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030525-0000/2 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:05:30,514 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:30,514 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-25 03:05:30,515 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030525-0000/2 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:05:30,515 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-25 03:05:30,527 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/2 is now RUNNING
[INFO] 2018-10-25 03:05:32,602 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:46260) with ID 2
[INFO] 2018-10-25 03:05:32,621 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 03:05:32,661 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6864405517193904776
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:05:32,678 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 03:05:32,689 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:05:32,698 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 2)
[INFO] 2018-10-25 03:05:32,699 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:32,699 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-25 03:05:32,700 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 2)
[INFO] 2018-10-25 03:05:33,031 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:05:33,031 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030525-0000/2 removed: Command exited with code 1
[INFO] 2018-10-25 03:05:33,033 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030525-0000/3 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:05:33,033 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-25 03:05:33,033 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:33,034 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030525-0000/3 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:05:33,034 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-25 03:05:33,061 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/3 is now RUNNING
[INFO] 2018-10-25 03:05:34,887 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:46266) with ID 3
[INFO] 2018-10-25 03:05:34,889 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 03:05:34,941 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5160880323212277006
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:05:34,955 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 03:05:34,955 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:05:34,956 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 3)
[INFO] 2018-10-25 03:05:34,957 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:34,957 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-25 03:05:34,958 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 3)
[INFO] 2018-10-25 03:05:35,312 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/3 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:05:35,313 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030525-0000/3 removed: Command exited with code 1
[INFO] 2018-10-25 03:05:35,314 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030525-0000/4 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:05:35,314 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-25 03:05:35,314 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:35,315 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030525-0000/4 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:05:35,315 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-25 03:05:35,320 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/4 is now RUNNING
[INFO] 2018-10-25 03:05:37,122 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:46272) with ID 4
[INFO] 2018-10-25 03:05:37,125 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 03:05:37,172 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6279974406039579951
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:05:37,186 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 03:05:37,187 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:05:37,188 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 4)
[INFO] 2018-10-25 03:05:37,189 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:37,189 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-25 03:05:37,190 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 4)
[INFO] 2018-10-25 03:05:37,530 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:05:37,531 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030525-0000/4 removed: Command exited with code 1
[INFO] 2018-10-25 03:05:37,532 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:37,532 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-25 03:05:37,532 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030525-0000/5 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:05:37,532 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-25 03:05:37,533 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030525-0000/5 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:05:37,538 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030525-0000/5 is now RUNNING
[INFO] 2018-10-25 03:05:39,312 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:46278) with ID 5
[INFO] 2018-10-25 03:05:39,314 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 03:05:39,354 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6184985499692418898
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:05:39,366 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 03:05:39,367 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-25 03:05:39,370 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-25 03:05:39,373 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 03:05:39,378 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-25 03:05:39,380 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 9.199 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-25 03:05:39,385 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 5)
[INFO] 2018-10-25 03:05:39,386 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-25 03:05:39,387 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 9.252396 s
[INFO] 2018-10-25 03:05:39,387 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-25 03:05:39,388 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 5)
[INFO] 2018-10-25 03:05:39,445 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 03:05:39,458 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:05:39,465 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 03:05:39,466 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 03:05:39,480 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 03:05:39,495 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 03:05:39,496 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 03:05:39,497 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 03:05:39,500 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 03:05:39,509 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 03:05:39,510 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 03:05:39,512 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d8c2f9e2-4649-4f28-8031-baaabec0625d
[INFO] 2018-10-25 03:05:39,513 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4400491a-50f6-4650-b52e-dce4ca3e0de7
[INFO] 2018-10-25 03:05:39,513 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d8c2f9e2-4649-4f28-8031-baaabec0625d/pyspark-6f3071d9-f6b8-4ce3-b13e-f48e1cd41928
[WARN] 2018-10-25 03:06:06,782 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 03:06:07,529 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 03:06:07,554 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-25 03:06:07,725 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 03:06:07,726 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 03:06:07,726 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 03:06:07,726 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 03:06:07,727 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 03:06:07,960 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 35968.
[INFO] 2018-10-25 03:06:07,991 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 03:06:08,014 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 03:06:08,017 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 03:06:08,018 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 03:06:08,028 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-ecb1c703-ebbc-4bef-b8b3-7ac3864842c1
[INFO] 2018-10-25 03:06:08,049 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 03:06:08,065 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 03:06:08,245 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 03:06:08,246 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 03:06:08,247 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 03:06:08,247 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 03:06:08,248 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 03:06:08,248 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[INFO] 2018-10-25 03:06:08,254 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4046.
[INFO] 2018-10-25 03:06:08,324 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:06:08,420 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540461968419
[INFO] 2018-10-25 03:06:08,422 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-b94f30b6-15a6-41b5-8861-f458f51817c5/userFiles-fe240c89-73ba-408a-b82b-44ea1de35cb2/etl_config.json
[INFO] 2018-10-25 03:06:08,435 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540461968435
[INFO] 2018-10-25 03:06:08,436 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-b94f30b6-15a6-41b5-8861-f458f51817c5/userFiles-fe240c89-73ba-408a-b82b-44ea1de35cb2/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 03:06:08,441 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540461968441
[INFO] 2018-10-25 03:06:08,442 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-b94f30b6-15a6-41b5-8861-f458f51817c5/userFiles-fe240c89-73ba-408a-b82b-44ea1de35cb2/packages.zip
[INFO] 2018-10-25 03:06:08,529 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 03:06:08,556 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 16152.
[INFO] 2018-10-25 03:06:08,557 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:16152
[INFO] 2018-10-25 03:06:08,559 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 03:06:08,592 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 16152, None)
[INFO] 2018-10-25 03:06:08,596 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:16152 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 16152, None)
[INFO] 2018-10-25 03:06:08,600 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 16152, None)
[INFO] 2018-10-25 03:06:08,601 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 16152, None)
[INFO] 2018-10-25 03:06:08,937 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 03:06:08,937 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 03:06:09,505 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 03:06:12,746 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 193.941661 ms
[INFO] 2018-10-25 03:06:12,888 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 03:06:12,914 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 03:06:12,915 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 03:06:12,915 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 03:06:12,917 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 03:06:12,926 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 03:06:13,000 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 03:06:13,034 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 03:06:13,036 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:16152 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 03:06:13,040 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 03:06:13,056 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 03:06:13,057 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 03:06:13,108 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 03:06:13,121 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 03:06:13,125 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540461968435
[INFO] 2018-10-25 03:06:13,154 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-b94f30b6-15a6-41b5-8861-f458f51817c5/userFiles-fe240c89-73ba-408a-b82b-44ea1de35cb2/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 03:06:13,159 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540461968419
[INFO] 2018-10-25 03:06:13,160 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-b94f30b6-15a6-41b5-8861-f458f51817c5/userFiles-fe240c89-73ba-408a-b82b-44ea1de35cb2/etl_config.json
[INFO] 2018-10-25 03:06:13,166 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540461968441
[INFO] 2018-10-25 03:06:13,166 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-b94f30b6-15a6-41b5-8861-f458f51817c5/userFiles-fe240c89-73ba-408a-b82b-44ea1de35cb2/packages.zip
[INFO] 2018-10-25 03:06:13,949 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 03:06:13,985 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 461, boot = 359, init = 102, finish = 0
[INFO] 2018-10-25 03:06:14,008 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 03:06:14,027 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 930 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 03:06:14,032 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 03:06:14,042 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.094 s
[INFO] 2018-10-25 03:06:14,046 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.157622 s
[INFO] 2018-10-25 03:06:14,469 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 03:06:14,471 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 03:06:14,472 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 03:06:14,472 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 03:06:14,472 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 03:06:14,473 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 03:06:14,479 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 03:06:14,481 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 03:06:14,482 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:16152 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 03:06:14,484 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 03:06:14,485 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 03:06:14,485 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 03:06:14,487 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 03:06:14,488 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 03:06:14,779 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 03:06:14,783 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -748, init = 790, finish = 0
[INFO] 2018-10-25 03:06:14,786 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 03:06:14,790 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 304 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 03:06:14,790 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 03:06:14,793 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.318 s
[INFO] 2018-10-25 03:06:14,794 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.324182 s
[INFO] 2018-10-25 03:06:15,137 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 03:06:15,138 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 03:06:15,139 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 03:06:15,139 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 03:06:15,140 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 03:06:15,141 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 03:06:15,146 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 03:06:15,149 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 03:06:15,150 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:16152 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 03:06:15,151 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 03:06:15,153 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 03:06:15,153 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 03:06:15,155 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 03:06:15,156 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 03:06:15,821 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 03:06:15,829 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 68, boot = -967, init = 1035, finish = 0
[INFO] 2018-10-25 03:06:15,832 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
[INFO] 2018-10-25 03:06:15,836 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 681 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 03:06:15,837 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 03:06:15,839 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.696 s
[INFO] 2018-10-25 03:06:15,842 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.704320 s
[INFO] 2018-10-25 03:06:16,262 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 03:06:16,275 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:06:16,292 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 03:06:16,319 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 03:06:16,320 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 03:06:16,333 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 03:06:16,339 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 03:06:16,360 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 03:06:16,361 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 03:06:16,362 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b94f30b6-15a6-41b5-8861-f458f51817c5/pyspark-b8b21ffb-af72-468e-b27c-3ab65ebefd54
[INFO] 2018-10-25 03:06:16,362 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-19b0a505-2ef6-47fa-abfa-95f7d45ff8d5
[INFO] 2018-10-25 03:06:16,363 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b94f30b6-15a6-41b5-8861-f458f51817c5
[WARN] 2018-10-25 03:06:33,286 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 03:06:34,054 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 03:06:34,079 org.apache.spark.SparkContext logInfo - Submitted application: JB_INVOICE_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-25 03:06:34,279 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 03:06:34,279 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 03:06:34,280 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 03:06:34,280 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 03:06:34,280 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 03:06:34,510 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 12605.
[INFO] 2018-10-25 03:06:34,537 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 03:06:34,559 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 03:06:34,562 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 03:06:34,563 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 03:06:34,574 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-c5f2cf14-ae56-415f-97f0-f5320d1744bb
[INFO] 2018-10-25 03:06:34,594 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 03:06:34,609 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 03:06:34,781 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 03:06:34,781 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 03:06:34,782 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 03:06:34,782 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 03:06:34,783 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 03:06:34,783 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[INFO] 2018-10-25 03:06:34,789 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4046.
[INFO] 2018-10-25 03:06:34,844 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:06:34,935 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:12605/files/etl_config.json with timestamp 1540461994934
[INFO] 2018-10-25 03:06:34,937 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-61792c3b-32da-4c00-9036-e5b3b7496c17/userFiles-2f95a81e-7339-4011-b276-db122e47c3d2/etl_config.json
[INFO] 2018-10-25 03:06:34,949 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py at spark://vmwebietl02-dev:12605/files/JB_INVOICE_TIER_PARTITION_TRUNCATE.py with timestamp 1540461994949
[INFO] 2018-10-25 03:06:34,950 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_INVOICE_TIER_PARTITION_TRUNCATE.py to /tmp/spark-61792c3b-32da-4c00-9036-e5b3b7496c17/userFiles-2f95a81e-7339-4011-b276-db122e47c3d2/JB_INVOICE_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 03:06:34,955 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:12605/files/packages.zip with timestamp 1540461994955
[INFO] 2018-10-25 03:06:34,956 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-61792c3b-32da-4c00-9036-e5b3b7496c17/userFiles-2f95a81e-7339-4011-b276-db122e47c3d2/packages.zip
[INFO] 2018-10-25 03:06:35,052 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 03:06:35,123 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 39 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 03:06:35,239 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025030635-0001
[INFO] 2018-10-25 03:06:35,243 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030635-0001/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:06:35,245 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030635-0001/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:06:35,250 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37053.
[INFO] 2018-10-25 03:06:35,251 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:37053
[INFO] 2018-10-25 03:06:35,253 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 03:06:35,260 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/0 is now RUNNING
[INFO] 2018-10-25 03:06:35,284 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 37053, None)
[INFO] 2018-10-25 03:06:35,290 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:37053 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 37053, None)
[INFO] 2018-10-25 03:06:35,295 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 37053, None)
[INFO] 2018-10-25 03:06:35,296 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 37053, None)
[INFO] 2018-10-25 03:06:35,468 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 03:06:35,680 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 03:06:35,681 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 03:06:36,118 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 03:06:37,256 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:60094) with ID 0
[ERROR] 2018-10-25 03:06:37,313 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7977754494617849002
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:06:37,349 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:06:37,355 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 0)
[INFO] 2018-10-25 03:06:37,368 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:37,370 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-25 03:06:37,371 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 0)
[INFO] 2018-10-25 03:06:37,703 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:06:37,704 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030635-0001/0 removed: Command exited with code 1
[INFO] 2018-10-25 03:06:37,705 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030635-0001/1 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:06:37,705 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030635-0001/1 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:06:37,705 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/1 is now RUNNING
[INFO] 2018-10-25 03:06:37,706 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:37,706 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-25 03:06:37,708 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-25 03:06:39,372 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 202.619079 ms
[INFO] 2018-10-25 03:06:39,495 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 03:06:39,516 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 03:06:39,517 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 03:06:39,517 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 03:06:39,519 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 03:06:39,534 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 03:06:39,595 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:60100) with ID 1
[INFO] 2018-10-25 03:06:39,599 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 03:06:39,630 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 03:06:39,633 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:37053 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 03:06:39,636 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[ERROR] 2018-10-25 03:06:39,643 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8934026715500004955
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 03:06:39,649 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 03:06:39,650 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[ERROR] 2018-10-25 03:06:39,667 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:06:39,673 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 1)
[INFO] 2018-10-25 03:06:39,673 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:39,674 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-25 03:06:39,674 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 1)
[INFO] 2018-10-25 03:06:40,012 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:06:40,013 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030635-0001/1 removed: Command exited with code 1
[INFO] 2018-10-25 03:06:40,013 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030635-0001/2 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:06:40,013 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-25 03:06:40,013 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:40,014 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030635-0001/2 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:06:40,014 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-25 03:06:40,018 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/2 is now RUNNING
[INFO] 2018-10-25 03:06:41,887 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:60106) with ID 2
[INFO] 2018-10-25 03:06:41,905 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 03:06:41,946 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8598795989860151815
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:06:41,960 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 03:06:41,966 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:06:41,976 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 2)
[INFO] 2018-10-25 03:06:41,977 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:41,977 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-25 03:06:41,978 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 2)
[INFO] 2018-10-25 03:06:42,312 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/2 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:06:42,313 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030635-0001/2 removed: Command exited with code 1
[INFO] 2018-10-25 03:06:42,313 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030635-0001/3 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:06:42,313 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-25 03:06:42,313 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:42,314 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030635-0001/3 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:06:42,314 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-25 03:06:42,321 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/3 is now RUNNING
[INFO] 2018-10-25 03:06:44,192 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:60112) with ID 3
[INFO] 2018-10-25 03:06:44,195 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 03:06:44,237 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5430643020258356242
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:06:44,251 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 03:06:44,252 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:06:44,253 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 3)
[INFO] 2018-10-25 03:06:44,253 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:44,254 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-25 03:06:44,254 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 3)
[INFO] 2018-10-25 03:06:44,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/3 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:06:44,601 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030635-0001/3 removed: Command exited with code 1
[INFO] 2018-10-25 03:06:44,602 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030635-0001/4 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:06:44,602 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:44,602 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-25 03:06:44,602 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030635-0001/4 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:06:44,603 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-25 03:06:44,607 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/4 is now RUNNING
[INFO] 2018-10-25 03:06:46,565 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:60118) with ID 4
[INFO] 2018-10-25 03:06:46,567 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 03:06:46,609 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5627209269608961559
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:06:46,623 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 03:06:46,624 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 03:06:46,625 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 4)
[INFO] 2018-10-25 03:06:46,625 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:46,626 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-25 03:06:46,626 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 4)
[INFO] 2018-10-25 03:06:46,972 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 03:06:46,973 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025030635-0001/4 removed: Command exited with code 1
[INFO] 2018-10-25 03:06:46,974 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025030635-0001/5 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:06:46,974 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-25 03:06:46,974 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:46,974 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-25 03:06:46,976 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025030635-0001/5 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:06:46,980 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025030635-0001/5 is now RUNNING
[INFO] 2018-10-25 03:06:48,770 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:60124) with ID 5
[INFO] 2018-10-25 03:06:48,772 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 03:06:48,818 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8773604815629272777
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 03:06:48,834 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 03:06:48,835 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-25 03:06:48,838 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-25 03:06:48,841 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 03:06:48,845 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-25 03:06:48,849 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 9.295 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-25 03:06:48,854 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 5)
[INFO] 2018-10-25 03:06:48,854 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-25 03:06:48,855 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-25 03:06:48,855 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 9.358201 s
[INFO] 2018-10-25 03:06:48,856 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 5)
[INFO] 2018-10-25 03:06:48,914 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 03:06:48,930 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:06:48,939 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 03:06:48,940 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 03:06:48,961 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 03:06:48,988 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 03:06:48,989 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 03:06:48,990 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 03:06:48,995 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 03:06:49,006 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 03:06:49,007 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 03:06:49,009 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b46ddc07-5ecb-401d-8f44-259d0515a234
[INFO] 2018-10-25 03:06:49,009 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-61792c3b-32da-4c00-9036-e5b3b7496c17
[INFO] 2018-10-25 03:06:49,010 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-61792c3b-32da-4c00-9036-e5b3b7496c17/pyspark-b0cda551-4542-4069-8025-5c7a96ff7ca1
[WARN] 2018-10-25 03:12:14,179 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 03:12:14,940 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 03:12:14,967 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE_DATAPREP_TRUNCATE
[INFO] 2018-10-25 03:12:15,205 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 03:12:15,205 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 03:12:15,206 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 03:12:15,206 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 03:12:15,206 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 03:12:15,422 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 16549.
[INFO] 2018-10-25 03:12:15,447 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 03:12:15,467 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 03:12:15,470 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 03:12:15,471 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 03:12:15,480 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-56ebffb0-a53d-4b06-8910-009c1a3699a5
[INFO] 2018-10-25 03:12:15,497 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 03:12:15,512 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 03:12:15,687 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 03:12:15,688 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 03:12:15,689 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 03:12:15,689 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 03:12:15,690 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 03:12:15,690 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[INFO] 2018-10-25 03:12:15,698 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4046.
[INFO] 2018-10-25 03:12:15,746 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:12:15,838 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:16549/files/etl_config.json with timestamp 1540462335838
[INFO] 2018-10-25 03:12:15,840 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-af02550b-0764-4d79-be90-de9d446540e6/userFiles-a5aaddd6-c240-4d56-92ab-352ab4b9e152/etl_config.json
[INFO] 2018-10-25 03:12:15,853 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py at spark://vmwebietl02-dev:16549/files/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py with timestamp 1540462335853
[INFO] 2018-10-25 03:12:15,853 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py to /tmp/spark-af02550b-0764-4d79-be90-de9d446540e6/userFiles-a5aaddd6-c240-4d56-92ab-352ab4b9e152/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py
[INFO] 2018-10-25 03:12:15,859 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:16549/files/packages.zip with timestamp 1540462335859
[INFO] 2018-10-25 03:12:15,859 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-af02550b-0764-4d79-be90-de9d446540e6/userFiles-a5aaddd6-c240-4d56-92ab-352ab4b9e152/packages.zip
[INFO] 2018-10-25 03:12:15,946 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 03:12:16,014 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 38 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 03:12:16,137 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025031216-0002
[INFO] 2018-10-25 03:12:16,141 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025031216-0002/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 03:12:16,143 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025031216-0002/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 03:12:16,149 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35101.
[INFO] 2018-10-25 03:12:16,150 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:35101
[INFO] 2018-10-25 03:12:16,152 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 03:12:16,154 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025031216-0002/0 is now RUNNING
[INFO] 2018-10-25 03:12:16,184 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 35101, None)
[INFO] 2018-10-25 03:12:16,189 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:35101 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 35101, None)
[INFO] 2018-10-25 03:12:16,194 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 35101, None)
[INFO] 2018-10-25 03:12:16,195 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 35101, None)
[INFO] 2018-10-25 03:12:16,363 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 03:12:16,583 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 03:12:16,583 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 03:12:17,027 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 03:12:17,505 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 03:12:17,518 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:12:17,529 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 03:12:17,535 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 03:12:17,550 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 03:12:17,562 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 03:12:17,563 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 03:12:17,573 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 03:12:17,576 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 03:12:17,588 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 03:12:17,589 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 03:12:17,590 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-af02550b-0764-4d79-be90-de9d446540e6
[INFO] 2018-10-25 03:12:17,591 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-af02550b-0764-4d79-be90-de9d446540e6/pyspark-268ee162-97cf-493f-9c83-cc8afdefa246
[INFO] 2018-10-25 03:12:17,591 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c570f18f-6e50-40b7-a6eb-7b293ab59054
[WARN] 2018-10-25 03:40:37,180 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 03:40:37,952 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 03:40:37,978 org.apache.spark.SparkContext logInfo - Submitted application: jobs/SCH/JB_WORK_TO_BOOKINGS
[INFO] 2018-10-25 03:40:38,185 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 03:40:38,185 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 03:40:38,186 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 03:40:38,186 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 03:40:38,186 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 03:40:38,382 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11054.
[INFO] 2018-10-25 03:40:38,406 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 03:40:38,425 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 03:40:38,428 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 03:40:38,428 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 03:40:38,437 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-ea2cf207-55c1-4348-be6e-bf139d7e3d78
[INFO] 2018-10-25 03:40:38,453 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 03:40:38,467 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 03:40:38,633 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 03:40:38,634 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 03:40:38,635 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 03:40:38,635 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 03:40:38,635 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 03:40:38,636 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[INFO] 2018-10-25 03:40:38,642 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4046.
[INFO] 2018-10-25 03:40:38,693 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 03:40:38,784 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540464038784
[INFO] 2018-10-25 03:40:38,786 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-57ce0872-2758-4f56-b35e-c08b9fb6b488/userFiles-335ba8f8-a470-401b-b567-8f36c5cc04c2/etl_config.json
[INFO] 2018-10-25 03:40:38,799 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py with timestamp 1540464038799
[INFO] 2018-10-25 03:40:38,799 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py to /tmp/spark-57ce0872-2758-4f56-b35e-c08b9fb6b488/userFiles-335ba8f8-a470-401b-b567-8f36c5cc04c2/JB_WORK_TO_BOOKINGS.py
[INFO] 2018-10-25 03:40:38,804 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540464038804
[INFO] 2018-10-25 03:40:38,805 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-57ce0872-2758-4f56-b35e-c08b9fb6b488/userFiles-335ba8f8-a470-401b-b567-8f36c5cc04c2/packages.zip
[INFO] 2018-10-25 03:40:38,869 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 03:40:38,892 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34012.
[INFO] 2018-10-25 03:40:38,893 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:34012
[INFO] 2018-10-25 03:40:38,895 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 03:40:38,929 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 03:40:38,933 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:34012 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 03:40:38,936 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 03:40:38,936 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 03:40:39,194 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 03:40:39,194 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 03:40:39,763 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[WARN] 2018-10-25 04:08:29,984 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:08:30,802 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:08:30,827 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP_TRUNCATE
[INFO] 2018-10-25 04:08:31,031 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:08:31,032 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:08:31,032 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:08:31,032 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:08:31,039 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:08:31,226 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 37031.
[INFO] 2018-10-25 04:08:31,251 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:08:31,270 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:08:31,273 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:08:31,274 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:08:31,282 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-633ab77c-ccfa-4859-a11e-e32cc3c6fb99
[INFO] 2018-10-25 04:08:31,299 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:08:31,312 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:08:31,478 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:08:31,479 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:08:31,480 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:08:31,480 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:08:31,481 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:08:31,481 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:08:31,482 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:08:31,489 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:08:31,537 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:08:31,626 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540465711625
[INFO] 2018-10-25 04:08:31,628 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-296be909-55ae-4544-906d-f72f70e36357/userFiles-5954c9d6-f09a-4c8e-92f6-a73546169638/etl_config.json
[INFO] 2018-10-25 04:08:31,639 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py with timestamp 1540465711639
[INFO] 2018-10-25 04:08:31,640 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py to /tmp/spark-296be909-55ae-4544-906d-f72f70e36357/userFiles-5954c9d6-f09a-4c8e-92f6-a73546169638/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py
[INFO] 2018-10-25 04:08:31,645 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540465711645
[INFO] 2018-10-25 04:08:31,645 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-296be909-55ae-4544-906d-f72f70e36357/userFiles-5954c9d6-f09a-4c8e-92f6-a73546169638/packages.zip
[INFO] 2018-10-25 04:08:31,724 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:08:31,752 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 20576.
[INFO] 2018-10-25 04:08:31,753 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:20576
[INFO] 2018-10-25 04:08:31,757 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:08:31,798 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 20576, None)
[INFO] 2018-10-25 04:08:31,805 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:20576 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 20576, None)
[INFO] 2018-10-25 04:08:31,808 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 20576, None)
[INFO] 2018-10-25 04:08:31,809 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 20576, None)
[INFO] 2018-10-25 04:08:32,071 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:08:32,071 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:08:32,597 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:08:32,938 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:08:32,950 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:08:32,963 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:08:32,980 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:08:32,981 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:08:32,994 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:08:32,998 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:08:33,015 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:08:33,015 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:08:33,017 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-296be909-55ae-4544-906d-f72f70e36357/pyspark-a621a156-ecd4-4bf6-96fa-81d3e9022682
[INFO] 2018-10-25 04:08:33,017 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ef975911-94c9-42ac-9f9d-0d55649ba354
[INFO] 2018-10-25 04:08:33,018 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-296be909-55ae-4544-906d-f72f70e36357
[WARN] 2018-10-25 04:10:48,075 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:10:48,829 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:10:48,856 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-25 04:10:48,994 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:10:48,995 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:10:48,995 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:10:48,996 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:10:48,996 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:10:49,191 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 32374.
[INFO] 2018-10-25 04:10:49,214 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:10:49,233 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:10:49,236 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:10:49,236 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:10:49,245 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1a0dac8c-336d-45b5-89ec-98717ca4f5f9
[INFO] 2018-10-25 04:10:49,262 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:10:49,275 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:10:49,436 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:10:49,437 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:10:49,437 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:10:49,438 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:10:49,438 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:10:49,439 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:10:49,439 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:10:49,445 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:10:49,492 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:10:49,582 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540465849582
[INFO] 2018-10-25 04:10:49,584 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-ec063d9b-9bb9-409a-b688-aa72e1e59f4e/userFiles-937c8d2d-01a6-4d3a-a657-5fe368ad8b08/etl_config.json
[INFO] 2018-10-25 04:10:49,597 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540465849597
[INFO] 2018-10-25 04:10:49,597 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-ec063d9b-9bb9-409a-b688-aa72e1e59f4e/userFiles-937c8d2d-01a6-4d3a-a657-5fe368ad8b08/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-25 04:10:49,601 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540465849601
[INFO] 2018-10-25 04:10:49,601 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-ec063d9b-9bb9-409a-b688-aa72e1e59f4e/userFiles-937c8d2d-01a6-4d3a-a657-5fe368ad8b08/packages.zip
[INFO] 2018-10-25 04:10:49,669 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:10:49,699 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15589.
[INFO] 2018-10-25 04:10:49,700 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:15589
[INFO] 2018-10-25 04:10:49,702 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:10:49,735 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 15589, None)
[INFO] 2018-10-25 04:10:49,742 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:15589 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 15589, None)
[INFO] 2018-10-25 04:10:49,746 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 15589, None)
[INFO] 2018-10-25 04:10:49,747 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 15589, None)
[INFO] 2018-10-25 04:10:50,062 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:10:50,063 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:10:50,615 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:10:51,878 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:10:51,892 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:10:51,906 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:10:51,915 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:10:51,916 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:10:51,929 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:10:51,934 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:10:51,946 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:10:51,947 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:10:51,949 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ec063d9b-9bb9-409a-b688-aa72e1e59f4e
[INFO] 2018-10-25 04:10:51,950 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-41b4b0da-633b-48cf-81f7-e9c5b0228281
[INFO] 2018-10-25 04:10:51,951 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ec063d9b-9bb9-409a-b688-aa72e1e59f4e/pyspark-e3ae2b46-222f-4516-9182-58f3fcd7566c
[WARN] 2018-10-25 04:11:09,577 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:11:10,383 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:11:10,408 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_TRUNCATE
[INFO] 2018-10-25 04:11:10,586 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:11:10,586 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:11:10,587 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:11:10,587 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:11:10,588 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:11:10,827 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 18122.
[INFO] 2018-10-25 04:11:10,852 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:11:10,872 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:11:10,875 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:11:10,875 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:11:10,885 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-179ed1cb-b29c-446c-9c59-3044615356e4
[INFO] 2018-10-25 04:11:10,902 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:11:10,918 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:11:11,095 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:11:11,096 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:11:11,096 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:11:11,097 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:11:11,097 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:11:11,098 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:11:11,099 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:11:11,105 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:11:11,182 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:11:11,301 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540465871300
[INFO] 2018-10-25 04:11:11,303 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-8893bce0-081d-4d80-8a1f-64bf2618fb32/userFiles-45c69ade-3aac-4fcb-b522-54cd31893f0f/etl_config.json
[INFO] 2018-10-25 04:11:11,319 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py with timestamp 1540465871319
[INFO] 2018-10-25 04:11:11,320 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py to /tmp/spark-8893bce0-081d-4d80-8a1f-64bf2618fb32/userFiles-45c69ade-3aac-4fcb-b522-54cd31893f0f/JB_WORK_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-25 04:11:11,324 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540465871324
[INFO] 2018-10-25 04:11:11,325 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-8893bce0-081d-4d80-8a1f-64bf2618fb32/userFiles-45c69ade-3aac-4fcb-b522-54cd31893f0f/packages.zip
[INFO] 2018-10-25 04:11:11,380 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:11:11,404 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11946.
[INFO] 2018-10-25 04:11:11,404 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:11946
[INFO] 2018-10-25 04:11:11,406 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:11:11,434 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 11946, None)
[INFO] 2018-10-25 04:11:11,440 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:11946 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 11946, None)
[INFO] 2018-10-25 04:11:11,445 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 11946, None)
[INFO] 2018-10-25 04:11:11,446 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 11946, None)
[INFO] 2018-10-25 04:11:11,695 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:11:11,696 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:11:12,229 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:11:13,036 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:11:13,049 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:11:13,066 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:11:13,075 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:11:13,076 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:11:13,088 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:11:13,092 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:11:13,100 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:11:13,101 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:11:13,102 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1d30374a-a79c-4e1a-b6a0-34be063f0766
[INFO] 2018-10-25 04:11:13,103 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8893bce0-081d-4d80-8a1f-64bf2618fb32
[INFO] 2018-10-25 04:11:13,103 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8893bce0-081d-4d80-8a1f-64bf2618fb32/pyspark-66901d8c-7809-4cfa-9ee4-4fecad677adb
[WARN] 2018-10-25 04:11:23,529 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:11:24,331 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:11:24,359 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP_TRUNCATE
[INFO] 2018-10-25 04:11:24,557 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:11:24,557 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:11:24,558 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:11:24,558 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:11:24,559 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:11:24,763 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 19429.
[INFO] 2018-10-25 04:11:24,790 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:11:24,809 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:11:24,812 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:11:24,813 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:11:24,822 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-20c62e62-d9be-41c6-a24f-2f71bafad7be
[INFO] 2018-10-25 04:11:24,840 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:11:24,854 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:11:25,033 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:11:25,034 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:11:25,034 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:11:25,035 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:11:25,035 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:11:25,036 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:11:25,036 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:11:25,042 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:11:25,093 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:11:25,186 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540465885185
[INFO] 2018-10-25 04:11:25,188 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-ee271aa4-c26c-405d-804d-3ab922113636/userFiles-b1916709-8408-4d67-bd7d-8dbec0ac9b2f/etl_config.json
[INFO] 2018-10-25 04:11:25,200 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py with timestamp 1540465885200
[INFO] 2018-10-25 04:11:25,200 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py to /tmp/spark-ee271aa4-c26c-405d-804d-3ab922113636/userFiles-b1916709-8408-4d67-bd7d-8dbec0ac9b2f/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py
[INFO] 2018-10-25 04:11:25,205 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540465885204
[INFO] 2018-10-25 04:11:25,205 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-ee271aa4-c26c-405d-804d-3ab922113636/userFiles-b1916709-8408-4d67-bd7d-8dbec0ac9b2f/packages.zip
[INFO] 2018-10-25 04:11:25,279 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:11:25,305 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25629.
[INFO] 2018-10-25 04:11:25,306 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:25629
[INFO] 2018-10-25 04:11:25,308 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:11:25,343 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25629, None)
[INFO] 2018-10-25 04:11:25,350 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:25629 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 25629, None)
[INFO] 2018-10-25 04:11:25,353 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25629, None)
[INFO] 2018-10-25 04:11:25,354 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 25629, None)
[INFO] 2018-10-25 04:11:25,647 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:11:25,647 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:11:26,231 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:11:26,622 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:11:26,635 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:11:26,647 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:11:26,661 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:11:26,662 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:11:26,673 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:11:26,678 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:11:26,686 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:11:26,687 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:11:26,688 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ee271aa4-c26c-405d-804d-3ab922113636
[INFO] 2018-10-25 04:11:26,689 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-44f6af4e-8b88-4430-b78e-d38c1ead22b8
[INFO] 2018-10-25 04:11:26,689 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ee271aa4-c26c-405d-804d-3ab922113636/pyspark-ab14e042-5f6a-4ad3-ba77-de41e4326766
[WARN] 2018-10-25 04:11:59,336 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:12:00,161 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:12:00,189 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP
[INFO] 2018-10-25 04:12:00,409 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:12:00,409 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:12:00,410 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:12:00,410 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:12:00,410 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:12:00,645 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 15692.
[INFO] 2018-10-25 04:12:00,678 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:12:00,701 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:12:00,704 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:12:00,705 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:12:00,715 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-16672eb7-ee7c-4512-990a-960fe74bb77a
[INFO] 2018-10-25 04:12:00,736 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:12:00,754 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:12:00,970 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:12:00,971 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:12:00,971 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:12:00,971 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:12:00,972 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:12:00,972 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:12:00,973 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:12:00,979 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:12:01,033 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:12:01,138 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540465921137
[INFO] 2018-10-25 04:12:01,140 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-99383431-cb80-448b-8749-acf003a32aa7/userFiles-411fdaa0-98f8-4705-8e24-59ed572a5656/etl_config.json
[INFO] 2018-10-25 04:12:01,155 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540465921154
[INFO] 2018-10-25 04:12:01,155 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py to /tmp/spark-99383431-cb80-448b-8749-acf003a32aa7/userFiles-411fdaa0-98f8-4705-8e24-59ed572a5656/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-25 04:12:01,161 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540465921161
[INFO] 2018-10-25 04:12:01,162 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-99383431-cb80-448b-8749-acf003a32aa7/userFiles-411fdaa0-98f8-4705-8e24-59ed572a5656/packages.zip
[INFO] 2018-10-25 04:12:01,225 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:12:01,251 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9559.
[INFO] 2018-10-25 04:12:01,252 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:9559
[INFO] 2018-10-25 04:12:01,254 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:12:01,283 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 9559, None)
[INFO] 2018-10-25 04:12:01,289 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:9559 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 9559, None)
[INFO] 2018-10-25 04:12:01,294 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 9559, None)
[INFO] 2018-10-25 04:12:01,295 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 9559, None)
[INFO] 2018-10-25 04:12:01,559 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:12:01,560 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:12:02,102 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:12:09,935 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:12:09,948 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:12:09,962 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:12:09,975 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:12:09,976 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:12:09,988 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:12:09,993 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:12:10,006 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:12:10,007 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:12:10,009 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-342ac8a1-a347-4437-acfb-9103f2e22b8f
[INFO] 2018-10-25 04:12:10,010 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-99383431-cb80-448b-8749-acf003a32aa7
[INFO] 2018-10-25 04:12:10,011 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-99383431-cb80-448b-8749-acf003a32aa7/pyspark-61a19e09-d8c9-48e2-bc4d-177313ec015e
[WARN] 2018-10-25 04:17:26,652 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:17:27,478 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:17:27,506 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 04:17:27,766 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:17:27,767 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:17:27,767 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:17:27,767 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:17:27,781 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:17:27,972 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 24998.
[INFO] 2018-10-25 04:17:27,998 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:17:28,017 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:17:28,020 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:17:28,020 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:17:28,029 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-75dbaa2e-da27-4b45-994a-c4f249e59fd9
[INFO] 2018-10-25 04:17:28,047 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:17:28,060 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:17:28,234 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:17:28,235 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:17:28,236 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:17:28,236 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:17:28,236 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:17:28,237 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:17:28,237 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:17:28,243 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:17:28,297 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:17:28,389 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540466248389
[INFO] 2018-10-25 04:17:28,391 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-d2604099-f063-4671-abb6-b0e989fa9c8f/userFiles-259959af-11e3-40d0-87f3-23db75828dd8/etl_config.json
[INFO] 2018-10-25 04:17:28,404 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540466248404
[INFO] 2018-10-25 04:17:28,405 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-d2604099-f063-4671-abb6-b0e989fa9c8f/userFiles-259959af-11e3-40d0-87f3-23db75828dd8/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 04:17:28,411 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540466248411
[INFO] 2018-10-25 04:17:28,411 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-d2604099-f063-4671-abb6-b0e989fa9c8f/userFiles-259959af-11e3-40d0-87f3-23db75828dd8/packages.zip
[INFO] 2018-10-25 04:17:28,475 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:17:28,502 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27258.
[INFO] 2018-10-25 04:17:28,503 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:27258
[INFO] 2018-10-25 04:17:28,505 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:17:28,539 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 27258, None)
[INFO] 2018-10-25 04:17:28,546 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:27258 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 27258, None)
[INFO] 2018-10-25 04:17:28,551 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 27258, None)
[INFO] 2018-10-25 04:17:28,552 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 27258, None)
[INFO] 2018-10-25 04:17:28,849 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:17:28,850 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:17:29,489 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:17:29,909 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:17:29,918 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:17:29,931 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:17:29,940 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:17:29,941 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:17:29,948 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:17:29,954 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:17:29,962 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:17:29,963 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:17:29,964 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d2604099-f063-4671-abb6-b0e989fa9c8f/pyspark-fc267295-7275-4644-90b9-01ca3a874a60
[INFO] 2018-10-25 04:17:29,965 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d2604099-f063-4671-abb6-b0e989fa9c8f
[INFO] 2018-10-25 04:17:29,965 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-14e77998-cd5a-4767-b5e8-636458e3a4eb
[WARN] 2018-10-25 04:19:06,649 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:19:07,449 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:19:07,477 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 04:19:07,612 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:19:07,613 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:19:07,613 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:19:07,613 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:19:07,620 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:19:07,811 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 35696.
[INFO] 2018-10-25 04:19:07,836 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:19:07,855 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:19:07,858 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:19:07,859 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:19:07,868 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-2e83c6a0-7610-479b-b3be-9684d88b107f
[INFO] 2018-10-25 04:19:07,884 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:19:07,898 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:19:08,083 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:19:08,084 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:19:08,085 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:19:08,085 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:19:08,086 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:19:08,086 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:19:08,087 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:19:08,094 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:19:08,160 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:19:08,252 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540466348251
[INFO] 2018-10-25 04:19:08,254 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-d868d3a0-b892-49f7-8d68-81829e9d9754/userFiles-d75f586a-fefb-45d9-a38f-5ca95c5170d7/etl_config.json
[INFO] 2018-10-25 04:19:08,265 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540466348265
[INFO] 2018-10-25 04:19:08,266 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-d868d3a0-b892-49f7-8d68-81829e9d9754/userFiles-d75f586a-fefb-45d9-a38f-5ca95c5170d7/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 04:19:08,270 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540466348270
[INFO] 2018-10-25 04:19:08,270 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-d868d3a0-b892-49f7-8d68-81829e9d9754/userFiles-d75f586a-fefb-45d9-a38f-5ca95c5170d7/packages.zip
[INFO] 2018-10-25 04:19:08,339 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:19:08,363 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 20790.
[INFO] 2018-10-25 04:19:08,364 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:20790
[INFO] 2018-10-25 04:19:08,366 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:19:08,400 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 20790, None)
[INFO] 2018-10-25 04:19:08,406 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:20790 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 20790, None)
[INFO] 2018-10-25 04:19:08,410 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 20790, None)
[INFO] 2018-10-25 04:19:08,411 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 20790, None)
[INFO] 2018-10-25 04:19:08,672 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:19:08,673 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:19:09,212 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:19:09,625 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:19:09,639 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:19:09,652 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:19:09,661 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:19:09,661 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:19:09,668 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:19:09,674 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:19:09,684 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:19:09,684 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:19:09,685 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d868d3a0-b892-49f7-8d68-81829e9d9754
[INFO] 2018-10-25 04:19:09,686 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-16e8aea6-417b-4dc7-ae78-38d358c2d0d7
[INFO] 2018-10-25 04:19:09,687 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d868d3a0-b892-49f7-8d68-81829e9d9754/pyspark-303383a5-4d4d-4ef6-9437-59a5dcc9da62
[WARN] 2018-10-25 04:20:41,339 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:20:42,241 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:20:42,263 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 04:20:42,515 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:20:42,516 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:20:42,516 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:20:42,516 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:20:42,516 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:20:42,725 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 23662.
[INFO] 2018-10-25 04:20:42,751 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:20:42,771 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:20:42,774 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:20:42,775 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:20:42,784 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-cfe8c152-b298-4be3-ac28-64eabd9ca6ff
[INFO] 2018-10-25 04:20:42,802 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:20:42,819 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:20:43,009 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:20:43,009 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:20:43,010 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:20:43,010 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:20:43,011 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:20:43,011 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:20:43,012 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:20:43,019 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:20:43,067 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:20:43,158 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540466443158
[INFO] 2018-10-25 04:20:43,160 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-e96394b4-4980-4975-baed-352abecb1f2c/userFiles-407916b9-96cf-4caa-b40b-d79fda15b6bd/etl_config.json
[INFO] 2018-10-25 04:20:43,173 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540466443173
[INFO] 2018-10-25 04:20:43,174 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-e96394b4-4980-4975-baed-352abecb1f2c/userFiles-407916b9-96cf-4caa-b40b-d79fda15b6bd/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 04:20:43,179 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540466443178
[INFO] 2018-10-25 04:20:43,180 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-e96394b4-4980-4975-baed-352abecb1f2c/userFiles-407916b9-96cf-4caa-b40b-d79fda15b6bd/packages.zip
[INFO] 2018-10-25 04:20:43,256 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:20:43,284 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34312.
[INFO] 2018-10-25 04:20:43,285 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:34312
[INFO] 2018-10-25 04:20:43,287 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:20:43,321 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34312, None)
[INFO] 2018-10-25 04:20:43,327 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:34312 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 34312, None)
[INFO] 2018-10-25 04:20:43,331 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34312, None)
[INFO] 2018-10-25 04:20:43,332 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 34312, None)
[INFO] 2018-10-25 04:20:43,643 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:20:43,644 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:20:44,216 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:20:44,551 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:20:44,563 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:20:44,574 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:20:44,584 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:20:44,584 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:20:44,594 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:20:44,601 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:20:44,605 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:20:44,605 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:20:44,607 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e96394b4-4980-4975-baed-352abecb1f2c/pyspark-f3983ba7-1a8f-42c1-8076-e4d0f227ff81
[INFO] 2018-10-25 04:20:44,607 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e96394b4-4980-4975-baed-352abecb1f2c
[INFO] 2018-10-25 04:20:44,608 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c118e672-4fc2-437b-9b87-f930681b86f3
[WARN] 2018-10-25 04:21:25,332 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:21:26,080 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:21:26,106 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 04:21:26,291 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:21:26,291 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:21:26,292 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:21:26,292 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:21:26,292 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:21:26,482 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 12803.
[INFO] 2018-10-25 04:21:26,506 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:21:26,524 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:21:26,527 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:21:26,527 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:21:26,538 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-a6965643-7b4c-4f78-a211-d0531c26fc68
[INFO] 2018-10-25 04:21:26,555 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:21:26,568 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:21:26,738 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:21:26,738 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:21:26,739 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:21:26,739 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:21:26,740 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:21:26,740 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:21:26,741 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:21:26,747 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:21:26,803 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:21:26,907 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540466486906
[INFO] 2018-10-25 04:21:26,908 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-c7e7b8c2-8e21-4d20-9119-117fad64504f/userFiles-6ccbf8b9-7d86-4502-8ccd-1e7c13027f82/etl_config.json
[INFO] 2018-10-25 04:21:26,922 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540466486922
[INFO] 2018-10-25 04:21:26,922 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-c7e7b8c2-8e21-4d20-9119-117fad64504f/userFiles-6ccbf8b9-7d86-4502-8ccd-1e7c13027f82/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 04:21:26,927 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540466486927
[INFO] 2018-10-25 04:21:26,928 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-c7e7b8c2-8e21-4d20-9119-117fad64504f/userFiles-6ccbf8b9-7d86-4502-8ccd-1e7c13027f82/packages.zip
[INFO] 2018-10-25 04:21:26,995 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:21:27,023 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30962.
[INFO] 2018-10-25 04:21:27,025 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:30962
[INFO] 2018-10-25 04:21:27,027 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:21:27,061 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 30962, None)
[INFO] 2018-10-25 04:21:27,067 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:30962 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 30962, None)
[INFO] 2018-10-25 04:21:27,071 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 30962, None)
[INFO] 2018-10-25 04:21:27,072 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 30962, None)
[INFO] 2018-10-25 04:21:27,336 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:21:27,336 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:21:27,866 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:21:28,198 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:21:28,210 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:21:28,221 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:21:28,232 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:21:28,233 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:21:28,245 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:21:28,252 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:21:28,260 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:21:28,261 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:21:28,262 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3c53212e-2b0b-4f67-93ec-144a00f56520
[INFO] 2018-10-25 04:21:28,262 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c7e7b8c2-8e21-4d20-9119-117fad64504f
[INFO] 2018-10-25 04:21:28,263 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c7e7b8c2-8e21-4d20-9119-117fad64504f/pyspark-e90d3c65-5290-4e36-842b-7b80a23c3594
[WARN] 2018-10-25 04:25:02,382 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:25:03,185 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:25:03,208 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS
[INFO] 2018-10-25 04:25:03,404 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:25:03,404 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:25:03,405 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:25:03,405 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:25:03,405 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:25:03,658 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 9650.
[INFO] 2018-10-25 04:25:03,687 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:25:03,708 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:25:03,712 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:25:03,712 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:25:03,723 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-8ebca4b7-15be-4e41-8f2d-d5809868cc8e
[INFO] 2018-10-25 04:25:03,743 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:25:03,759 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:25:03,944 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:25:03,945 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:25:03,946 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:25:03,946 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:25:03,946 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:25:03,947 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:25:03,947 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:25:03,953 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:25:04,000 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:25:04,089 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540466704088
[INFO] 2018-10-25 04:25:04,091 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-ffc2a3ff-4f0c-447f-9fb9-467c7a8fd098/userFiles-1c001a40-2963-45f1-bc12-a55712e271ee/etl_config.json
[INFO] 2018-10-25 04:25:04,103 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py with timestamp 1540466704103
[INFO] 2018-10-25 04:25:04,104 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py to /tmp/spark-ffc2a3ff-4f0c-447f-9fb9-467c7a8fd098/userFiles-1c001a40-2963-45f1-bc12-a55712e271ee/JB_WORK_BOOKINGS.py
[INFO] 2018-10-25 04:25:04,109 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540466704109
[INFO] 2018-10-25 04:25:04,110 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-ffc2a3ff-4f0c-447f-9fb9-467c7a8fd098/userFiles-1c001a40-2963-45f1-bc12-a55712e271ee/packages.zip
[INFO] 2018-10-25 04:25:04,182 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:25:04,208 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 17752.
[INFO] 2018-10-25 04:25:04,209 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:17752
[INFO] 2018-10-25 04:25:04,211 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:25:04,245 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 17752, None)
[INFO] 2018-10-25 04:25:04,252 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:17752 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 17752, None)
[INFO] 2018-10-25 04:25:04,256 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 17752, None)
[INFO] 2018-10-25 04:25:04,257 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 17752, None)
[INFO] 2018-10-25 04:25:04,562 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:25:04,562 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:25:05,127 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:25:05,525 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:25:05,539 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:25:05,554 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:25:05,574 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:25:05,574 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:25:05,582 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:25:05,588 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:25:05,610 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:25:05,611 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:25:05,613 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ffc2a3ff-4f0c-447f-9fb9-467c7a8fd098/pyspark-daf8e2fb-f166-4dfa-b452-39af9474a54b
[INFO] 2018-10-25 04:25:05,614 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ffc2a3ff-4f0c-447f-9fb9-467c7a8fd098
[INFO] 2018-10-25 04:25:05,615 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e441aa95-a1f9-4827-9bc8-37f5f2ccb5d6
[WARN] 2018-10-25 04:28:49,473 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:28:50,283 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:28:50,308 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_TO_BOOKINGS
[INFO] 2018-10-25 04:28:50,502 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:28:50,503 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:28:50,503 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:28:50,503 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:28:50,504 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:28:50,692 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11603.
[INFO] 2018-10-25 04:28:50,716 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:28:50,735 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:28:50,737 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:28:50,738 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:28:50,747 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-4e05e613-c366-43c7-97ea-8339ac76c12f
[INFO] 2018-10-25 04:28:50,763 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:28:50,777 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:28:50,943 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:28:50,943 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:28:50,944 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:28:50,944 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:28:50,945 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:28:50,945 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:28:50,945 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:28:50,953 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:28:51,009 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:28:51,099 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540466931098
[INFO] 2018-10-25 04:28:51,101 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-da9b1a3b-13e8-4b09-92ea-e2ea77c2ae81/userFiles-140f65d1-0b3a-4021-96a6-25fd2f68ec19/etl_config.json
[INFO] 2018-10-25 04:28:51,112 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py with timestamp 1540466931112
[INFO] 2018-10-25 04:28:51,113 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py to /tmp/spark-da9b1a3b-13e8-4b09-92ea-e2ea77c2ae81/userFiles-140f65d1-0b3a-4021-96a6-25fd2f68ec19/JB_WORK_TO_BOOKINGS.py
[INFO] 2018-10-25 04:28:51,117 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540466931117
[INFO] 2018-10-25 04:28:51,118 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-da9b1a3b-13e8-4b09-92ea-e2ea77c2ae81/userFiles-140f65d1-0b3a-4021-96a6-25fd2f68ec19/packages.zip
[INFO] 2018-10-25 04:28:51,180 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:28:51,205 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36451.
[INFO] 2018-10-25 04:28:51,206 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:36451
[INFO] 2018-10-25 04:28:51,207 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:28:51,238 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 36451, None)
[INFO] 2018-10-25 04:28:51,245 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:36451 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 36451, None)
[INFO] 2018-10-25 04:28:51,249 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 36451, None)
[INFO] 2018-10-25 04:28:51,250 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 36451, None)
[INFO] 2018-10-25 04:28:51,511 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:28:51,511 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:28:52,063 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:28:52,106 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:28:52,118 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:28:52,132 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:28:52,147 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:28:52,148 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:28:52,155 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:28:52,158 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:28:52,192 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:28:52,193 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:28:52,194 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-69eb4a8a-cbe9-4b8a-81d4-b9ca3146705a
[INFO] 2018-10-25 04:28:52,195 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-da9b1a3b-13e8-4b09-92ea-e2ea77c2ae81
[INFO] 2018-10-25 04:28:52,195 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-da9b1a3b-13e8-4b09-92ea-e2ea77c2ae81/pyspark-1fb858e0-1b3c-4691-94c3-2261ac1aac17
[WARN] 2018-10-25 04:30:14,362 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:30:15,111 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:30:15,136 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_TO_BOOKINGS
[INFO] 2018-10-25 04:30:15,332 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:30:15,333 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:30:15,333 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:30:15,334 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:30:15,334 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:30:15,530 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 12092.
[INFO] 2018-10-25 04:30:15,553 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:30:15,573 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:30:15,575 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:30:15,576 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:30:15,585 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-1b0c7d7f-c97f-4061-b7ac-8f40fb440645
[INFO] 2018-10-25 04:30:15,602 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:30:15,615 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:30:15,794 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:30:15,795 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:30:15,795 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:30:15,796 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:30:15,796 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:30:15,797 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:30:15,797 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:30:15,805 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:30:15,868 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:30:15,985 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540467015985
[INFO] 2018-10-25 04:30:15,988 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-7c77e37b-653c-4de9-a8f8-b9a8fb4a6e83/userFiles-afc09872-a444-4df0-ab4b-5e20aaaa972e/etl_config.json
[INFO] 2018-10-25 04:30:16,004 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py with timestamp 1540467016004
[INFO] 2018-10-25 04:30:16,005 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py to /tmp/spark-7c77e37b-653c-4de9-a8f8-b9a8fb4a6e83/userFiles-afc09872-a444-4df0-ab4b-5e20aaaa972e/JB_WORK_TO_BOOKINGS.py
[INFO] 2018-10-25 04:30:16,011 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540467016011
[INFO] 2018-10-25 04:30:16,012 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-7c77e37b-653c-4de9-a8f8-b9a8fb4a6e83/userFiles-afc09872-a444-4df0-ab4b-5e20aaaa972e/packages.zip
[INFO] 2018-10-25 04:30:16,083 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:30:16,109 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14076.
[INFO] 2018-10-25 04:30:16,110 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:14076
[INFO] 2018-10-25 04:30:16,112 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:30:16,147 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14076, None)
[INFO] 2018-10-25 04:30:16,154 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:14076 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 14076, None)
[INFO] 2018-10-25 04:30:16,157 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14076, None)
[INFO] 2018-10-25 04:30:16,158 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 14076, None)
[INFO] 2018-10-25 04:30:16,458 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:30:16,459 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:30:17,034 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:30:17,075 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:30:17,084 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:30:17,096 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:30:17,105 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:30:17,106 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:30:17,113 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:30:17,117 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:30:17,134 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:30:17,134 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:30:17,135 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7c77e37b-653c-4de9-a8f8-b9a8fb4a6e83
[INFO] 2018-10-25 04:30:17,136 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4ecee3aa-a323-4918-b081-0ef0a2c42743
[INFO] 2018-10-25 04:30:17,136 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7c77e37b-653c-4de9-a8f8-b9a8fb4a6e83/pyspark-05b36020-3836-4ff9-9fbf-6b1127bfd502
[WARN] 2018-10-25 04:31:00,968 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:31:01,713 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:31:01,738 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_TO_BOOKINGS
[INFO] 2018-10-25 04:31:01,949 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:31:01,950 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:31:01,950 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:31:01,951 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:31:01,951 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:31:02,152 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11217.
[INFO] 2018-10-25 04:31:02,177 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:31:02,196 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:31:02,199 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:31:02,199 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:31:02,208 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-c7491144-46cd-47d6-8f34-b7ca64dc2326
[INFO] 2018-10-25 04:31:02,225 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:31:02,238 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:31:02,402 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:31:02,403 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:31:02,403 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:31:02,404 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:31:02,404 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:31:02,404 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:31:02,405 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:31:02,411 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:31:02,457 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:31:02,549 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540467062548
[INFO] 2018-10-25 04:31:02,551 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-11591269-057c-40e6-a5ef-bfb0ab83e1f7/userFiles-5de1e040-3fc5-4d1f-acc8-7fe3a7dd871c/etl_config.json
[INFO] 2018-10-25 04:31:02,563 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py with timestamp 1540467062563
[INFO] 2018-10-25 04:31:02,564 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py to /tmp/spark-11591269-057c-40e6-a5ef-bfb0ab83e1f7/userFiles-5de1e040-3fc5-4d1f-acc8-7fe3a7dd871c/JB_WORK_TO_BOOKINGS.py
[INFO] 2018-10-25 04:31:02,568 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540467062568
[INFO] 2018-10-25 04:31:02,569 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-11591269-057c-40e6-a5ef-bfb0ab83e1f7/userFiles-5de1e040-3fc5-4d1f-acc8-7fe3a7dd871c/packages.zip
[INFO] 2018-10-25 04:31:02,634 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:31:02,658 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 13637.
[INFO] 2018-10-25 04:31:02,659 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:13637
[INFO] 2018-10-25 04:31:02,662 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:31:02,695 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 13637, None)
[INFO] 2018-10-25 04:31:02,701 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:13637 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 13637, None)
[INFO] 2018-10-25 04:31:02,705 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 13637, None)
[INFO] 2018-10-25 04:31:02,706 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 13637, None)
[INFO] 2018-10-25 04:31:02,990 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:31:02,991 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:31:03,554 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:31:03,936 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:31:03,950 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:31:03,961 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:31:03,975 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:31:03,976 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:31:03,987 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:31:03,994 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:31:04,008 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:31:04,009 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:31:04,011 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-11591269-057c-40e6-a5ef-bfb0ab83e1f7
[INFO] 2018-10-25 04:31:04,012 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4c64758f-c754-4771-9c73-3b6a423678d7
[INFO] 2018-10-25 04:31:04,013 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-11591269-057c-40e6-a5ef-bfb0ab83e1f7/pyspark-2d7e6de6-ea16-4854-9b3f-a5aa6ec19cfe
[WARN] 2018-10-25 04:31:37,089 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 04:31:37,852 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 04:31:37,879 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_TO_BOOKINGS
[INFO] 2018-10-25 04:31:38,104 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 04:31:38,104 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 04:31:38,104 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 04:31:38,105 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 04:31:38,105 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 04:31:38,305 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 36922.
[INFO] 2018-10-25 04:31:38,330 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 04:31:38,349 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 04:31:38,352 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 04:31:38,352 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 04:31:38,361 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-a605b0f2-655d-4307-931d-a6d2df920b6c
[INFO] 2018-10-25 04:31:38,378 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 04:31:38,392 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 04:31:38,564 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 04:31:38,564 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 04:31:38,565 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 04:31:38,565 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 04:31:38,565 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 04:31:38,566 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 04:31:38,566 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 04:31:38,572 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 04:31:38,632 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:31:38,721 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540467098721
[INFO] 2018-10-25 04:31:38,723 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-17d671da-30cd-4c78-bf31-f31a995e6ba1/userFiles-92629888-e13c-4cd7-a897-e93f81b9f5c2/etl_config.json
[INFO] 2018-10-25 04:31:38,736 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py with timestamp 1540467098736
[INFO] 2018-10-25 04:31:38,736 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py to /tmp/spark-17d671da-30cd-4c78-bf31-f31a995e6ba1/userFiles-92629888-e13c-4cd7-a897-e93f81b9f5c2/JB_WORK_TO_BOOKINGS.py
[INFO] 2018-10-25 04:31:38,741 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540467098741
[INFO] 2018-10-25 04:31:38,741 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-17d671da-30cd-4c78-bf31-f31a995e6ba1/userFiles-92629888-e13c-4cd7-a897-e93f81b9f5c2/packages.zip
[INFO] 2018-10-25 04:31:38,814 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 04:31:38,838 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24203.
[INFO] 2018-10-25 04:31:38,839 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:24203
[INFO] 2018-10-25 04:31:38,842 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 04:31:38,884 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 24203, None)
[INFO] 2018-10-25 04:31:38,890 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:24203 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 24203, None)
[INFO] 2018-10-25 04:31:38,894 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 24203, None)
[INFO] 2018-10-25 04:31:38,895 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 24203, None)
[INFO] 2018-10-25 04:31:39,184 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 04:31:39,184 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 04:31:39,734 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 04:31:41,175 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 04:31:41,186 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 04:31:41,199 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 04:31:41,215 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 04:31:41,216 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 04:31:41,229 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 04:31:41,233 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 04:31:41,261 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 04:31:41,262 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 04:31:41,264 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-17d671da-30cd-4c78-bf31-f31a995e6ba1/pyspark-8c2866a8-3c47-4e84-aa3a-cea2f2a950ab
[INFO] 2018-10-25 04:31:41,264 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7457d612-4c0f-481b-bf19-aa08ba7ed007
[INFO] 2018-10-25 04:31:41,265 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-17d671da-30cd-4c78-bf31-f31a995e6ba1
[WARN] 2018-10-25 05:06:35,777 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 05:06:36,595 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 05:06:36,622 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_TO_BOOKINGS
[INFO] 2018-10-25 05:06:36,794 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 05:06:36,794 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 05:06:36,795 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 05:06:36,795 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 05:06:36,795 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 05:06:37,006 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 20011.
[INFO] 2018-10-25 05:06:37,031 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 05:06:37,050 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 05:06:37,053 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 05:06:37,054 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 05:06:37,063 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-f1b021b0-d6b6-4429-be04-4a8e253a9e90
[INFO] 2018-10-25 05:06:37,080 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 05:06:37,095 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 05:06:37,275 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 05:06:37,275 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 05:06:37,276 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 05:06:37,276 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 05:06:37,277 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 05:06:37,277 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 05:06:37,278 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 05:06:37,284 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 05:06:37,339 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 05:06:37,441 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540469197441
[INFO] 2018-10-25 05:06:37,443 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-e3f7c1fc-44c4-40db-8cb1-39fd6b48f963/userFiles-f4685681-b70b-4d41-8989-76cee5d68fc0/etl_config.json
[INFO] 2018-10-25 05:06:37,455 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py with timestamp 1540469197455
[INFO] 2018-10-25 05:06:37,456 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_TO_BOOKINGS.py to /tmp/spark-e3f7c1fc-44c4-40db-8cb1-39fd6b48f963/userFiles-f4685681-b70b-4d41-8989-76cee5d68fc0/JB_WORK_TO_BOOKINGS.py
[INFO] 2018-10-25 05:06:37,461 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540469197461
[INFO] 2018-10-25 05:06:37,461 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-e3f7c1fc-44c4-40db-8cb1-39fd6b48f963/userFiles-f4685681-b70b-4d41-8989-76cee5d68fc0/packages.zip
[INFO] 2018-10-25 05:06:37,528 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 05:06:37,559 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14817.
[INFO] 2018-10-25 05:06:37,560 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:14817
[INFO] 2018-10-25 05:06:37,561 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 05:06:37,593 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 14817, None)
[INFO] 2018-10-25 05:06:37,599 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:14817 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 14817, None)
[INFO] 2018-10-25 05:06:37,603 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 14817, None)
[INFO] 2018-10-25 05:06:37,603 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 14817, None)
[INFO] 2018-10-25 05:06:37,878 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 05:06:37,879 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 05:06:38,443 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 05:06:40,761 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 05:06:40,774 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 05:06:40,787 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 05:06:40,797 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 05:06:40,797 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 05:06:40,805 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 05:06:40,811 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 05:06:40,823 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 05:06:40,824 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 05:06:40,826 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e3f7c1fc-44c4-40db-8cb1-39fd6b48f963/pyspark-7a2271ae-07b5-45cd-8513-2de675a3ef75
[INFO] 2018-10-25 05:06:40,827 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d7a01f2a-83d9-4561-95eb-7167180ecffa
[INFO] 2018-10-25 05:06:40,827 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e3f7c1fc-44c4-40db-8cb1-39fd6b48f963
[WARN] 2018-10-25 05:23:48,728 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 05:23:49,484 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 05:23:49,510 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS
[INFO] 2018-10-25 05:23:49,726 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 05:23:49,726 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 05:23:49,727 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 05:23:49,727 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 05:23:49,727 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 05:23:49,934 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 28915.
[INFO] 2018-10-25 05:23:49,959 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 05:23:49,979 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 05:23:49,981 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 05:23:49,982 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 05:23:49,991 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-dcea46bb-1282-47e1-9048-0888974f52ec
[INFO] 2018-10-25 05:23:50,008 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 05:23:50,022 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 05:23:50,193 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 05:23:50,194 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN] 2018-10-25 05:23:50,194 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN] 2018-10-25 05:23:50,194 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[WARN] 2018-10-25 05:23:50,195 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WARN] 2018-10-25 05:23:50,195 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WARN] 2018-10-25 05:23:50,196 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[INFO] 2018-10-25 05:23:50,202 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4047.
[INFO] 2018-10-25 05:23:50,261 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 05:23:50,368 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540470230367
[INFO] 2018-10-25 05:23:50,370 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-f74f91ff-34c5-4c92-b4c3-d2bd71b929f4/userFiles-575cf5df-bf87-4fef-b3b2-6d4bc78f0c0d/etl_config.json
[INFO] 2018-10-25 05:23:50,384 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py with timestamp 1540470230384
[INFO] 2018-10-25 05:23:50,384 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py to /tmp/spark-f74f91ff-34c5-4c92-b4c3-d2bd71b929f4/userFiles-575cf5df-bf87-4fef-b3b2-6d4bc78f0c0d/JB_WORK_BOOKINGS.py
[INFO] 2018-10-25 05:23:50,390 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540470230390
[INFO] 2018-10-25 05:23:50,390 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-f74f91ff-34c5-4c92-b4c3-d2bd71b929f4/userFiles-575cf5df-bf87-4fef-b3b2-6d4bc78f0c0d/packages.zip
[INFO] 2018-10-25 05:23:50,462 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 05:23:50,487 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 16585.
[INFO] 2018-10-25 05:23:50,488 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:16585
[INFO] 2018-10-25 05:23:50,490 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 05:23:50,523 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 16585, None)
[INFO] 2018-10-25 05:23:50,529 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:16585 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 16585, None)
[INFO] 2018-10-25 05:23:50,534 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 16585, None)
[INFO] 2018-10-25 05:23:50,534 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 16585, None)
[INFO] 2018-10-25 05:23:50,825 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 05:23:50,825 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 05:23:51,422 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 05:23:51,897 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 05:23:51,910 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4047
[INFO] 2018-10-25 05:23:51,927 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 05:23:51,942 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 05:23:51,943 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 05:23:51,955 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 05:23:51,962 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 05:23:51,990 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 05:23:51,991 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 05:23:51,992 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f74f91ff-34c5-4c92-b4c3-d2bd71b929f4
[INFO] 2018-10-25 05:23:51,993 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1f5627ec-0b8e-403e-9053-3a54f1920c93
[INFO] 2018-10-25 05:23:51,994 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f74f91ff-34c5-4c92-b4c3-d2bd71b929f4/pyspark-b619544a-4023-4067-b80f-95b8f31181df
[WARN] 2018-10-25 05:24:19,303 org.apache.spark.HeartbeatReceiver logWarning - Removing executor driver with no recent heartbeats: 4735129 ms exceeds timeout 120000 ms
[ERROR] 2018-10-25 05:24:19,307 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost an executor driver (already removed): Executor heartbeat timed out after 4735129 ms
[INFO] 2018-10-25 05:24:19,307 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-25 05:24:19,312 org.apache.spark.SparkContext logWarning - Killing executors is not supported by current scheduler.
[INFO] 2018-10-25 05:24:19,314 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-25 05:24:19,315 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None) re-registering with master
[INFO] 2018-10-25 05:24:19,316 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 05:24:19,317 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 05:24:19,318 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-25 05:24:19,320 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-25 05:24:19,321 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None) re-registering with master
[INFO] 2018-10-25 05:24:19,321 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4046
[INFO] 2018-10-25 05:24:19,321 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 05:24:19,322 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 05:24:19,323 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-25 05:24:19,324 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-25 05:24:19,324 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None) re-registering with master
[INFO] 2018-10-25 05:24:19,324 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 05:24:19,325 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 05:24:19,325 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-25 05:24:19,326 org.apache.spark.executor.Executor logInfo - Told to re-register on heartbeat
[INFO] 2018-10-25 05:24:19,326 org.apache.spark.storage.BlockManager logInfo - BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None) re-registering with master
[INFO] 2018-10-25 05:24:19,327 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 05:24:19,328 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34012, None)
[INFO] 2018-10-25 05:24:19,329 org.apache.spark.storage.BlockManager logInfo - Reporting 0 blocks to the master.
[INFO] 2018-10-25 05:24:19,334 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 05:24:19,344 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 05:24:19,345 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 05:24:19,346 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 05:24:19,352 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 05:24:19,359 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 05:24:19,360 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 05:24:19,361 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ff1ff514-8139-4e66-b2a5-4088e32b548f
[INFO] 2018-10-25 05:24:19,362 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-57ce0872-2758-4f56-b35e-c08b9fb6b488
[INFO] 2018-10-25 05:24:19,363 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-57ce0872-2758-4f56-b35e-c08b9fb6b488/pyspark-92ad035e-7f2a-4704-af73-15001396fe97
[WARN] 2018-10-25 05:33:26,154 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,155 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,155 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,155 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,155 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,155 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,156 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,156 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,157 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,157 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,157 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,157 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,158 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,158 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,158 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,158 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,158 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,158 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,158 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,158 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,159 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,159 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,159 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,159 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,159 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,159 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,159 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,160 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,160 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,161 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,162 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,162 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,162 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,162 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,162 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,162 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,163 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,163 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,163 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,163 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,163 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,163 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,164 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,580 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,581 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,581 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,581 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,581 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,582 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,582 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,582 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,582 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,582 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,582 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,584 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,584 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,584 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,584 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,584 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,584 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,587 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,587 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,587 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,587 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,587 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,587 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,588 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,588 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,588 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,588 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,588 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,588 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,588 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,592 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,592 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,592 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,592 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,592 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,592 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,592 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,657 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,657 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,657 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,657 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,657 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,658 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,658 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,658 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,658 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,658 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,658 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,658 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,164 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,019 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,019 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,019 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,019 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,019 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,204 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,204 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,204 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,204 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,204 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,254 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,255 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,255 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,255 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,255 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,256 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,161 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,256 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,256 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,256 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,256 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,256 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,258 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,258 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,257 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,258 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,258 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,258 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,258 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,258 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,258 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,259 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,259 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,260 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,260 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,260 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,260 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logWarning - Disconnected from Spark cluster! Waiting for reconnection...
[INFO] 2018-10-25 05:33:27,260 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-25 05:33:27,263 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,263 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,263 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logWarning - Disconnected from Spark cluster! Waiting for reconnection...
[WARN] 2018-10-25 05:33:27,264 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,260 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,264 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,264 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,264 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,264 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,264 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,265 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,265 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,265 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,265 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,265 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,265 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,265 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,265 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,266 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,266 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,266 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,266 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,266 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,266 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,266 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,266 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,267 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,267 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,267 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,267 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,267 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,267 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,267 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,267 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,268 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,268 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,268 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,268 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,268 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,268 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,268 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,268 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,269 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,269 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,269 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,269 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,269 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,269 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,269 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,269 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,270 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,271 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,271 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,272 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,273 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,274 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,274 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,274 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,274 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,274 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,274 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,275 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,275 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,275 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,275 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,274 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,276 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,276 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,278 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,278 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,278 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,279 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,279 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,279 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,279 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,279 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,279 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,280 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,280 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,280 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,280 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,280 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,281 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,281 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,281 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,281 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,281 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,289 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,289 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,289 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,289 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4043
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,292 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,292 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4044
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,293 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,294 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,295 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,295 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,295 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,295 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,295 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,295 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,296 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,296 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,296 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,296 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,296 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,296 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,296 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,297 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,298 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,298 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,298 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,298 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,298 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,298 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,298 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,299 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,299 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,299 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,299 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,300 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,300 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,300 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,300 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,301 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,301 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,301 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,301 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,301 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,302 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,302 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,302 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,302 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,302 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,302 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,302 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,303 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,303 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,303 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,303 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,303 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,303 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,303 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,304 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,304 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,304 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,304 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,304 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,304 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,304 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,304 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,298 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,306 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,306 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,306 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,306 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,306 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,306 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,306 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,307 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,307 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,307 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,305 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,316 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,316 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,317 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,318 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,319 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,320 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,321 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,322 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,322 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,322 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,322 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,322 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,322 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,322 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,322 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,323 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,323 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,330 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,330 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,330 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,330 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,330 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,330 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,330 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,330 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,331 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,332 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,332 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,332 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,332 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,332 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,332 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,335 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,336 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,337 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,337 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,337 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,337 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,337 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,337 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,340 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,338 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,340 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,340 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,340 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,340 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,342 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,343 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,344 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,341 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,344 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,344 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,345 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,345 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,345 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,345 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,345 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,345 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,345 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,346 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,344 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,309 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 9688.534 s due to Stage cancelled because SparkContext was shut down
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,349 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,308 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 9688.594401 s
[WARN] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,344 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 10683.492 s due to Stage cancelled because SparkContext was shut down
[WARN] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[WARN] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,336 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,348 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,347 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,351 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,351 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,351 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,350 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,351 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,351 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logWarning - Disconnected from Spark cluster! Waiting for reconnection...
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,352 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,353 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,355 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,355 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,355 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,355 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,355 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,356 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,356 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,356 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,356 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,356 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,354 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 10683.557067 s
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,357 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,358 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,359 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,362 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,361 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,364 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,366 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,365 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,368 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,368 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,368 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,368 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,368 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,368 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,368 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,369 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,369 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,369 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,369 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,369 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,369 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,369 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,369 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,370 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,370 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,370 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,370 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,370 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,370 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,370 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,370 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,365 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,371 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,359 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,360 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,373 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,372 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,363 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,374 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,375 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,367 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,376 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logWarning - Disconnected from Spark cluster! Waiting for reconnection...
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,378 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,379 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,379 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,379 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,379 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:26,150 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,379 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,380 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,377 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,380 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,380 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,380 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,380 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,380 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,380 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,381 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,379 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,381 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,381 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,381 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,381 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,382 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,382 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,382 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,382 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,382 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,382 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,382 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,381 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,379 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 11284.243118 s
[WARN] 2018-10-25 05:33:27,383 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,383 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-25 05:33:27,382 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,383 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,383 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,383 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,383 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,383 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,384 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,384 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,384 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,384 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,384 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,384 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,384 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,385 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,386 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,387 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,383 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,389 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4045
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 11284.193 s due to Stage cancelled because SparkContext was shut down
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,388 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,390 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,389 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,391 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,392 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,393 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,394 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,395 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,397 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,397 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,397 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,397 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,397 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,397 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,397 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,397 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,396 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,399 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,400 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logWarning - Disconnected from Spark cluster! Waiting for reconnection...
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,402 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,403 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,404 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,404 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,404 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,404 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,404 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,404 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,401 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,405 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 9649.563 s due to Stage cancelled because SparkContext was shut down
[WARN] 2018-10-25 05:33:27,407 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,404 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 9649.623203 s
[WARN] 2018-10-25 05:33:27,398 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,408 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,409 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,410 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,412 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,412 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,412 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,411 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,412 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,412 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,412 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,407 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,412 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,414 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,415 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,416 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,417 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,418 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,404 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,419 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,420 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,421 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,413 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,422 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,415 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,423 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,424 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,425 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,426 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,427 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,428 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,429 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,431 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,430 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,432 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,433 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,434 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,436 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,435 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 11719.023450 s
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,438 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,439 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,437 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,440 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,440 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 11718.963 s due to Stage cancelled because SparkContext was shut down
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,441 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,442 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,443 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,445 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,444 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,446 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,447 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,448 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,449 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,451 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,452 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,453 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,450 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,454 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,455 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,456 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,457 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,458 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,459 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,460 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,461 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,462 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,463 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,464 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,465 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,466 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,467 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,468 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,469 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,469 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,469 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,469 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,469 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,469 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,469 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,469 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,470 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,470 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,470 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,470 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,470 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,470 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,470 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,470 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,471 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,471 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,471 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,471 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,471 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,471 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,471 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,471 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,472 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,472 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,472 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,472 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,472 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,472 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,472 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,473 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[WARN] 2018-10-25 05:33:27,474 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,474 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,474 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[WARN] 2018-10-25 05:33:27,474 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,476 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,476 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,477 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,477 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,477 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,477 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,477 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,477 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,477 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,477 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,478 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,478 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,478 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,478 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,479 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,480 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,481 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,481 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,482 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,483 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,483 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,484 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,484 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,485 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,487 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,487 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,487 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,487 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,487 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,487 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,486 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,488 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,489 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,490 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,490 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,491 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,491 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,490 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,491 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,491 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[WARN] 2018-10-25 05:33:27,491 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,492 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,493 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,493 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,494 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,494 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,495 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,495 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,495 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,495 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,495 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,495 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,495 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,495 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,496 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,497 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,499 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,499 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,499 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,499 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,499 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,499 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,499 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[WARN] 2018-10-25 05:33:27,499 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,499 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,500 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,500 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,501 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,501 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,502 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,502 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,502 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,502 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,502 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,502 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,502 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[WARN] 2018-10-25 05:33:27,502 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,502 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,503 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,504 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,505 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,505 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,505 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,505 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,505 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,505 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,505 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,506 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,507 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,508 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,508 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,509 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,509 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,509 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,509 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[WARN] 2018-10-25 05:33:27,509 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,509 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 05:33:27,511 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-502bbc63-9ac3-479a-a124-4f9cee8c481f
[INFO] 2018-10-25 05:33:27,512 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[WARN] 2018-10-25 05:33:27,512 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,512 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-5fbbb6ab-ba88-4ec6-b408-e38167b5dcca
[INFO] 2018-10-25 05:33:27,512 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-502bbc63-9ac3-479a-a124-4f9cee8c481f/pyspark-6fed2313-ec6d-4ee3-96b4-3751ea780a15
[WARN] 2018-10-25 05:33:27,512 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,514 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,514 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,515 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,516 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,516 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,516 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,516 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,516 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,516 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[WARN] 2018-10-25 05:33:27,516 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,516 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,516 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,517 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,517 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,518 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,519 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,519 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,519 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,520 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-634ceeee-901b-4356-93f3-353404eede77/pyspark-18b4a612-5939-43e9-993f-3e7fac5dd9c7
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,520 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,521 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-9ac096c8-3218-4df1-8917-e119fc115527
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,521 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-634ceeee-901b-4356-93f3-353404eede77
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,521 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,522 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,523 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,523 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,523 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,523 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,523 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,523 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,524 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,524 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,524 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,524 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,525 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6794a24f-77c5-4887-a24a-19dbf245b0b7
[WARN] 2018-10-25 05:33:27,525 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,526 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6794a24f-77c5-4887-a24a-19dbf245b0b7/pyspark-c406ffde-5382-40f3-8438-a62ac466cb8e
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,526 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d93b7d16-856c-46c5-a334-b20538eb523b
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[WARN] 2018-10-25 05:33:27,526 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,527 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,527 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[WARN] 2018-10-25 05:33:27,527 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,527 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,527 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,527 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,527 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,528 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,528 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,528 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,528 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,528 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,528 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,529 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,530 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,530 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,530 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,530 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,530 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,530 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,531 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,531 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,531 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,531 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,531 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,531 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,531 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,532 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,532 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,538 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,538 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 05:33:27,538 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 05:33:27,539 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[WARN] 2018-10-25 05:33:27,539 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,539 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,540 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,540 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,540 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,540 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4e1f1f35-3622-451a-aded-9df185555b17
[WARN] 2018-10-25 05:33:27,540 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,541 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,541 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b1a11759-1ec6-47f8-ab7d-e4da62835f26
[WARN] 2018-10-25 05:33:27,541 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,541 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,541 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,541 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,541 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-b1a11759-1ec6-47f8-ab7d-e4da62835f26/pyspark-8a586e16-69d6-4548-a8cf-295556ddfbcc
[WARN] 2018-10-25 05:33:27,541 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,541 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,541 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,542 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,542 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,543 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,543 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,544 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,544 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,544 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,544 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,544 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,546 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logWarning - Disconnected from Spark cluster! Waiting for reconnection...
[INFO] 2018-10-25 05:33:27,546 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[WARN] 2018-10-25 05:33:27,548 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logWarning - Connection to vmwbidapp03-dev.corp.netapp.com:7077 failed; waiting for master to reconnect...
[WARN] 2018-10-25 05:33:27,548 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,553 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[WARN] 2018-10-25 05:33:27,552 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,554 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[WARN] 2018-10-25 05:33:27,554 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,556 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,556 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,556 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,556 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,556 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,556 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,557 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,557 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,557 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,557 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,557 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,557 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,557 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,558 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,558 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,558 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,558 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,558 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,558 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,558 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,558 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,559 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,559 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,559 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,559 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,559 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,559 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,559 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,559 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,560 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,560 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,560 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,560 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,560 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,560 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,560 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,561 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,561 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,561 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,562 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,562 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,562 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,562 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,562 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,562 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,563 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,563 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,563 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[WARN] 2018-10-25 05:33:27,563 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,563 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,563 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,563 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,563 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,563 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,564 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,564 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,564 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,564 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,564 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,564 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,564 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,565 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,565 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,565 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,565 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,565 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,565 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,565 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,565 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,566 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[WARN] 2018-10-25 05:33:27,566 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,566 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,566 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,566 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,566 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,566 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,566 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,567 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,567 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,567 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,567 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,567 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,567 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,567 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,568 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,568 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[WARN] 2018-10-25 05:33:27,568 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,568 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,569 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,569 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,569 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,569 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,569 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,569 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,569 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,570 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,570 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,570 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,570 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,570 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,570 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,571 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,571 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,571 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,571 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,571 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,571 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,572 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,574 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,574 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 05:33:27,574 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[WARN] 2018-10-25 05:33:27,574 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,575 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,575 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,575 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7ceda005-5daa-4b1b-9264-164c510d6da2
[INFO] 2018-10-25 05:33:27,576 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-7ceda005-5daa-4b1b-9264-164c510d6da2/pyspark-b10fd55f-a31a-4b6d-94a4-bedf933e18a9
[INFO] 2018-10-25 05:33:27,576 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ed3e441c-d767-4320-bb2d-389cfc5e0630
[WARN] 2018-10-25 05:33:27,575 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,578 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 11029.465 s due to Stage cancelled because SparkContext was shut down
[INFO] 2018-10-25 05:33:27,576 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 11029.518055 s
[WARN] 2018-10-25 05:33:27,578 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,579 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,582 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[WARN] 2018-10-25 05:33:27,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,583 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,584 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,585 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,586 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,588 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,589 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,590 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,591 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,593 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,594 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,595 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,596 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,597 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,598 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,598 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,598 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,599 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,599 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,599 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,599 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,599 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,599 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,600 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,600 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,600 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,600 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,600 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,601 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,601 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,601 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,601 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,601 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,601 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,602 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,602 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,602 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,602 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,602 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,602 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,602 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,602 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,603 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,604 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,605 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,606 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,608 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,609 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,609 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,609 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,609 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,609 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,609 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,609 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,607 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[WARN] 2018-10-25 05:33:27,609 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,610 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,611 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,612 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,613 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,614 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,614 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,614 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,614 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,615 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,616 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,616 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,616 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,616 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,616 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[WARN] 2018-10-25 05:33:27,616 org.apache.spark.scheduler.TaskSchedulerImpl logWarning - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[INFO] 2018-10-25 05:33:27,623 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 05:33:27,633 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 05:33:27,634 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 05:33:27,644 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 05:33:27,648 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 05:33:27,657 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 05:33:27,658 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 05:33:27,659 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-de15f453-e9ea-4943-8e01-e3cae5a5061f/pyspark-860e0074-4209-4231-bf0e-239ef65175c1
[INFO] 2018-10-25 05:33:27,659 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-80b8d5a0-ebf4-4968-93d0-332c97c0131a
[INFO] 2018-10-25 05:33:27,660 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-de15f453-e9ea-4943-8e01-e3cae5a5061f
[WARN] 2018-10-25 09:38:38,801 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WARN] 2018-10-25 09:46:17,739 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 09:46:18,118 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 09:46:18,120 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6575e43d-fd5f-4458-a4f3-2978919fc08a
[WARN] 2018-10-25 09:47:45,247 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 09:47:45,697 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 09:47:45,699 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-62249540-fa22-417c-af91-1690eaa719ea
[WARN] 2018-10-25 09:49:09,360 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 09:49:09,770 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 09:49:09,771 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-08e8138a-a221-4bf7-aaa7-f3d950baa11b
[WARN] 2018-10-25 20:11:46,156 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 20:11:47,056 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 20:11:47,058 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-aeda9b8d-9126-4200-a951-08df53232116
[WARN] 2018-10-25 20:11:59,551 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 20:12:00,409 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 20:12:00,434 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP_TRUNCATE
[INFO] 2018-10-25 20:12:00,621 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 20:12:00,621 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 20:12:00,621 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 20:12:00,622 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 20:12:00,631 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 20:12:00,846 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 21767.
[INFO] 2018-10-25 20:12:00,876 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 20:12:00,900 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 20:12:00,903 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 20:12:00,904 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 20:12:00,915 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-3e417f60-0a85-4971-960a-fce07162af4b
[INFO] 2018-10-25 20:12:00,935 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 20:12:00,952 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 20:12:01,158 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 20:12:01,219 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:12:01,314 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540523521313
[INFO] 2018-10-25 20:12:01,316 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-8216a557-52e9-4ed3-9a70-4d0aefa308b2/userFiles-acbac4df-a334-45ab-b591-135aa996c6a0/etl_config.json
[INFO] 2018-10-25 20:12:01,328 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py with timestamp 1540523521328
[INFO] 2018-10-25 20:12:01,329 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py to /tmp/spark-8216a557-52e9-4ed3-9a70-4d0aefa308b2/userFiles-acbac4df-a334-45ab-b591-135aa996c6a0/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py
[INFO] 2018-10-25 20:12:01,333 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540523521333
[INFO] 2018-10-25 20:12:01,334 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-8216a557-52e9-4ed3-9a70-4d0aefa308b2/userFiles-acbac4df-a334-45ab-b591-135aa996c6a0/packages.zip
[INFO] 2018-10-25 20:12:01,399 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 20:12:01,431 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24348.
[INFO] 2018-10-25 20:12:01,432 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:24348
[INFO] 2018-10-25 20:12:01,434 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 20:12:01,469 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 24348, None)
[INFO] 2018-10-25 20:12:01,476 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:24348 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 24348, None)
[INFO] 2018-10-25 20:12:01,481 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 24348, None)
[INFO] 2018-10-25 20:12:01,483 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 24348, None)
[INFO] 2018-10-25 20:12:01,792 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 20:12:01,793 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 20:12:02,371 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 20:12:02,772 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 20:12:02,783 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:12:02,799 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 20:12:02,809 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 20:12:02,810 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 20:12:02,821 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 20:12:02,825 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 20:12:02,837 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 20:12:02,838 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 20:12:02,839 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8216a557-52e9-4ed3-9a70-4d0aefa308b2
[INFO] 2018-10-25 20:12:02,840 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8216a557-52e9-4ed3-9a70-4d0aefa308b2/pyspark-b8b000cc-f771-450e-a31e-40a875cf0cd0
[INFO] 2018-10-25 20:12:02,840 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-9ff45ab9-d655-4b45-ac60-a775a7b13894
[WARN] 2018-10-25 20:22:24,850 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 20:22:25,624 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 20:22:25,651 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-25 20:22:25,878 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 20:22:25,878 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 20:22:25,878 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 20:22:25,879 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 20:22:25,879 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 20:22:26,086 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 26523.
[INFO] 2018-10-25 20:22:26,115 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 20:22:26,141 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 20:22:26,145 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 20:22:26,146 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 20:22:26,157 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-c9ccb692-08db-45a2-bba7-b15888f38d90
[INFO] 2018-10-25 20:22:26,178 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 20:22:26,194 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 20:22:26,384 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 20:22:26,468 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:22:26,567 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540524146566
[INFO] 2018-10-25 20:22:26,569 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-c64aa4a0-d441-4199-87c3-a26b1c7c8ec4/userFiles-26be0871-566f-45e1-a9ed-b878967bcc1f/etl_config.json
[INFO] 2018-10-25 20:22:26,583 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540524146583
[INFO] 2018-10-25 20:22:26,584 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-c64aa4a0-d441-4199-87c3-a26b1c7c8ec4/userFiles-26be0871-566f-45e1-a9ed-b878967bcc1f/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-25 20:22:26,589 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540524146589
[INFO] 2018-10-25 20:22:26,590 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-c64aa4a0-d441-4199-87c3-a26b1c7c8ec4/userFiles-26be0871-566f-45e1-a9ed-b878967bcc1f/packages.zip
[INFO] 2018-10-25 20:22:26,663 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 20:22:26,689 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30218.
[INFO] 2018-10-25 20:22:26,691 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:30218
[INFO] 2018-10-25 20:22:26,693 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 20:22:26,728 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 30218, None)
[INFO] 2018-10-25 20:22:26,736 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:30218 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 30218, None)
[INFO] 2018-10-25 20:22:26,741 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 30218, None)
[INFO] 2018-10-25 20:22:26,742 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 30218, None)
[INFO] 2018-10-25 20:22:27,032 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 20:22:27,032 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 20:22:27,605 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 20:22:28,053 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 20:22:28,067 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:22:28,080 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 20:22:28,094 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 20:22:28,095 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 20:22:28,105 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 20:22:28,113 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 20:22:28,123 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 20:22:28,124 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 20:22:28,125 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-eb21269a-5bd2-4a62-a1bb-a57c435c6bc9
[INFO] 2018-10-25 20:22:28,126 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c64aa4a0-d441-4199-87c3-a26b1c7c8ec4
[INFO] 2018-10-25 20:22:28,126 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c64aa4a0-d441-4199-87c3-a26b1c7c8ec4/pyspark-7f084015-8a77-48c6-8e4e-3bcb469920d9
[WARN] 2018-10-25 20:22:50,420 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 20:22:51,260 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 20:22:51,286 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_TRUNCATE
[INFO] 2018-10-25 20:22:51,432 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 20:22:51,432 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 20:22:51,433 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 20:22:51,433 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 20:22:51,434 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 20:22:51,644 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 26940.
[INFO] 2018-10-25 20:22:51,667 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 20:22:51,686 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 20:22:51,689 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 20:22:51,689 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 20:22:51,698 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-fb14548b-17c8-4758-95f6-4cd7f26e747d
[INFO] 2018-10-25 20:22:51,715 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 20:22:51,728 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 20:22:51,916 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 20:22:51,983 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:22:52,091 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540524172090
[INFO] 2018-10-25 20:22:52,093 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-29f61975-79f0-46f8-b53a-2d310bc601d8/userFiles-af1e8fe9-be83-4fde-a68a-d4273917d783/etl_config.json
[INFO] 2018-10-25 20:22:52,108 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py with timestamp 1540524172108
[INFO] 2018-10-25 20:22:52,109 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_TRUNCATE.py to /tmp/spark-29f61975-79f0-46f8-b53a-2d310bc601d8/userFiles-af1e8fe9-be83-4fde-a68a-d4273917d783/JB_WORK_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-25 20:22:52,114 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540524172114
[INFO] 2018-10-25 20:22:52,115 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-29f61975-79f0-46f8-b53a-2d310bc601d8/userFiles-af1e8fe9-be83-4fde-a68a-d4273917d783/packages.zip
[INFO] 2018-10-25 20:22:52,196 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 20:22:52,221 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25725.
[INFO] 2018-10-25 20:22:52,222 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:25725
[INFO] 2018-10-25 20:22:52,224 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 20:22:52,256 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25725, None)
[INFO] 2018-10-25 20:22:52,262 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:25725 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 25725, None)
[INFO] 2018-10-25 20:22:52,266 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25725, None)
[INFO] 2018-10-25 20:22:52,267 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 25725, None)
[INFO] 2018-10-25 20:22:52,559 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 20:22:52,560 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 20:22:53,095 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 20:22:53,514 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 20:22:53,526 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:22:53,538 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 20:22:53,547 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 20:22:53,548 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 20:22:53,555 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 20:22:53,560 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 20:22:53,567 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 20:22:53,567 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 20:22:53,568 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-29f61975-79f0-46f8-b53a-2d310bc601d8/pyspark-632e8f20-fce7-4c74-a032-2008b76c23b5
[INFO] 2018-10-25 20:22:53,569 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-047df020-ff8b-403a-957e-b3e7eb080083
[INFO] 2018-10-25 20:22:53,569 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-29f61975-79f0-46f8-b53a-2d310bc601d8
[WARN] 2018-10-25 20:23:20,081 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 20:23:20,860 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 20:23:20,884 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 20:23:21,069 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 20:23:21,069 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 20:23:21,070 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 20:23:21,070 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 20:23:21,070 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 20:23:21,270 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 35082.
[INFO] 2018-10-25 20:23:21,295 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 20:23:21,314 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 20:23:21,317 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 20:23:21,317 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 20:23:21,326 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-7b32bf43-f593-4556-94d1-0dc60f2b5f35
[INFO] 2018-10-25 20:23:21,344 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 20:23:21,357 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 20:23:21,532 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 20:23:21,591 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:23:21,683 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540524201683
[INFO] 2018-10-25 20:23:21,685 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-3a84f565-2086-427d-a7cb-0b3c1bd697b2/userFiles-37c3ba29-ffc3-4b1b-a349-eaac14a41e51/etl_config.json
[INFO] 2018-10-25 20:23:21,699 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540524201699
[INFO] 2018-10-25 20:23:21,700 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-3a84f565-2086-427d-a7cb-0b3c1bd697b2/userFiles-37c3ba29-ffc3-4b1b-a349-eaac14a41e51/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 20:23:21,704 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540524201704
[INFO] 2018-10-25 20:23:21,705 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-3a84f565-2086-427d-a7cb-0b3c1bd697b2/userFiles-37c3ba29-ffc3-4b1b-a349-eaac14a41e51/packages.zip
[INFO] 2018-10-25 20:23:21,774 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 20:23:21,802 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 22707.
[INFO] 2018-10-25 20:23:21,803 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:22707
[INFO] 2018-10-25 20:23:21,805 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 20:23:21,838 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 22707, None)
[INFO] 2018-10-25 20:23:21,843 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:22707 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 22707, None)
[INFO] 2018-10-25 20:23:21,848 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 22707, None)
[INFO] 2018-10-25 20:23:21,848 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 22707, None)
[INFO] 2018-10-25 20:23:22,107 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 20:23:22,108 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 20:23:22,617 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 20:23:23,027 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 20:23:23,038 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:23:23,050 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 20:23:23,059 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 20:23:23,059 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 20:23:23,071 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 20:23:23,075 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 20:23:23,085 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 20:23:23,086 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 20:23:23,088 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-448c7719-7dbd-4e4a-accc-889951515534
[INFO] 2018-10-25 20:23:23,089 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3a84f565-2086-427d-a7cb-0b3c1bd697b2
[INFO] 2018-10-25 20:23:23,090 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3a84f565-2086-427d-a7cb-0b3c1bd697b2/pyspark-7a85a7d1-d282-4b6d-8c6a-4eb72a1a0f3b
[WARN] 2018-10-25 20:24:16,286 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 20:24:17,049 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 20:24:17,079 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS
[INFO] 2018-10-25 20:24:17,286 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 20:24:17,286 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 20:24:17,287 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 20:24:17,287 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 20:24:17,302 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 20:24:17,495 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 23830.
[INFO] 2018-10-25 20:24:17,524 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 20:24:17,552 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 20:24:17,555 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 20:24:17,556 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 20:24:17,566 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-285c261b-0f9a-4e4c-96af-1ca9f9daa7a5
[INFO] 2018-10-25 20:24:17,587 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 20:24:17,605 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 20:24:17,811 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 20:24:17,884 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:24:17,980 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540524257979
[INFO] 2018-10-25 20:24:17,987 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-c8938602-1b7a-4d87-81a8-fd0e55e4e7b6/userFiles-4a89c66f-da03-48f4-b543-4582463db5cc/etl_config.json
[INFO] 2018-10-25 20:24:18,005 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py with timestamp 1540524258005
[INFO] 2018-10-25 20:24:18,006 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS.py to /tmp/spark-c8938602-1b7a-4d87-81a8-fd0e55e4e7b6/userFiles-4a89c66f-da03-48f4-b543-4582463db5cc/JB_WORK_BOOKINGS.py
[INFO] 2018-10-25 20:24:18,010 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540524258010
[INFO] 2018-10-25 20:24:18,010 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-c8938602-1b7a-4d87-81a8-fd0e55e4e7b6/userFiles-4a89c66f-da03-48f4-b543-4582463db5cc/packages.zip
[INFO] 2018-10-25 20:24:18,076 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 20:24:18,099 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15895.
[INFO] 2018-10-25 20:24:18,100 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:15895
[INFO] 2018-10-25 20:24:18,102 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 20:24:18,131 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 15895, None)
[INFO] 2018-10-25 20:24:18,139 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:15895 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 15895, None)
[INFO] 2018-10-25 20:24:18,144 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 15895, None)
[INFO] 2018-10-25 20:24:18,145 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 15895, None)
[INFO] 2018-10-25 20:24:18,431 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 20:24:18,431 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 20:24:18,973 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 20:24:19,469 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 20:24:19,480 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:24:19,493 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 20:24:19,503 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 20:24:19,504 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 20:24:19,516 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 20:24:19,522 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 20:24:19,531 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 20:24:19,532 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 20:24:19,533 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-353cfead-9603-4fc0-b82e-faa2978a8e0e
[INFO] 2018-10-25 20:24:19,533 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c8938602-1b7a-4d87-81a8-fd0e55e4e7b6/pyspark-57171cec-865c-4067-a7bb-f23f34925776
[INFO] 2018-10-25 20:24:19,534 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-c8938602-1b7a-4d87-81a8-fd0e55e4e7b6
[WARN] 2018-10-25 20:54:06,044 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 20:54:06,924 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 20:54:06,950 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER
[INFO] 2018-10-25 20:54:07,229 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 20:54:07,229 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 20:54:07,229 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 20:54:07,230 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 20:54:07,230 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 20:54:07,433 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 30735.
[INFO] 2018-10-25 20:54:07,458 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 20:54:07,480 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 20:54:07,484 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 20:54:07,485 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 20:54:07,495 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-2e3ce755-b5ee-451a-926b-ec2491acb854
[INFO] 2018-10-25 20:54:07,515 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 20:54:07,531 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 20:54:07,738 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 20:54:07,791 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 20:54:07,897 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540526047897
[INFO] 2018-10-25 20:54:07,899 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-3981f75a-90dd-4d15-860b-9558e10bc355/userFiles-63a8e252-ba26-4acb-807d-22e7f596c2d9/etl_config.json
[INFO] 2018-10-25 20:54:07,914 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540526047914
[INFO] 2018-10-25 20:54:07,915 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py to /tmp/spark-3981f75a-90dd-4d15-860b-9558e10bc355/userFiles-63a8e252-ba26-4acb-807d-22e7f596c2d9/JB_BOOKINGS_TIER.py
[INFO] 2018-10-25 20:54:07,920 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540526047920
[INFO] 2018-10-25 20:54:07,920 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-3981f75a-90dd-4d15-860b-9558e10bc355/userFiles-63a8e252-ba26-4acb-807d-22e7f596c2d9/packages.zip
[INFO] 2018-10-25 20:54:07,989 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 20:54:08,015 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24273.
[INFO] 2018-10-25 20:54:08,016 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:24273
[INFO] 2018-10-25 20:54:08,018 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 20:54:08,053 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 24273, None)
[INFO] 2018-10-25 20:54:08,060 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:24273 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 24273, None)
[INFO] 2018-10-25 20:54:08,063 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 24273, None)
[INFO] 2018-10-25 20:54:08,064 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 24273, None)
[INFO] 2018-10-25 20:54:08,327 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 20:54:08,327 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 20:54:08,846 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 21:03:20,734 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 21:03:20,749 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 21:03:20,767 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 21:03:20,786 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 21:03:20,787 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 21:03:20,789 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 21:03:20,796 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 21:03:20,807 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 21:03:20,808 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 21:03:20,810 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3981f75a-90dd-4d15-860b-9558e10bc355/pyspark-7ac588b5-42ef-4000-a25f-b7ba470ca52b
[INFO] 2018-10-25 21:03:20,810 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3981f75a-90dd-4d15-860b-9558e10bc355
[INFO] 2018-10-25 21:03:20,811 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f964810b-eabc-4444-9262-838a9ac51bf3
[WARN] 2018-10-25 21:55:42,338 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 21:55:42,717 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 21:55:42,720 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3ba54e13-f3d5-490b-a535-e42245c5d3d8
[WARN] 2018-10-25 21:56:27,933 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 21:56:28,754 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 21:56:28,780 org.apache.spark.SparkContext logInfo - Submitted application: JB_PRODUCT_BOOKING
[INFO] 2018-10-25 21:56:28,924 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 21:56:28,925 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 21:56:28,925 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 21:56:28,925 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 21:56:28,926 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 21:56:29,171 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 30677.
[INFO] 2018-10-25 21:56:29,199 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 21:56:29,221 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 21:56:29,224 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 21:56:29,225 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 21:56:29,236 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-0c741cd9-d262-4774-8504-2af0e3859e68
[INFO] 2018-10-25 21:56:29,257 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 21:56:29,273 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 21:56:29,470 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 21:56:29,529 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 21:56:29,620 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:30677/files/etl_config.json with timestamp 1540529789620
[INFO] 2018-10-25 21:56:29,622 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-e0857f6d-9bee-40e2-a7a5-672885d24ded/userFiles-a49b7b47-c847-4da1-8fbf-6909369ed6b3/etl_config.json
[INFO] 2018-10-25 21:56:29,635 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING.py at spark://vmwebietl02-dev:30677/files/JB_PRODUCT_BOOKING.py with timestamp 1540529789635
[INFO] 2018-10-25 21:56:29,636 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_PRODUCT_BOOKING.py to /tmp/spark-e0857f6d-9bee-40e2-a7a5-672885d24ded/userFiles-a49b7b47-c847-4da1-8fbf-6909369ed6b3/JB_PRODUCT_BOOKING.py
[INFO] 2018-10-25 21:56:29,640 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:30677/files/packages.zip with timestamp 1540529789640
[INFO] 2018-10-25 21:56:29,641 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-e0857f6d-9bee-40e2-a7a5-672885d24ded/userFiles-a49b7b47-c847-4da1-8fbf-6909369ed6b3/packages.zip
[INFO] 2018-10-25 21:56:29,728 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 21:56:29,791 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 38 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 21:56:29,919 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025215629-0003
[INFO] 2018-10-25 21:56:29,921 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025215629-0003/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 32 core(s)
[INFO] 2018-10-25 21:56:29,922 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025215629-0003/0 on hostPort 10.103.164.240:7078 with 32 core(s), 25.0 GB RAM
[INFO] 2018-10-25 21:56:29,929 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23827.
[INFO] 2018-10-25 21:56:29,930 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:23827
[INFO] 2018-10-25 21:56:29,932 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 21:56:29,960 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025215629-0003/0 is now RUNNING
[INFO] 2018-10-25 21:56:29,965 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 23827, None)
[INFO] 2018-10-25 21:56:29,971 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:23827 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 23827, None)
[INFO] 2018-10-25 21:56:29,977 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 23827, None)
[INFO] 2018-10-25 21:56:29,978 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 23827, None)
[INFO] 2018-10-25 21:56:30,153 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 21:56:30,363 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 21:56:30,364 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 21:56:30,855 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 21:56:30,901 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 21:56:30,912 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 21:56:30,918 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 21:56:30,921 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 21:56:30,934 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 21:56:30,946 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 21:56:30,947 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 21:56:30,957 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 21:56:30,961 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 21:56:30,969 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 21:56:30,969 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 21:56:30,970 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-56f5d4a5-fbbf-48f2-b633-1d2e89aadb87
[INFO] 2018-10-25 21:56:30,970 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e0857f6d-9bee-40e2-a7a5-672885d24ded/pyspark-4ab68502-9cfc-480d-b15c-42f542b87c67
[INFO] 2018-10-25 21:56:30,971 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e0857f6d-9bee-40e2-a7a5-672885d24ded
[WARN] 2018-10-25 22:21:38,751 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 22:21:39,647 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 22:21:39,673 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP_TRUNCATE
[INFO] 2018-10-25 22:21:39,834 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 22:21:39,835 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 22:21:39,835 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 22:21:39,835 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 22:21:39,836 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 22:21:40,059 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 20876.
[INFO] 2018-10-25 22:21:40,085 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 22:21:40,104 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 22:21:40,106 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 22:21:40,107 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 22:21:40,116 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-8b08ac1c-7b1a-4e50-bdb5-3cd45fc87144
[INFO] 2018-10-25 22:21:40,134 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 22:21:40,148 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 22:21:40,326 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 22:21:40,382 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 22:21:40,491 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540531300491
[INFO] 2018-10-25 22:21:40,494 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-1ca13080-1fab-49de-8130-241e5f91318e/userFiles-09fcdb9d-2723-4925-a0e3-fc71af3fb644/etl_config.json
[INFO] 2018-10-25 22:21:40,510 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py with timestamp 1540531300510
[INFO] 2018-10-25 22:21:40,511 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py to /tmp/spark-1ca13080-1fab-49de-8130-241e5f91318e/userFiles-09fcdb9d-2723-4925-a0e3-fc71af3fb644/JB_WORK_BOOKINGS_DATAPREP_TRUNCATE.py
[INFO] 2018-10-25 22:21:40,516 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540531300516
[INFO] 2018-10-25 22:21:40,517 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-1ca13080-1fab-49de-8130-241e5f91318e/userFiles-09fcdb9d-2723-4925-a0e3-fc71af3fb644/packages.zip
[INFO] 2018-10-25 22:21:40,587 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 22:21:40,613 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 31206.
[INFO] 2018-10-25 22:21:40,614 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:31206
[INFO] 2018-10-25 22:21:40,615 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 22:21:40,651 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 31206, None)
[INFO] 2018-10-25 22:21:40,656 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:31206 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 31206, None)
[INFO] 2018-10-25 22:21:40,658 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 31206, None)
[INFO] 2018-10-25 22:21:40,659 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 31206, None)
[INFO] 2018-10-25 22:21:40,927 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 22:21:40,928 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 22:21:41,459 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 22:21:41,837 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 22:21:41,849 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 22:21:41,860 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 22:21:41,870 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 22:21:41,871 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 22:21:41,878 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 22:21:41,883 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 22:21:41,893 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 22:21:41,894 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 22:21:41,895 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1ca13080-1fab-49de-8130-241e5f91318e
[INFO] 2018-10-25 22:21:41,895 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1ca13080-1fab-49de-8130-241e5f91318e/pyspark-b190d848-1603-4b0a-b722-665e2bf49ea4
[INFO] 2018-10-25 22:21:41,896 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-afdbfdb8-ceb7-47b5-9eae-8529cb060310
[WARN] 2018-10-25 22:21:59,977 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 22:22:00,784 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 22:22:00,810 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP
[INFO] 2018-10-25 22:22:01,014 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 22:22:01,015 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 22:22:01,015 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 22:22:01,015 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 22:22:01,016 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 22:22:01,221 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 23291.
[INFO] 2018-10-25 22:22:01,246 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 22:22:01,265 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 22:22:01,268 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 22:22:01,268 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 22:22:01,278 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-58728a03-da22-4d01-b62c-84f146700c59
[INFO] 2018-10-25 22:22:01,295 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 22:22:01,309 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[INFO] 2018-10-25 22:22:01,480 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4040.
[INFO] 2018-10-25 22:22:01,531 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4040
[INFO] 2018-10-25 22:22:01,620 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540531321619
[INFO] 2018-10-25 22:22:01,622 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-b4eb1b9f-4f05-43d4-8351-a2be3cb70891/userFiles-df4c3cf1-0081-46b1-abad-a141b03c91dd/etl_config.json
[INFO] 2018-10-25 22:22:01,634 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540531321634
[INFO] 2018-10-25 22:22:01,634 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py to /tmp/spark-b4eb1b9f-4f05-43d4-8351-a2be3cb70891/userFiles-df4c3cf1-0081-46b1-abad-a141b03c91dd/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-25 22:22:01,638 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540531321638
[INFO] 2018-10-25 22:22:01,639 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-b4eb1b9f-4f05-43d4-8351-a2be3cb70891/userFiles-df4c3cf1-0081-46b1-abad-a141b03c91dd/packages.zip
[INFO] 2018-10-25 22:22:01,705 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 22:22:01,731 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25631.
[INFO] 2018-10-25 22:22:01,732 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:25631
[INFO] 2018-10-25 22:22:01,734 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 22:22:01,767 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-25 22:22:01,774 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:25631 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-25 22:22:01,778 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-25 22:22:01,779 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 25631, None)
[INFO] 2018-10-25 22:22:02,057 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 22:22:02,058 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 22:22:02,645 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[WARN] 2018-10-25 22:39:47,484 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 22:39:48,325 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 22:39:48,349 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_BOOKINGS_DATAPREP
[INFO] 2018-10-25 22:39:48,680 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 22:39:48,681 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 22:39:48,681 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 22:39:48,681 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 22:39:48,689 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 22:39:48,877 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 23656.
[INFO] 2018-10-25 22:39:48,901 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 22:39:48,920 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 22:39:48,923 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 22:39:48,923 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 22:39:48,932 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-de862b7c-bbe5-4a21-9c11-20992e3610c7
[INFO] 2018-10-25 22:39:48,948 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 22:39:48,961 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 22:39:49,131 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 22:39:49,137 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 22:39:49,188 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 22:39:49,288 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540532389288
[INFO] 2018-10-25 22:39:49,290 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-e80ec2c7-c3b2-434d-a969-f8cb99cf58f4/userFiles-80e2bb51-c361-42b7-a1b7-cfb436b27465/etl_config.json
[INFO] 2018-10-25 22:39:49,302 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py with timestamp 1540532389302
[INFO] 2018-10-25 22:39:49,303 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_BOOKINGS_DATAPREP.py to /tmp/spark-e80ec2c7-c3b2-434d-a969-f8cb99cf58f4/userFiles-80e2bb51-c361-42b7-a1b7-cfb436b27465/JB_WORK_BOOKINGS_DATAPREP.py
[INFO] 2018-10-25 22:39:49,307 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540532389307
[INFO] 2018-10-25 22:39:49,308 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-e80ec2c7-c3b2-434d-a969-f8cb99cf58f4/userFiles-80e2bb51-c361-42b7-a1b7-cfb436b27465/packages.zip
[INFO] 2018-10-25 22:39:49,369 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 22:39:49,392 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18191.
[INFO] 2018-10-25 22:39:49,393 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:18191
[INFO] 2018-10-25 22:39:49,395 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 22:39:49,425 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18191, None)
[INFO] 2018-10-25 22:39:49,429 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:18191 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 18191, None)
[INFO] 2018-10-25 22:39:49,432 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18191, None)
[INFO] 2018-10-25 22:39:49,433 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 18191, None)
[INFO] 2018-10-25 22:39:49,735 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 22:39:49,736 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 22:39:50,322 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 22:49:29,488 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 22:49:29,503 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 22:49:29,518 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 22:49:29,532 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 22:49:29,533 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 22:49:29,534 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 22:49:29,541 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 22:49:29,550 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 22:49:29,551 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 22:49:29,553 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e80ec2c7-c3b2-434d-a969-f8cb99cf58f4
[INFO] 2018-10-25 22:49:29,554 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-e80ec2c7-c3b2-434d-a969-f8cb99cf58f4/pyspark-2c334eaf-7cb9-4e6a-9385-e3b9e5138929
[INFO] 2018-10-25 22:49:29,554 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1b42fa73-e3f6-4a5e-b4ec-2a0254e47206
[WARN] 2018-10-25 22:52:18,619 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 22:52:19,438 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 22:52:19,465 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 22:52:19,675 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 22:52:19,675 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 22:52:19,676 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 22:52:19,676 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 22:52:19,690 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 22:52:19,906 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 9555.
[INFO] 2018-10-25 22:52:19,931 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 22:52:19,950 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 22:52:19,953 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 22:52:19,953 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 22:52:19,962 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-06c2a3dd-d626-4b6d-aba0-cf56edc5dd89
[INFO] 2018-10-25 22:52:19,979 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 22:52:19,992 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 22:52:20,168 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 22:52:20,174 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 22:52:20,234 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 22:52:20,331 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540533140330
[INFO] 2018-10-25 22:52:20,333 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-91c3f3e7-b318-49e1-99f8-0e8e71f056f8/userFiles-8efcff17-4364-4cd2-add9-fa7a5bdf076e/etl_config.json
[INFO] 2018-10-25 22:52:20,347 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540533140347
[INFO] 2018-10-25 22:52:20,348 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-91c3f3e7-b318-49e1-99f8-0e8e71f056f8/userFiles-8efcff17-4364-4cd2-add9-fa7a5bdf076e/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 22:52:20,352 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540533140352
[INFO] 2018-10-25 22:52:20,352 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-91c3f3e7-b318-49e1-99f8-0e8e71f056f8/userFiles-8efcff17-4364-4cd2-add9-fa7a5bdf076e/packages.zip
[INFO] 2018-10-25 22:52:20,423 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 22:52:20,449 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 13163.
[INFO] 2018-10-25 22:52:20,450 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:13163
[INFO] 2018-10-25 22:52:20,452 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 22:52:20,488 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 13163, None)
[INFO] 2018-10-25 22:52:20,495 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:13163 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 13163, None)
[INFO] 2018-10-25 22:52:20,500 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 13163, None)
[INFO] 2018-10-25 22:52:20,501 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 13163, None)
[INFO] 2018-10-25 22:52:20,777 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 22:52:20,778 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 22:52:21,369 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 22:53:47,758 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 22:53:47,771 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 22:53:47,784 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 22:53:47,794 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 22:53:47,794 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 22:53:47,795 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 22:53:47,801 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 22:53:47,808 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 22:53:47,809 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 22:53:47,810 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-91c3f3e7-b318-49e1-99f8-0e8e71f056f8
[INFO] 2018-10-25 22:53:47,810 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-91c3f3e7-b318-49e1-99f8-0e8e71f056f8/pyspark-c4eb555e-f661-479e-bf3f-126c1033f41f
[INFO] 2018-10-25 22:53:47,811 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-8434374a-0398-4c5e-af5d-1772b65fd5ea
[WARN] 2018-10-25 23:04:57,402 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:04:58,224 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:04:58,249 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-25 23:04:58,509 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:04:58,510 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:04:58,510 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:04:58,510 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:04:58,511 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:04:58,712 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 18816.
[INFO] 2018-10-25 23:04:58,737 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:04:58,756 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:04:58,759 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:04:58,760 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:04:58,769 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-d4ef4a6c-cda8-406a-96b8-a387debace9d
[INFO] 2018-10-25 23:04:58,786 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:04:58,800 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:04:58,968 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 23:04:58,974 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 23:04:59,024 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:04:59,117 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540533899116
[INFO] 2018-10-25 23:04:59,118 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-989ef046-1449-41e1-b529-4ebef56c43f0/userFiles-553a6310-6c0d-4a2e-bd1f-89d6554d54a8/etl_config.json
[INFO] 2018-10-25 23:04:59,132 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py with timestamp 1540533899132
[INFO] 2018-10-25 23:04:59,133 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py to /tmp/spark-989ef046-1449-41e1-b529-4ebef56c43f0/userFiles-553a6310-6c0d-4a2e-bd1f-89d6554d54a8/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 23:04:59,137 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540533899137
[INFO] 2018-10-25 23:04:59,138 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-989ef046-1449-41e1-b529-4ebef56c43f0/userFiles-553a6310-6c0d-4a2e-bd1f-89d6554d54a8/packages.zip
[INFO] 2018-10-25 23:04:59,199 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 23:04:59,225 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30299.
[INFO] 2018-10-25 23:04:59,226 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:30299
[INFO] 2018-10-25 23:04:59,228 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:04:59,260 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 30299, None)
[INFO] 2018-10-25 23:04:59,264 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:30299 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 30299, None)
[INFO] 2018-10-25 23:04:59,266 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 30299, None)
[INFO] 2018-10-25 23:04:59,267 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 30299, None)
[INFO] 2018-10-25 23:04:59,539 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:04:59,539 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:05:00,050 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:05:03,174 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 205.772082 ms
[INFO] 2018-10-25 23:05:03,326 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 23:05:03,353 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 23:05:03,354 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 23:05:03,355 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:05:03,357 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:05:03,376 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 23:05:03,442 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 23:05:03,480 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 23:05:03,484 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:30299 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:05:03,487 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:05:03,501 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:05:03,502 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 23:05:03,544 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 23:05:03,555 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 23:05:03,560 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540533899116
[INFO] 2018-10-25 23:05:03,591 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-989ef046-1449-41e1-b529-4ebef56c43f0/userFiles-553a6310-6c0d-4a2e-bd1f-89d6554d54a8/etl_config.json
[INFO] 2018-10-25 23:05:03,597 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540533899137
[INFO] 2018-10-25 23:05:03,598 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-989ef046-1449-41e1-b529-4ebef56c43f0/userFiles-553a6310-6c0d-4a2e-bd1f-89d6554d54a8/packages.zip
[INFO] 2018-10-25 23:05:03,603 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py with timestamp 1540533899132
[INFO] 2018-10-25 23:05:03,605 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-989ef046-1449-41e1-b529-4ebef56c43f0/userFiles-553a6310-6c0d-4a2e-bd1f-89d6554d54a8/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 23:05:04,462 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 23:05:04,499 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 510, boot = 414, init = 96, finish = 0
[INFO] 2018-10-25 23:05:04,519 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 23:05:04,536 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1002 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 23:05:04,541 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:05:04,551 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.154 s
[INFO] 2018-10-25 23:05:04,556 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.228949 s
[INFO] 2018-10-25 23:05:04,954 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 23:05:04,956 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 23:05:04,957 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 23:05:04,957 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:05:04,958 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:05:04,959 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 23:05:04,964 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 23:05:04,968 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 23:05:04,969 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:30299 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:05:04,971 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:05:04,972 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:05:04,972 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 23:05:04,974 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 23:05:04,975 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 23:05:05,300 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 23:05:05,303 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 51, boot = -746, init = 797, finish = 0
[INFO] 2018-10-25 23:05:05,306 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 23:05:05,310 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 337 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 23:05:05,310 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:05:05,312 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.352 s
[INFO] 2018-10-25 23:05:05,315 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.359928 s
[INFO] 2018-10-25 23:05:06,039 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 23:05:06,040 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 23:05:06,041 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 23:05:06,041 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:05:06,042 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:05:06,043 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 23:05:06,048 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 23:05:06,050 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 23:05:06,052 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:30299 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:05:06,053 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:05:06,054 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:05:06,055 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 23:05:06,056 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 23:05:06,057 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 23:05:08,219 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 23:05:08,222 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 47, boot = -2867, init = 2914, finish = 0
[INFO] 2018-10-25 23:05:08,230 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1675 bytes result sent to driver
[INFO] 2018-10-25 23:05:08,233 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 2177 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 23:05:08,234 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:05:08,236 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 2.191 s
[INFO] 2018-10-25 23:05:08,237 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 2.197751 s
[INFO] 2018-10-25 23:05:08,269 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:05:08,279 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:05:08,295 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:05:08,310 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:05:08,311 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:05:08,322 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:05:08,326 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:05:08,339 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:05:08,340 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:05:08,343 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-989ef046-1449-41e1-b529-4ebef56c43f0
[INFO] 2018-10-25 23:05:08,344 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-989ef046-1449-41e1-b529-4ebef56c43f0/pyspark-2c2dd370-8b23-41e0-91ba-b4bbb7f8a6f5
[INFO] 2018-10-25 23:05:08,345 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-be8cff97-a526-4e5a-90e5-bb9b6524f667
[WARN] 2018-10-25 23:08:21,465 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:08:22,209 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:08:22,236 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER_PARTITION_TRUNCATE
[INFO] 2018-10-25 23:08:22,459 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:08:22,459 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:08:22,460 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:08:22,460 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:08:22,460 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:08:22,657 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 27307.
[INFO] 2018-10-25 23:08:22,683 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:08:22,702 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:08:22,705 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:08:22,705 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:08:22,714 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-69b7d2e3-8438-4d17-9644-c51ea5fcb57a
[INFO] 2018-10-25 23:08:22,731 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:08:22,744 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:08:22,924 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 23:08:22,932 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 23:08:22,994 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:08:23,132 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540534103131
[INFO] 2018-10-25 23:08:23,134 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-baf0b456-da1f-4353-a0a2-a2b1ae2d108f/userFiles-00c0334c-c3fc-4326-8eb0-a0823e21db1f/etl_config.json
[INFO] 2018-10-25 23:08:23,148 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py with timestamp 1540534103148
[INFO] 2018-10-25 23:08:23,149 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py to /tmp/spark-baf0b456-da1f-4353-a0a2-a2b1ae2d108f/userFiles-00c0334c-c3fc-4326-8eb0-a0823e21db1f/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 23:08:23,153 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540534103153
[INFO] 2018-10-25 23:08:23,153 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-baf0b456-da1f-4353-a0a2-a2b1ae2d108f/userFiles-00c0334c-c3fc-4326-8eb0-a0823e21db1f/packages.zip
[INFO] 2018-10-25 23:08:23,228 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 23:08:23,257 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12520.
[INFO] 2018-10-25 23:08:23,258 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:12520
[INFO] 2018-10-25 23:08:23,260 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:08:23,296 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 12520, None)
[INFO] 2018-10-25 23:08:23,303 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:12520 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 12520, None)
[INFO] 2018-10-25 23:08:23,308 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 12520, None)
[INFO] 2018-10-25 23:08:23,308 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 12520, None)
[INFO] 2018-10-25 23:08:23,598 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:08:23,599 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:08:24,195 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:08:27,331 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 233.063571 ms
[INFO] 2018-10-25 23:08:27,492 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 23:08:27,518 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 23:08:27,519 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 23:08:27,520 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:08:27,522 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:08:27,535 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 23:08:27,611 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 23:08:27,646 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 23:08:27,650 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:12520 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:08:27,653 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:08:27,667 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:08:27,668 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 23:08:27,715 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 23:08:27,727 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 23:08:27,732 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540534103131
[INFO] 2018-10-25 23:08:27,759 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-baf0b456-da1f-4353-a0a2-a2b1ae2d108f/userFiles-00c0334c-c3fc-4326-8eb0-a0823e21db1f/etl_config.json
[INFO] 2018-10-25 23:08:27,764 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540534103153
[INFO] 2018-10-25 23:08:27,765 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-baf0b456-da1f-4353-a0a2-a2b1ae2d108f/userFiles-00c0334c-c3fc-4326-8eb0-a0823e21db1f/packages.zip
[INFO] 2018-10-25 23:08:27,770 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py with timestamp 1540534103148
[INFO] 2018-10-25 23:08:27,771 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py has been previously copied to /tmp/spark-baf0b456-da1f-4353-a0a2-a2b1ae2d108f/userFiles-00c0334c-c3fc-4326-8eb0-a0823e21db1f/JB_BOOKINGS_TIER_PARTITION_TRUNCATE.py
[INFO] 2018-10-25 23:08:28,580 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 23:08:28,615 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 496, boot = 387, init = 108, finish = 1
[INFO] 2018-10-25 23:08:28,632 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1706 bytes result sent to driver
[INFO] 2018-10-25 23:08:28,650 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 946 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 23:08:28,655 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:08:28,666 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.107 s
[INFO] 2018-10-25 23:08:28,671 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.178855 s
[INFO] 2018-10-25 23:08:29,014 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412
[INFO] 2018-10-25 23:08:29,016 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) with 1 output partitions
[INFO] 2018-10-25 23:08:29,016 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412)
[INFO] 2018-10-25 23:08:29,016 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:08:29,016 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:08:29,017 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412), which has no missing parents
[INFO] 2018-10-25 23:08:29,021 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 10.3 KB, free 13.2 GB)
[INFO] 2018-10-25 23:08:29,023 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 13.2 GB)
[INFO] 2018-10-25 23:08:29,025 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:12520 (size: 5.8 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:08:29,026 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:08:29,027 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:08:29,027 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 23:08:29,029 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 23:08:29,030 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 23:08:29,312 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 23:08:29,333 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 42, boot = -662, init = 704, finish = 0
[INFO] 2018-10-25 23:08:29,336 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1661 bytes result sent to driver
[INFO] 2018-10-25 23:08:29,340 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 311 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 23:08:29,341 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:08:29,343 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412) finished in 0.325 s
[INFO] 2018-10-25 23:08:29,345 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:412, took 0.330292 s
[INFO] 2018-10-25 23:08:29,745 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446
[INFO] 2018-10-25 23:08:29,746 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) with 1 output partitions
[INFO] 2018-10-25 23:08:29,747 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446)
[INFO] 2018-10-25 23:08:29,747 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:08:29,748 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:08:29,749 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446), which has no missing parents
[INFO] 2018-10-25 23:08:29,755 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2 stored as values in memory (estimated size 10.4 KB, free 13.2 GB)
[INFO] 2018-10-25 23:08:29,758 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KB, free 13.2 GB)
[INFO] 2018-10-25 23:08:29,759 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_2_piece0 in memory on vmwebietl02-dev:12520 (size: 5.9 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:08:29,761 org.apache.spark.SparkContext logInfo - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:08:29,762 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[14] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:08:29,762 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 2.0 with 1 tasks
[INFO] 2018-10-25 23:08:29,763 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 23:08:29,764 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2018-10-25 23:08:30,412 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 23:08:30,414 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 44, boot = -1037, init = 1081, finish = 0
[INFO] 2018-10-25 23:08:30,417 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 2.0 (TID 2). 1675 bytes result sent to driver
[INFO] 2018-10-25 23:08:30,420 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 2.0 (TID 2) in 657 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 23:08:30,420 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:08:30,422 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 2 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446) finished in 0.671 s
[INFO] 2018-10-25 23:08:30,423 org.apache.spark.scheduler.DAGScheduler logInfo - Job 2 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:446, took 0.677954 s
[INFO] 2018-10-25 23:08:30,984 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:08:30,998 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:08:31,019 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:08:31,041 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:08:31,042 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:08:31,053 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:08:31,059 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:08:31,073 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:08:31,074 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:08:31,075 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-dedd7c99-0620-40d4-b9ca-74cdbfe65ca5
[INFO] 2018-10-25 23:08:31,076 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-baf0b456-da1f-4353-a0a2-a2b1ae2d108f
[INFO] 2018-10-25 23:08:31,077 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-baf0b456-da1f-4353-a0a2-a2b1ae2d108f/pyspark-ddb55f7d-2a7b-4d7e-955e-21d2d120a109
[WARN] 2018-10-25 23:10:21,665 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:10:22,429 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:10:22,452 org.apache.spark.SparkContext logInfo - Submitted application: JB_BOOKINGS_TIER
[INFO] 2018-10-25 23:10:22,691 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:10:22,691 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:10:22,692 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:10:22,692 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:10:22,692 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:10:22,891 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 27259.
[INFO] 2018-10-25 23:10:22,915 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:10:22,933 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:10:22,936 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:10:22,936 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:10:22,945 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-a1a15526-cdf2-4322-9563-59b9a623a74d
[INFO] 2018-10-25 23:10:22,962 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:10:22,975 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:10:23,139 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 23:10:23,144 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 23:10:23,193 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:10:23,282 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540534223282
[INFO] 2018-10-25 23:10:23,284 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-3fe5188b-ba8b-41e2-828a-0131d778f901/userFiles-c75a65b5-9752-408e-b67c-caa1b3e0df2d/etl_config.json
[INFO] 2018-10-25 23:10:23,296 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py at file:/home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py with timestamp 1540534223296
[INFO] 2018-10-25 23:10:23,297 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_BOOKINGS_TIER.py to /tmp/spark-3fe5188b-ba8b-41e2-828a-0131d778f901/userFiles-c75a65b5-9752-408e-b67c-caa1b3e0df2d/JB_BOOKINGS_TIER.py
[INFO] 2018-10-25 23:10:23,300 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540534223300
[INFO] 2018-10-25 23:10:23,300 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-3fe5188b-ba8b-41e2-828a-0131d778f901/userFiles-c75a65b5-9752-408e-b67c-caa1b3e0df2d/packages.zip
[INFO] 2018-10-25 23:10:23,365 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 23:10:23,390 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36697.
[INFO] 2018-10-25 23:10:23,391 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:36697
[INFO] 2018-10-25 23:10:23,392 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:10:23,428 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 36697, None)
[INFO] 2018-10-25 23:10:23,433 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:36697 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 36697, None)
[INFO] 2018-10-25 23:10:23,438 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 36697, None)
[INFO] 2018-10-25 23:10:23,438 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 36697, None)
[INFO] 2018-10-25 23:10:23,728 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:10:23,728 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:10:24,277 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:19:53,523 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:19:53,535 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:19:53,552 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:19:53,566 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:19:53,566 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:19:53,567 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:19:53,573 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:19:53,581 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:19:53,582 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:19:53,583 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-43ae5c9b-de0e-4dd0-968b-2fe5cd513731
[INFO] 2018-10-25 23:19:53,584 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3fe5188b-ba8b-41e2-828a-0131d778f901/pyspark-9ef96e5c-77d5-40d1-9e27-d2001896a362
[INFO] 2018-10-25 23:19:53,584 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3fe5188b-ba8b-41e2-828a-0131d778f901
[WARN] 2018-10-25 23:31:42,927 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:31:43,741 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:31:43,766 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE_DATAPREP_TRUNCATE
[INFO] 2018-10-25 23:31:43,974 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:31:43,975 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:31:43,975 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:31:43,975 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:31:43,976 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:31:44,197 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 33292.
[INFO] 2018-10-25 23:31:44,224 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:31:44,246 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:31:44,249 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:31:44,250 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:31:44,261 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-997b10eb-4e9e-4a45-ad10-411e46f30170
[INFO] 2018-10-25 23:31:44,281 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:31:44,297 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:31:44,470 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 23:31:44,476 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 23:31:44,524 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:31:44,614 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:33292/files/etl_config.json with timestamp 1540535504614
[INFO] 2018-10-25 23:31:44,616 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-ab623080-b505-4a3a-86d1-6030bd4754d1/userFiles-f29fa379-5568-41f8-ab28-10e2121d7544/etl_config.json
[INFO] 2018-10-25 23:31:44,628 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py at spark://vmwebietl02-dev:33292/files/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py with timestamp 1540535504628
[INFO] 2018-10-25 23:31:44,629 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py to /tmp/spark-ab623080-b505-4a3a-86d1-6030bd4754d1/userFiles-f29fa379-5568-41f8-ab28-10e2121d7544/JB_WORK_INVOICE_DATAPREP_TRUNCATE.py
[INFO] 2018-10-25 23:31:44,634 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:33292/files/packages.zip with timestamp 1540535504634
[INFO] 2018-10-25 23:31:44,635 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-ab623080-b505-4a3a-86d1-6030bd4754d1/userFiles-f29fa379-5568-41f8-ab28-10e2121d7544/packages.zip
[INFO] 2018-10-25 23:31:44,727 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 23:31:44,794 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 38 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 23:31:44,914 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025233144-0004
[INFO] 2018-10-25 23:31:44,920 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,922 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/0 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,922 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/1 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,923 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/1 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,923 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/2 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,924 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/2 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,924 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/3 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,925 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/3 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,925 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/4 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,926 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/4 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,926 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/5 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,927 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/5 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,927 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/6 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,928 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/6 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,928 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/7 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,929 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29077.
[INFO] 2018-10-25 23:31:44,929 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/7 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,930 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/8 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,930 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:29077
[INFO] 2018-10-25 23:31:44,930 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/8 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,931 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/9 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,931 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/9 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,931 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/10 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,932 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:31:44,932 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/10 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,933 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/11 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,933 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/11 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,934 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/12 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,934 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/12 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,935 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/13 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,936 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/13 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,936 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/14 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,937 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/14 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,937 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/15 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,937 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/15 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,938 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/16 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,938 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/16 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,938 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/17 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,939 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/17 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,939 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/18 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,940 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/18 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,940 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/19 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,940 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/19 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,941 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/20 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,941 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/20 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,942 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/21 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,942 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/21 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,942 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/22 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,943 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/22 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,943 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/23 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,943 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/23 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,944 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/24 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,944 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/24 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,944 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/25 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,945 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/25 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,945 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/26 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,946 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/26 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,946 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/27 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,946 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/27 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,946 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/28 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,947 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/28 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,947 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233144-0004/29 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:31:44,947 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233144-0004/29 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:31:44,966 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 29077, None)
[INFO] 2018-10-25 23:31:44,971 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:29077 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 29077, None)
[INFO] 2018-10-25 23:31:44,975 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/0 is now RUNNING
[INFO] 2018-10-25 23:31:44,976 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/1 is now RUNNING
[INFO] 2018-10-25 23:31:44,976 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/2 is now RUNNING
[INFO] 2018-10-25 23:31:44,977 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 29077, None)
[INFO] 2018-10-25 23:31:44,977 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/3 is now RUNNING
[INFO] 2018-10-25 23:31:44,978 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 29077, None)
[INFO] 2018-10-25 23:31:44,979 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/4 is now RUNNING
[INFO] 2018-10-25 23:31:44,980 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/5 is now RUNNING
[INFO] 2018-10-25 23:31:44,981 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/6 is now RUNNING
[INFO] 2018-10-25 23:31:44,983 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/7 is now RUNNING
[INFO] 2018-10-25 23:31:44,985 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/8 is now RUNNING
[INFO] 2018-10-25 23:31:44,986 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/9 is now RUNNING
[INFO] 2018-10-25 23:31:44,988 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/10 is now RUNNING
[INFO] 2018-10-25 23:31:44,990 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/11 is now RUNNING
[INFO] 2018-10-25 23:31:44,992 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/12 is now RUNNING
[INFO] 2018-10-25 23:31:44,993 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/13 is now RUNNING
[INFO] 2018-10-25 23:31:44,994 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/14 is now RUNNING
[INFO] 2018-10-25 23:31:44,996 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/15 is now RUNNING
[INFO] 2018-10-25 23:31:44,997 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/16 is now RUNNING
[INFO] 2018-10-25 23:31:44,998 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/17 is now RUNNING
[INFO] 2018-10-25 23:31:44,999 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/18 is now RUNNING
[INFO] 2018-10-25 23:31:45,002 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/19 is now RUNNING
[INFO] 2018-10-25 23:31:45,003 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/20 is now RUNNING
[INFO] 2018-10-25 23:31:45,006 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/21 is now RUNNING
[INFO] 2018-10-25 23:31:45,007 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/22 is now RUNNING
[INFO] 2018-10-25 23:31:45,009 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/23 is now RUNNING
[INFO] 2018-10-25 23:31:45,010 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/24 is now RUNNING
[INFO] 2018-10-25 23:31:45,010 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/25 is now RUNNING
[INFO] 2018-10-25 23:31:45,011 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/26 is now RUNNING
[INFO] 2018-10-25 23:31:45,012 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/27 is now RUNNING
[INFO] 2018-10-25 23:31:45,013 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/28 is now RUNNING
[INFO] 2018-10-25 23:31:45,014 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233144-0004/29 is now RUNNING
[INFO] 2018-10-25 23:31:45,157 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 23:31:45,374 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:31:45,375 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:31:45,843 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:31:46,663 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:31:46,676 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:31:46,683 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 23:31:46,689 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 23:31:46,703 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:31:46,713 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:31:46,714 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:31:46,725 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:31:46,728 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:31:46,743 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:31:46,744 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:31:46,746 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ab623080-b505-4a3a-86d1-6030bd4754d1/pyspark-672614d7-d58d-4856-8af4-3544dd6cee8a
[INFO] 2018-10-25 23:31:46,747 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-ab623080-b505-4a3a-86d1-6030bd4754d1
[INFO] 2018-10-25 23:31:46,748 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-3e0fd4cd-5203-4f38-984d-ad9bd84dfd8b
[WARN] 2018-10-25 23:32:31,681 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:32:32,451 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:32:32,478 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE_DATAPREP
[INFO] 2018-10-25 23:32:32,638 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:32:32,638 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:32:32,639 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:32:32,639 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:32:32,646 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:32:32,842 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 25948.
[INFO] 2018-10-25 23:32:32,866 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:32:32,886 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:32:32,889 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:32:32,889 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:32:32,899 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-da5bc0ab-5918-4fbc-9dd5-2ff014085769
[INFO] 2018-10-25 23:32:32,917 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:32:32,930 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:32:33,103 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 23:32:33,109 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 23:32:33,160 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:32:33,252 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:25948/files/etl_config.json with timestamp 1540535553252
[INFO] 2018-10-25 23:32:33,254 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-cf3d74ee-89d3-426c-a7d1-d0f2ddb29e76/userFiles-37f93bd9-50d6-4754-81bc-a5797f696776/etl_config.json
[INFO] 2018-10-25 23:32:33,268 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py at spark://vmwebietl02-dev:25948/files/JB_WORK_INVOICE_DATAPREP.py with timestamp 1540535553268
[INFO] 2018-10-25 23:32:33,269 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py to /tmp/spark-cf3d74ee-89d3-426c-a7d1-d0f2ddb29e76/userFiles-37f93bd9-50d6-4754-81bc-a5797f696776/JB_WORK_INVOICE_DATAPREP.py
[INFO] 2018-10-25 23:32:33,273 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:25948/files/packages.zip with timestamp 1540535553273
[INFO] 2018-10-25 23:32:33,274 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-cf3d74ee-89d3-426c-a7d1-d0f2ddb29e76/userFiles-37f93bd9-50d6-4754-81bc-a5797f696776/packages.zip
[INFO] 2018-10-25 23:32:33,368 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 23:32:33,435 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 35 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 23:32:33,551 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025233233-0005
[INFO] 2018-10-25 23:32:33,557 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,558 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/0 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,558 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/1 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,559 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/1 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,559 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/2 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,560 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/2 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,560 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/3 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,561 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/3 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,561 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/4 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,562 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/4 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,562 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/5 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,563 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/5 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,563 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/6 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,563 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 22005.
[INFO] 2018-10-25 23:32:33,564 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/6 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,564 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:22005
[INFO] 2018-10-25 23:32:33,564 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/7 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,565 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/7 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,565 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/8 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,566 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:32:33,566 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/8 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,567 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/9 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,567 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/9 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,568 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/10 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,568 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/10 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,569 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/11 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,569 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/11 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,569 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/12 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,570 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/12 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,570 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/13 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,571 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/13 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,572 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/14 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,572 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/14 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,573 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/15 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,573 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/15 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,574 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/16 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,574 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/16 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,574 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/17 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,575 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/17 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,575 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/18 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,576 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/18 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,576 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/19 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,576 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/19 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,577 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/20 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,577 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/20 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,577 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/21 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,578 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/21 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,578 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/22 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,579 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/22 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,579 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/23 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,579 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/23 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,580 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/24 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,580 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/24 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,580 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/25 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,581 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/25 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,582 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/26 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,582 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/26 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,582 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/27 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,583 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/27 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,583 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/28 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,583 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/28 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,584 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233233-0005/29 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:32:33,584 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233233-0005/29 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:32:33,590 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/0 is now RUNNING
[INFO] 2018-10-25 23:32:33,591 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/1 is now RUNNING
[INFO] 2018-10-25 23:32:33,592 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/2 is now RUNNING
[INFO] 2018-10-25 23:32:33,593 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/3 is now RUNNING
[INFO] 2018-10-25 23:32:33,595 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/4 is now RUNNING
[INFO] 2018-10-25 23:32:33,597 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/5 is now RUNNING
[INFO] 2018-10-25 23:32:33,598 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/6 is now RUNNING
[INFO] 2018-10-25 23:32:33,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/7 is now RUNNING
[INFO] 2018-10-25 23:32:33,602 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/8 is now RUNNING
[INFO] 2018-10-25 23:32:33,605 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/9 is now RUNNING
[INFO] 2018-10-25 23:32:33,605 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 22005, None)
[INFO] 2018-10-25 23:32:33,606 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/10 is now RUNNING
[INFO] 2018-10-25 23:32:33,608 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/11 is now RUNNING
[INFO] 2018-10-25 23:32:33,608 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/12 is now RUNNING
[INFO] 2018-10-25 23:32:33,609 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/13 is now RUNNING
[INFO] 2018-10-25 23:32:33,610 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:22005 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 22005, None)
[INFO] 2018-10-25 23:32:33,611 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/14 is now RUNNING
[INFO] 2018-10-25 23:32:33,613 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 22005, None)
[INFO] 2018-10-25 23:32:33,613 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/15 is now RUNNING
[INFO] 2018-10-25 23:32:33,614 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 22005, None)
[INFO] 2018-10-25 23:32:33,614 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/16 is now RUNNING
[INFO] 2018-10-25 23:32:33,616 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/17 is now RUNNING
[INFO] 2018-10-25 23:32:33,617 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/18 is now RUNNING
[INFO] 2018-10-25 23:32:33,618 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/19 is now RUNNING
[INFO] 2018-10-25 23:32:33,619 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/20 is now RUNNING
[INFO] 2018-10-25 23:32:33,619 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/21 is now RUNNING
[INFO] 2018-10-25 23:32:33,621 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/22 is now RUNNING
[INFO] 2018-10-25 23:32:33,622 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/23 is now RUNNING
[INFO] 2018-10-25 23:32:33,623 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/24 is now RUNNING
[INFO] 2018-10-25 23:32:33,624 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/25 is now RUNNING
[INFO] 2018-10-25 23:32:33,627 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/26 is now RUNNING
[INFO] 2018-10-25 23:32:33,629 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/27 is now RUNNING
[INFO] 2018-10-25 23:32:33,630 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/28 is now RUNNING
[INFO] 2018-10-25 23:32:33,632 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233233-0005/29 is now RUNNING
[INFO] 2018-10-25 23:32:33,788 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 23:32:34,046 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:32:34,047 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:32:34,482 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:32:34,525 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:32:34,538 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:32:34,544 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 23:32:34,548 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 23:32:34,562 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:32:34,571 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:32:34,571 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:32:34,582 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:32:34,584 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:32:34,593 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:32:34,594 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:32:34,595 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f6f96bb9-613d-4372-bef6-0fa8812be6cd
[INFO] 2018-10-25 23:32:34,596 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-cf3d74ee-89d3-426c-a7d1-d0f2ddb29e76/pyspark-30c8d970-4bc5-4835-a8c3-2bf4c65e6632
[INFO] 2018-10-25 23:32:34,597 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-cf3d74ee-89d3-426c-a7d1-d0f2ddb29e76
[WARN] 2018-10-25 23:35:13,177 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:35:13,945 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:35:13,972 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE_DATAPREP
[INFO] 2018-10-25 23:35:14,176 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:35:14,177 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:35:14,177 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:35:14,177 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:35:14,178 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:35:14,376 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 26872.
[INFO] 2018-10-25 23:35:14,400 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:35:14,419 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:35:14,422 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:35:14,423 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:35:14,432 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-32525f61-444f-48dd-86c5-f17cf569cbf6
[INFO] 2018-10-25 23:35:14,449 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:35:14,464 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:35:14,655 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 23:35:14,661 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 23:35:14,721 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:35:14,843 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at spark://vmwebietl02-dev:26872/files/etl_config.json with timestamp 1540535714842
[INFO] 2018-10-25 23:35:14,846 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-4f6aade2-24b8-4d3f-b70c-b616deaf25f4/userFiles-1d0ac31d-9cf3-4dd0-8a44-657045261475/etl_config.json
[INFO] 2018-10-25 23:35:14,863 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py at spark://vmwebietl02-dev:26872/files/JB_WORK_INVOICE_DATAPREP.py with timestamp 1540535714863
[INFO] 2018-10-25 23:35:14,864 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py to /tmp/spark-4f6aade2-24b8-4d3f-b70c-b616deaf25f4/userFiles-1d0ac31d-9cf3-4dd0-8a44-657045261475/JB_WORK_INVOICE_DATAPREP.py
[INFO] 2018-10-25 23:35:14,868 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at spark://vmwebietl02-dev:26872/files/packages.zip with timestamp 1540535714868
[INFO] 2018-10-25 23:35:14,869 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-4f6aade2-24b8-4d3f-b70c-b616deaf25f4/userFiles-1d0ac31d-9cf3-4dd0-8a44-657045261475/packages.zip
[INFO] 2018-10-25 23:35:14,964 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Connecting to master spark://vmwbidapp03-dev.corp.netapp.com:7077...
[INFO] 2018-10-25 23:35:15,031 org.apache.spark.network.client.TransportClientFactory createClient - Successfully created connection to vmwbidapp03-dev.corp.netapp.com/10.103.164.240:7077 after 39 ms (0 ms spent in bootstraps)
[INFO] 2018-10-25 23:35:15,149 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Connected to Spark cluster with app ID app-20181025233515-0006
[INFO] 2018-10-25 23:35:15,152 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/0 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,154 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/0 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,155 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/1 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,155 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/1 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,155 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/2 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,156 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/2 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,156 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/3 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,157 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/3 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,158 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/4 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,158 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/4 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,158 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/5 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,159 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/5 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,159 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/6 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,160 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/6 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,160 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/7 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,161 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/7 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,161 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/8 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,162 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/8 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,162 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/9 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,164 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/9 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,164 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/10 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,164 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11777.
[INFO] 2018-10-25 23:35:15,165 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/10 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,165 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/11 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,165 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:11777
[INFO] 2018-10-25 23:35:15,165 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/11 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,166 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/12 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,166 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/12 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,167 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/13 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,167 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/13 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,168 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:35:15,168 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/14 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,168 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/14 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,168 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/15 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,169 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/15 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,169 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/16 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,170 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/16 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,170 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/17 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,171 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/17 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,171 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/18 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,171 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/18 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,172 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/19 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,172 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/19 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,172 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/20 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,173 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/20 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,174 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/21 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,174 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/21 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,174 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/22 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,175 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/22 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,175 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/23 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,175 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/23 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,176 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/24 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,177 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/24 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,177 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/25 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,177 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/25 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,178 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/26 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,178 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/26 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,178 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/27 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,179 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/27 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,179 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/28 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,179 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/28 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,180 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/29 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:15,181 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/29 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:15,183 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/0 is now RUNNING
[INFO] 2018-10-25 23:35:15,183 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/1 is now RUNNING
[INFO] 2018-10-25 23:35:15,184 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/2 is now RUNNING
[INFO] 2018-10-25 23:35:15,185 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/3 is now RUNNING
[INFO] 2018-10-25 23:35:15,186 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/4 is now RUNNING
[INFO] 2018-10-25 23:35:15,187 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/5 is now RUNNING
[INFO] 2018-10-25 23:35:15,190 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/6 is now RUNNING
[INFO] 2018-10-25 23:35:15,193 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/7 is now RUNNING
[INFO] 2018-10-25 23:35:15,195 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/8 is now RUNNING
[INFO] 2018-10-25 23:35:15,196 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/9 is now RUNNING
[INFO] 2018-10-25 23:35:15,197 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/10 is now RUNNING
[INFO] 2018-10-25 23:35:15,197 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/11 is now RUNNING
[INFO] 2018-10-25 23:35:15,199 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/12 is now RUNNING
[INFO] 2018-10-25 23:35:15,200 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/13 is now RUNNING
[INFO] 2018-10-25 23:35:15,201 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/14 is now RUNNING
[INFO] 2018-10-25 23:35:15,202 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/15 is now RUNNING
[INFO] 2018-10-25 23:35:15,204 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/16 is now RUNNING
[INFO] 2018-10-25 23:35:15,206 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 11777, None)
[INFO] 2018-10-25 23:35:15,206 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/17 is now RUNNING
[INFO] 2018-10-25 23:35:15,210 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:11777 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 11777, None)
[INFO] 2018-10-25 23:35:15,213 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 11777, None)
[INFO] 2018-10-25 23:35:15,213 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/18 is now RUNNING
[INFO] 2018-10-25 23:35:15,214 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 11777, None)
[INFO] 2018-10-25 23:35:15,214 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/19 is now RUNNING
[INFO] 2018-10-25 23:35:15,215 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/20 is now RUNNING
[INFO] 2018-10-25 23:35:15,216 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/21 is now RUNNING
[INFO] 2018-10-25 23:35:15,217 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/22 is now RUNNING
[INFO] 2018-10-25 23:35:15,218 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/23 is now RUNNING
[INFO] 2018-10-25 23:35:15,219 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/24 is now RUNNING
[INFO] 2018-10-25 23:35:15,220 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/25 is now RUNNING
[INFO] 2018-10-25 23:35:15,221 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/26 is now RUNNING
[INFO] 2018-10-25 23:35:15,224 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/27 is now RUNNING
[INFO] 2018-10-25 23:35:15,227 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/28 is now RUNNING
[INFO] 2018-10-25 23:35:15,236 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/29 is now RUNNING
[INFO] 2018-10-25 23:35:15,398 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO] 2018-10-25 23:35:15,621 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:35:15,622 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:35:16,075 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:35:19,373 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 213.332509 ms
[INFO] 2018-10-25 23:35:19,517 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 23:35:19,541 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 23:35:19,542 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 23:35:19,543 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:35:19,544 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:35:19,559 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 23:35:19,581 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34394) with ID 8
[INFO] 2018-10-25 23:35:19,635 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 23:35:19,653 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34386) with ID 21
[INFO] 2018-10-25 23:35:19,671 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 23:35:19,675 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:11777 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:35:19,678 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:35:19,697 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:35:19,698 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[ERROR] 2018-10-25 23:35:19,699 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6042833870254231958
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:19,733 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 21, partition 0, PROCESS_LOCAL, 7681 bytes)
[ERROR] 2018-10-25 23:35:19,750 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 8 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:19,757 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34406) with ID 6
[INFO] 2018-10-25 23:35:19,759 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34408) with ID 24
[INFO] 2018-10-25 23:35:19,759 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 8 (epoch 0)
[INFO] 2018-10-25 23:35:19,767 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 8 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:19,769 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 8 successfully in removeExecutor
[INFO] 2018-10-25 23:35:19,770 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 8 (epoch 0)
[INFO] 2018-10-25 23:35:19,771 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34398) with ID 1
[ERROR] 2018-10-25 23:35:19,813 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7718637493716791112
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:19,822 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34434) with ID 3
[INFO] 2018-10-25 23:35:19,829 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34416) with ID 17
[ERROR] 2018-10-25 23:35:19,842 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 21 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[WARN] 2018-10-25 23:35:19,848 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.0 in stage 0.0 (TID 0, 10.103.164.240, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:19,850 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34420) with ID 10
[INFO] 2018-10-25 23:35:19,851 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 10, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-25 23:35:19,857 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34438) with ID 26
[INFO] 2018-10-25 23:35:19,857 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 21 (epoch 1)
[INFO] 2018-10-25 23:35:19,857 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 21 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:19,860 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34412) with ID 22
[INFO] 2018-10-25 23:35:19,860 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 21 successfully in removeExecutor
[INFO] 2018-10-25 23:35:19,861 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 21 (epoch 1)
[INFO] 2018-10-25 23:35:19,874 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34430) with ID 5
[ERROR] 2018-10-25 23:35:19,887 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6105671496257316025
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:19,895 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34432) with ID 20
[INFO] 2018-10-25 23:35:19,907 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34436) with ID 9
[ERROR] 2018-10-25 23:35:19,908 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 24 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:19,909 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34424) with ID 19
[INFO] 2018-10-25 23:35:19,909 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 24 (epoch 2)
[INFO] 2018-10-25 23:35:19,909 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 24 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:19,910 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 24 successfully in removeExecutor
[INFO] 2018-10-25 23:35:19,910 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 24 (epoch 2)
[INFO] 2018-10-25 23:35:19,923 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34426) with ID 2
[INFO] 2018-10-25 23:35:19,932 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34414) with ID 4
[ERROR] 2018-10-25 23:35:19,943 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7037994928856332878
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:19,946 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34418) with ID 13
[INFO] 2018-10-25 23:35:19,960 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34410) with ID 0
[ERROR] 2018-10-25 23:35:19,964 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6498067517974495411
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:19,977 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4818292914056278825
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:19,988 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 6 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:19,989 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 6 (epoch 3)
[INFO] 2018-10-25 23:35:19,989 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 6 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:19,990 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 6 successfully in removeExecutor
[ERROR] 2018-10-25 23:35:19,990 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6575943451447388974
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:19,990 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 6 (epoch 3)
[ERROR] 2018-10-25 23:35:19,996 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5003957181113626435
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:19,997 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6213295986464105495
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:20,130 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/8 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-25 23:35:20,086 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5309019243674432248
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,062 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5139556244861717445
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,030 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4719206578229831014
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,003 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7202104136854533567
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,003 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 26 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-25 23:35:19,999 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9123569874733957791
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:19,998 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 4914919163751803448
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:20,376 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34434
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,337 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8388786755416899598
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,336 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6333147799368917005
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:20,336 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 26 (epoch 4)
[ERROR] 2018-10-25 23:35:20,336 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 3 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,333 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/8 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:20,578 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 26 from BlockManagerMaster.
[ERROR] 2018-10-25 23:35:20,576 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9164272185934564489
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:20,578 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/30 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:20,578 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 26 successfully in removeExecutor
[ERROR] 2018-10-25 23:35:20,578 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 22 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,579 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/30 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:20,580 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34450) with ID 29
[INFO] 2018-10-25 23:35:20,580 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 26 (epoch 4)
[INFO] 2018-10-25 23:35:20,580 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/30 is now RUNNING
[INFO] 2018-10-25 23:35:20,580 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 3 (epoch 5)
[INFO] 2018-10-25 23:35:20,581 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34462) with ID 18
[INFO] 2018-10-25 23:35:20,581 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/21 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:20,581 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,582 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34422) with ID 14
[INFO] 2018-10-25 23:35:20,582 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 3 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,582 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 3 (epoch 5)
[INFO] 2018-10-25 23:35:20,581 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/21 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:20,582 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 22 (epoch 6)
[INFO] 2018-10-25 23:35:20,582 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34440) with ID 27
[INFO] 2018-10-25 23:35:20,583 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 22 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,583 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 22 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,583 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/31 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:20,584 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 22 (epoch 6)
[INFO] 2018-10-25 23:35:20,584 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34446) with ID 25
[INFO] 2018-10-25 23:35:20,585 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/31 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:20,585 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/31 is now RUNNING
[ERROR] 2018-10-25 23:35:20,585 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 13 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,585 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/6 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:20,586 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 13 (epoch 7)
[INFO] 2018-10-25 23:35:20,586 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 13 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,586 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/6 removed: Command exited with code 1
[ERROR] 2018-10-25 23:35:20,586 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 10 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,586 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 13 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,587 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/32 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:20,589 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 13 (epoch 7)
[INFO] 2018-10-25 23:35:20,590 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/32 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[WARN] 2018-10-25 23:35:20,587 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.1 in stage 0.0 (TID 1, 10.103.164.240, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-25 23:35:20,591 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 19 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,590 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/24 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:20,591 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 10 (epoch 8)
[INFO] 2018-10-25 23:35:20,591 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 10 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,592 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 10 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,592 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 10 (epoch 8)
[INFO] 2018-10-25 23:35:20,592 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/24 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:20,592 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/33 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:20,593 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/33 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:20,593 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/32 is now RUNNING
[INFO] 2018-10-25 23:35:20,592 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 19 (epoch 9)
[ERROR] 2018-10-25 23:35:20,592 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 9 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,593 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/33 is now RUNNING
[INFO] 2018-10-25 23:35:20,593 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 19 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,594 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/3 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:20,594 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34454) with ID 11
[INFO] 2018-10-25 23:35:20,594 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 19 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,594 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 19 (epoch 9)
[INFO] 2018-10-25 23:35:20,594 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 9 (epoch 10)
[INFO] 2018-10-25 23:35:20,594 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 9 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,594 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/3 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:20,595 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 18, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-25 23:35:20,595 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 9 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,595 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 9 (epoch 10)
[INFO] 2018-10-25 23:35:20,595 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/34 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:20,596 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/34 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:20,596 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34456) with ID 16
[INFO] 2018-10-25 23:35:20,596 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/22 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:20,596 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/22 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:20,597 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/35 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-25 23:35:20,597 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 17 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,597 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/35 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:20,597 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/17 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:20,598 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 17 (epoch 11)
[INFO] 2018-10-25 23:35:20,598 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/17 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:20,598 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 17 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,598 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 17 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,598 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 8 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,598 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 17 (epoch 11)
[INFO] 2018-10-25 23:35:20,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/36 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:20,599 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 8 requested
[INFO] 2018-10-25 23:35:20,599 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/36 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:20,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/34 is now RUNNING
[INFO] 2018-10-25 23:35:20,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/35 is now RUNNING
[INFO] 2018-10-25 23:35:20,599 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/36 is now RUNNING
[INFO] 2018-10-25 23:35:20,600 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 8
[INFO] 2018-10-25 23:35:20,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/26 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:20,600 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/26 removed: Command exited with code 1
[ERROR] 2018-10-25 23:35:20,600 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 2 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,600 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/37 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:20,600 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/37 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:20,601 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/37 is now RUNNING
[INFO] 2018-10-25 23:35:20,602 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 2 (epoch 12)
[INFO] 2018-10-25 23:35:20,602 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,603 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 2 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,603 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 2 (epoch 12)
[ERROR] 2018-10-25 23:35:20,603 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 1 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,604 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 1 (epoch 13)
[INFO] 2018-10-25 23:35:20,604 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 21 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,604 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 21 requested
[INFO] 2018-10-25 23:35:20,604 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,604 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 21
[INFO] 2018-10-25 23:35:20,605 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 1 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,605 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 1 (epoch 13)
[INFO] 2018-10-25 23:35:20,605 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 6 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,605 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 6 requested
[INFO] 2018-10-25 23:35:20,605 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 6
[INFO] 2018-10-25 23:35:20,606 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 24 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,606 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 24 requested
[INFO] 2018-10-25 23:35:20,606 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 24
[INFO] 2018-10-25 23:35:20,607 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 3 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,607 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 3 requested
[INFO] 2018-10-25 23:35:20,607 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 3
[INFO] 2018-10-25 23:35:20,607 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 22 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,607 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 22 requested
[INFO] 2018-10-25 23:35:20,608 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 22
[INFO] 2018-10-25 23:35:20,608 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 17 requested
[INFO] 2018-10-25 23:35:20,608 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 17
[INFO] 2018-10-25 23:35:20,608 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 17 from BlockManagerMaster.
[ERROR] 2018-10-25 23:35:20,609 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 5 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:20,609 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34442) with ID 12
[INFO] 2018-10-25 23:35:20,609 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 5 (epoch 14)
[INFO] 2018-10-25 23:35:20,610 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,610 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 5 successfully in removeExecutor
[INFO] 2018-10-25 23:35:20,610 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 5 (epoch 14)
[INFO] 2018-10-25 23:35:20,610 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 26 requested
[INFO] 2018-10-25 23:35:20,611 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 26
[INFO] 2018-10-25 23:35:20,610 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 26 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:20,611 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34448) with ID 23
[ERROR] 2018-10-25 23:35:20,612 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 4 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-25 23:35:20,613 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 20 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[ERROR] 2018-10-25 23:35:20,724 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8561146107146391419
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:20,718 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34418
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:20,712 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/2 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-25 23:35:20,695 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6339619939117853308
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,693 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 7545708229849872239
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,687 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8049025392455308174
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,676 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8883682440582714459
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,670 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8353011493962864664
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:20,660 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5479880286955475000
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:20,613 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 4 (epoch 15)
[ERROR] 2018-10-25 23:35:21,042 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6859718344881181658
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,042 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34426
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,040 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34420
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,039 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34436
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:21,037 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 8872480481130036543
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:21,035 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/2 removed: Command exited with code 1
[WARN] 2018-10-25 23:35:21,035 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34398
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:21,034 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 0 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,375 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/38 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[WARN] 2018-10-25 23:35:21,045 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34424
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:21,044 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,376 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/38 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,377 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 4 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,377 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34464) with ID 28
[INFO] 2018-10-25 23:35:21,377 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/10 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,377 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 4 (epoch 15)
[INFO] 2018-10-25 23:35:21,377 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/10 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,378 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 20 (epoch 16)
[INFO] 2018-10-25 23:35:21,378 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34460) with ID 15
[INFO] 2018-10-25 23:35:21,378 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 20 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,378 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/39 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,379 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 20 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,379 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/39 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,379 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/1 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,379 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 20 (epoch 16)
[INFO] 2018-10-25 23:35:21,379 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.103.164.240:34466) with ID 7
[INFO] 2018-10-25 23:35:21,380 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 0 (epoch 17)
[INFO] 2018-10-25 23:35:21,380 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/1 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,381 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,381 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/40 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,382 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 0 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,382 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/40 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,382 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 0 (epoch 17)
[INFO] 2018-10-25 23:35:21,382 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/9 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,383 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/9 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,383 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/41 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,384 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/41 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,384 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/38 is now RUNNING
[INFO] 2018-10-25 23:35:21,384 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/13 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-25 23:35:21,384 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 23 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,384 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/13 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,385 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/42 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,385 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 23 (epoch 18)
[ERROR] 2018-10-25 23:35:21,385 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 12 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,385 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 23 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,385 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/42 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,385 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 23 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,386 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/19 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-25 23:35:21,386 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 14 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,386 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 23 (epoch 18)
[INFO] 2018-10-25 23:35:21,386 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/19 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,386 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 12 (epoch 19)
[ERROR] 2018-10-25 23:35:21,386 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 11 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,386 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/43 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,386 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 12 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,387 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/43 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[ERROR] 2018-10-25 23:35:21,387 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 18 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,387 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/39 is now RUNNING
[WARN] 2018-10-25 23:35:21,387 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.2 in stage 0.0 (TID 2, 10.103.164.240, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,388 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/40 is now RUNNING
[INFO] 2018-10-25 23:35:21,388 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/41 is now RUNNING
[INFO] 2018-10-25 23:35:21,388 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 12 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,388 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 2 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,388 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 2 requested
[INFO] 2018-10-25 23:35:21,388 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/42 is now RUNNING
[INFO] 2018-10-25 23:35:21,388 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 2
[INFO] 2018-10-25 23:35:21,388 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 12 (epoch 19)
[INFO] 2018-10-25 23:35:21,389 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/43 is now RUNNING
[INFO] 2018-10-25 23:35:21,389 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 14 (epoch 20)
[INFO] 2018-10-25 23:35:21,389 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/0 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,389 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 14 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,389 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 28, partition 0, PROCESS_LOCAL, 7681 bytes)
[INFO] 2018-10-25 23:35:21,389 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/0 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,390 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/44 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,390 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 14 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,390 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/44 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,390 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 14 (epoch 20)
[INFO] 2018-10-25 23:35:21,390 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/44 is now RUNNING
[INFO] 2018-10-25 23:35:21,390 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 11 (epoch 21)
[INFO] 2018-10-25 23:35:21,390 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/20 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,390 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[ERROR] 2018-10-25 23:35:21,390 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 27 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,391 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/20 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,391 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 11 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,391 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/45 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,391 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 11 (epoch 21)
[INFO] 2018-10-25 23:35:21,391 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/45 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[ERROR] 2018-10-25 23:35:21,391 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 16 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,392 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 18 (epoch 22)
[INFO] 2018-10-25 23:35:21,391 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/45 is now RUNNING
[INFO] 2018-10-25 23:35:21,392 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 18 from BlockManagerMaster.
[ERROR] 2018-10-25 23:35:21,392 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 25 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,392 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 18 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,392 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/4 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,392 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 18 (epoch 22)
[INFO] 2018-10-25 23:35:21,392 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/4 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,393 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 27 (epoch 23)
[INFO] 2018-10-25 23:35:21,393 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 27 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,393 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 27 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,393 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/46 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,393 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 10 requested
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/46 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,393 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 10 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 27 (epoch 23)
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/5 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 10
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/5 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 1 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 16 (epoch 24)
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/47 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,394 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 1 requested
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 1
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 16 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 9 requested
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 9
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 9 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 16 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 13 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,396 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 16 (epoch 24)
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 13 requested
[INFO] 2018-10-25 23:35:21,396 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 13
[INFO] 2018-10-25 23:35:21,395 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/47 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,396 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 25 (epoch 25)
[INFO] 2018-10-25 23:35:21,397 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/46 is now RUNNING
[INFO] 2018-10-25 23:35:21,397 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 19 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,397 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/47 is now RUNNING
[INFO] 2018-10-25 23:35:21,397 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 19 requested
[INFO] 2018-10-25 23:35:21,397 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 25 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,397 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 19
[INFO] 2018-10-25 23:35:21,397 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 25 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,398 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 25 (epoch 25)
[INFO] 2018-10-25 23:35:21,398 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 0 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,398 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 0 requested
[INFO] 2018-10-25 23:35:21,398 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 0
[INFO] 2018-10-25 23:35:21,398 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 20 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,398 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 20 requested
[INFO] 2018-10-25 23:35:21,398 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 20
[INFO] 2018-10-25 23:35:21,399 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 4 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,399 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 4 requested
[INFO] 2018-10-25 23:35:21,399 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 4
[INFO] 2018-10-25 23:35:21,399 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 5 requested
[INFO] 2018-10-25 23:35:21,400 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 5
[INFO] 2018-10-25 23:35:21,399 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 5 from BlockManagerMaster.
[ERROR] 2018-10-25 23:35:21,400 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 29 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,401 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 29 (epoch 26)
[INFO] 2018-10-25 23:35:21,401 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 29 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,401 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 29 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,401 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 29 (epoch 26)
[ERROR] 2018-10-25 23:35:21,438 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 9099573702336136263
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,444 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34454
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,444 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34448
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:21,447 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/11 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,447 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/11 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,447 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 11 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,447 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 11 requested
[INFO] 2018-10-25 23:35:21,448 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 11
[INFO] 2018-10-25 23:35:21,452 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/48 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,453 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/48 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[WARN] 2018-10-25 23:35:21,463 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34446
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,465 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34442
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,467 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34456
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,469 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34422
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:21,470 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 6281621568770767913
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[WARN] 2018-10-25 23:35:21,472 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34440
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:21,476 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 15 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,476 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 15 (epoch 27)
[INFO] 2018-10-25 23:35:21,477 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 15 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,477 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 15 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,477 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 15 (epoch 27)
[WARN] 2018-10-25 23:35:21,482 org.apache.spark.network.server.TransportChannelHandler exceptionCaught - Exception in connection from /10.103.164.240:34462
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2018-10-25 23:35:21,482 org.apache.spark.network.server.TransportRequestHandler processRpcRequest - Error while invoking RpcHandler#receive() on RPC id 5163227861196820656
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId; local class incompatible: stream classdesc serialVersionUID = 6155820641931972169, local class serialVersionUID = -3720498261147521051
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:271)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:320)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:270)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:269)
	at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:611)
	at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:662)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:647)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[INFO] 2018-10-25 23:35:21,486 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/48 is now RUNNING
[INFO] 2018-10-25 23:35:21,487 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/23 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,487 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/23 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,488 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/49 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,488 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 23 requested
[INFO] 2018-10-25 23:35:21,488 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 23 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,488 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/49 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,488 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 23
[INFO] 2018-10-25 23:35:21,489 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/25 is now EXITED (Command exited with code 1)
[ERROR] 2018-10-25 23:35:21,853 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 7 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,853 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/25 removed: Command exited with code 1
[ERROR] 2018-10-25 23:35:21,854 org.apache.spark.scheduler.TaskSchedulerImpl logError - Lost executor 28 on 10.103.164.240: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,854 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/50 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,854 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 7 (epoch 28)
[INFO] 2018-10-25 23:35:21,854 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/50 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,854 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 7 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,855 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/12 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,855 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 7 successfully in removeExecutor
[WARN] 2018-10-25 23:35:21,855 org.apache.spark.scheduler.TaskSetManager logWarning - Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 28): ExecutorLostFailure (executor 28 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
[INFO] 2018-10-25 23:35:21,855 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 7 (epoch 28)
[INFO] 2018-10-25 23:35:21,855 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/12 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,856 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/51 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[ERROR] 2018-10-25 23:35:21,856 org.apache.spark.scheduler.TaskSetManager logError - Task 0 in stage 0.0 failed 4 times; aborting job
[INFO] 2018-10-25 23:35:21,856 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/51 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,857 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/16 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,857 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/16 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,857 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/52 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,857 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/52 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,858 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/27 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,858 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/27 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,858 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/53 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,858 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:35:21,859 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 25 requested
[INFO] 2018-10-25 23:35:21,859 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 25
[INFO] 2018-10-25 23:35:21,859 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 25 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,859 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/53 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,860 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 12 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,859 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 12 requested
[INFO] 2018-10-25 23:35:21,860 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 12
[INFO] 2018-10-25 23:35:21,860 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/14 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,860 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 16 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,860 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 16 requested
[INFO] 2018-10-25 23:35:21,861 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 16
[INFO] 2018-10-25 23:35:21,860 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/14 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,861 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 27 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,861 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 27 requested
[INFO] 2018-10-25 23:35:21,861 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 27
[INFO] 2018-10-25 23:35:21,861 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/54 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,862 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 14 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,861 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 14 requested
[INFO] 2018-10-25 23:35:21,862 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 14
[INFO] 2018-10-25 23:35:21,862 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/54 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,862 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/49 is now RUNNING
[INFO] 2018-10-25 23:35:21,863 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/50 is now RUNNING
[INFO] 2018-10-25 23:35:21,863 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/51 is now RUNNING
[INFO] 2018-10-25 23:35:21,863 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/52 is now RUNNING
[INFO] 2018-10-25 23:35:21,863 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/53 is now RUNNING
[INFO] 2018-10-25 23:35:21,863 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/54 is now RUNNING
[INFO] 2018-10-25 23:35:21,864 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/18 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,864 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/18 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,864 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Cancelling stage 0
[INFO] 2018-10-25 23:35:21,864 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 18 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,864 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/55 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,864 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 18 requested
[INFO] 2018-10-25 23:35:21,865 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 18
[INFO] 2018-10-25 23:35:21,865 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/55 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,865 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/55 is now RUNNING
[INFO] 2018-10-25 23:35:21,865 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/29 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,865 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/29 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,866 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 29 requested
[INFO] 2018-10-25 23:35:21,866 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 29
[INFO] 2018-10-25 23:35:21,866 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 29 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,866 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) failed in 2.287 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.103.164.240, executor 28): ExecutorLostFailure (executor 28 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: 
Driver stacktrace:
[INFO] 2018-10-25 23:35:21,866 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/56 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,867 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/56 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,867 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/56 is now RUNNING
[INFO] 2018-10-25 23:35:21,872 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/15 is now EXITED (Command exited with code 1)
[INFO] 2018-10-25 23:35:21,873 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Executor app-20181025233515-0006/15 removed: Command exited with code 1
[INFO] 2018-10-25 23:35:21,874 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor added: app-20181025233515-0006/57 on worker-20181025030317-10.103.164.240-7078 (10.103.164.240:7078) with 1 core(s)
[INFO] 2018-10-25 23:35:21,874 org.apache.spark.scheduler.DAGScheduler logInfo - Executor lost: 28 (epoch 29)
[INFO] 2018-10-25 23:35:21,874 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 15 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,874 org.apache.spark.storage.BlockManagerMaster logInfo - Removal of executor 15 requested
[INFO] 2018-10-25 23:35:21,875 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Trying to remove executor 28 from BlockManagerMaster.
[INFO] 2018-10-25 23:35:21,875 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Granted executor ID app-20181025233515-0006/57 on hostPort 10.103.164.240:7078 with 1 core(s), 1024.0 MB RAM
[INFO] 2018-10-25 23:35:21,874 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 failed: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 2.356062 s
[INFO] 2018-10-25 23:35:21,875 org.apache.spark.storage.BlockManagerMaster logInfo - Removed 28 successfully in removeExecutor
[INFO] 2018-10-25 23:35:21,875 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asked to remove non-existent executor 15
[INFO] 2018-10-25 23:35:21,876 org.apache.spark.scheduler.DAGScheduler logInfo - Shuffle files lost for executor: 28 (epoch 29)
[INFO] 2018-10-25 23:35:21,897 org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint logInfo - Executor updated: app-20181025233515-0006/57 is now RUNNING
[INFO] 2018-10-25 23:35:21,944 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:35:21,959 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:35:21,967 org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend logInfo - Shutting down all executors
[INFO] 2018-10-25 23:35:21,968 org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint logInfo - Asking each executor to shut down
[INFO] 2018-10-25 23:35:21,983 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:35:21,999 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:35:22,000 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:35:22,001 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:35:22,005 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:35:22,009 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:35:22,010 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:35:22,011 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4f6aade2-24b8-4d3f-b70c-b616deaf25f4/pyspark-287729bf-5158-454d-b5e5-c66ace7a0b55
[INFO] 2018-10-25 23:35:22,011 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-4f6aade2-24b8-4d3f-b70c-b616deaf25f4
[INFO] 2018-10-25 23:35:22,012 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-48ac2996-e61b-4df5-935a-292d7bef521d
[WARN] 2018-10-25 23:51:26,721 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:51:27,506 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:51:27,532 org.apache.spark.SparkContext logInfo - Submitted application: JB_WORK_INVOICE_DATAPREP
[INFO] 2018-10-25 23:51:27,736 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:51:27,736 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:51:27,737 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:51:27,737 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:51:27,738 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:51:27,957 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11718.
[INFO] 2018-10-25 23:51:27,980 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:51:27,999 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:51:28,001 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:51:28,002 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:51:28,011 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-80df4cf8-879e-4ca8-9d61-73458a63cf24
[INFO] 2018-10-25 23:51:28,028 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:51:28,041 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:51:28,208 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 23:51:28,214 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 23:51:28,265 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:51:28,357 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540536688356
[INFO] 2018-10-25 23:51:28,358 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-d97226d3-4339-4405-b015-03308ccf9cf1/userFiles-f355545e-e5ce-45b8-8e8f-4eb3c60b6c9c/etl_config.json
[INFO] 2018-10-25 23:51:28,371 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py at file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py with timestamp 1540536688371
[INFO] 2018-10-25 23:51:28,372 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py to /tmp/spark-d97226d3-4339-4405-b015-03308ccf9cf1/userFiles-f355545e-e5ce-45b8-8e8f-4eb3c60b6c9c/JB_WORK_INVOICE_DATAPREP.py
[INFO] 2018-10-25 23:51:28,377 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540536688377
[INFO] 2018-10-25 23:51:28,378 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-d97226d3-4339-4405-b015-03308ccf9cf1/userFiles-f355545e-e5ce-45b8-8e8f-4eb3c60b6c9c/packages.zip
[INFO] 2018-10-25 23:51:28,451 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 23:51:28,489 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30081.
[INFO] 2018-10-25 23:51:28,490 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:30081
[INFO] 2018-10-25 23:51:28,492 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:51:28,535 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 30081, None)
[INFO] 2018-10-25 23:51:28,542 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:30081 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 30081, None)
[INFO] 2018-10-25 23:51:28,546 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 30081, None)
[INFO] 2018-10-25 23:51:28,547 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 30081, None)
[INFO] 2018-10-25 23:51:28,878 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:51:28,879 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:51:29,457 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:51:32,412 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator logInfo - Code generated in 202.023659 ms
[INFO] 2018-10-25 23:51:32,562 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380
[INFO] 2018-10-25 23:51:32,582 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) with 1 output partitions
[INFO] 2018-10-25 23:51:32,583 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380)
[INFO] 2018-10-25 23:51:32,583 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:51:32,585 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:51:32,594 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380), which has no missing parents
[INFO] 2018-10-25 23:51:32,664 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 23:51:32,697 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 23:51:32,700 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_0_piece0 in memory on vmwebietl02-dev:30081 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:51:32,703 org.apache.spark.SparkContext logInfo - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:51:32,721 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:51:32,722 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 0.0 with 1 tasks
[INFO] 2018-10-25 23:51:32,780 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 23:51:32,793 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2018-10-25 23:51:32,798 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540536688356
[INFO] 2018-10-25 23:51:32,827 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/configs/etl_config.json has been previously copied to /tmp/spark-d97226d3-4339-4405-b015-03308ccf9cf1/userFiles-f355545e-e5ce-45b8-8e8f-4eb3c60b6c9c/etl_config.json
[INFO] 2018-10-25 23:51:32,833 org.apache.spark.executor.Executor logInfo - Fetching file:/home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py with timestamp 1540536688371
[INFO] 2018-10-25 23:51:32,834 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/jobs/SCH/JB_WORK_INVOICE_DATAPREP.py has been previously copied to /tmp/spark-d97226d3-4339-4405-b015-03308ccf9cf1/userFiles-f355545e-e5ce-45b8-8e8f-4eb3c60b6c9c/JB_WORK_INVOICE_DATAPREP.py
[INFO] 2018-10-25 23:51:32,840 org.apache.spark.executor.Executor logInfo - Fetching file:///home/spark/EBI_Project/packages.zip with timestamp 1540536688377
[INFO] 2018-10-25 23:51:32,840 org.apache.spark.util.Utils logInfo - /home/spark/EBI_Project/packages.zip has been previously copied to /tmp/spark-d97226d3-4339-4405-b015-03308ccf9cf1/userFiles-f355545e-e5ce-45b8-8e8f-4eb3c60b6c9c/packages.zip
[INFO] 2018-10-25 23:51:33,797 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 23:51:33,829 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 454, boot = 399, init = 55, finish = 0
[INFO] 2018-10-25 23:51:33,845 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 0.0 (TID 0). 1663 bytes result sent to driver
[INFO] 2018-10-25 23:51:33,857 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 0.0 (TID 0) in 1089 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 23:51:33,862 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:51:33,876 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 0 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380) finished in 1.262 s
[INFO] 2018-10-25 23:51:33,883 org.apache.spark.scheduler.DAGScheduler logInfo - Job 0 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:380, took 1.320406 s
[INFO] 2018-10-25 23:51:34,091 org.apache.spark.SparkContext logInfo - Starting job: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340
[INFO] 2018-10-25 23:51:34,094 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) with 1 output partitions
[INFO] 2018-10-25 23:51:34,094 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340)
[INFO] 2018-10-25 23:51:34,094 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List()
[INFO] 2018-10-25 23:51:34,095 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List()
[INFO] 2018-10-25 23:51:34,096 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340), which has no missing parents
[INFO] 2018-10-25 23:51:34,102 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1 stored as values in memory (estimated size 11.3 KB, free 13.2 GB)
[INFO] 2018-10-25 23:51:34,105 org.apache.spark.storage.memory.MemoryStore logInfo - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 13.2 GB)
[INFO] 2018-10-25 23:51:34,106 org.apache.spark.storage.BlockManagerInfo logInfo - Added broadcast_1_piece0 in memory on vmwebietl02-dev:30081 (size: 6.2 KB, free: 13.2 GB)
[INFO] 2018-10-25 23:51:34,108 org.apache.spark.SparkContext logInfo - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
[INFO] 2018-10-25 23:51:34,108 org.apache.spark.scheduler.DAGScheduler logInfo - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[9] at collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) (first 15 tasks are for partitions Vector(0))
[INFO] 2018-10-25 23:51:34,109 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Adding task set 1.0 with 1 tasks
[INFO] 2018-10-25 23:51:34,110 org.apache.spark.scheduler.TaskSetManager logInfo - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7677 bytes)
[INFO] 2018-10-25 23:51:34,111 org.apache.spark.executor.Executor logInfo - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2018-10-25 23:51:34,296 org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD logInfo - closed connection
[INFO] 2018-10-25 23:51:34,321 org.apache.spark.api.python.PythonRunner logInfo - Times: total = 41, boot = -440, init = 481, finish = 0
[INFO] 2018-10-25 23:51:34,324 org.apache.spark.executor.Executor logInfo - Finished task 0.0 in stage 1.0 (TID 1). 1668 bytes result sent to driver
[INFO] 2018-10-25 23:51:34,328 org.apache.spark.scheduler.TaskSetManager logInfo - Finished task 0.0 in stage 1.0 (TID 1) in 217 ms on localhost (executor driver) (1/1)
[INFO] 2018-10-25 23:51:34,328 org.apache.spark.scheduler.TaskSchedulerImpl logInfo - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2018-10-25 23:51:34,330 org.apache.spark.scheduler.DAGScheduler logInfo - ResultStage 1 (collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340) finished in 0.232 s
[INFO] 2018-10-25 23:51:34,332 org.apache.spark.scheduler.DAGScheduler logInfo - Job 1 finished: collect at /home/spark/EBI_Project/packages.zip/dependencies/EbiReadWrite.py:340, took 0.239873 s
[WARN] 2018-10-25 23:51:53,598 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:51:54,442 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:51:54,466 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS_TRUNCATE
[INFO] 2018-10-25 23:51:54,617 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:51:54,618 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:51:54,618 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:51:54,618 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:51:54,619 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:51:54,832 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 35696.
[INFO] 2018-10-25 23:51:54,855 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:51:54,874 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:51:54,876 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:51:54,877 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:51:54,885 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-6f2d76a6-c433-48b6-b3bc-913a704ffa67
[INFO] 2018-10-25 23:51:54,902 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:51:54,916 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:51:55,078 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 23:51:55,078 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-25 23:51:55,084 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-25 23:51:55,133 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 23:51:55,223 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540536715223
[INFO] 2018-10-25 23:51:55,225 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-76eccbb3-98e2-4f3d-9b32-1a4c901687f7/userFiles-1d15cc26-407c-43f4-a77c-ac69ef00458a/etl_config.json
[INFO] 2018-10-25 23:51:55,238 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py with timestamp 1540536715238
[INFO] 2018-10-25 23:51:55,238 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS_TRUNCATE.py to /tmp/spark-76eccbb3-98e2-4f3d-9b32-1a4c901687f7/userFiles-1d15cc26-407c-43f4-a77c-ac69ef00458a/JB_STG_BOOKINGS_TRUNCATE.py
[INFO] 2018-10-25 23:51:55,241 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540536715241
[INFO] 2018-10-25 23:51:55,242 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-76eccbb3-98e2-4f3d-9b32-1a4c901687f7/userFiles-1d15cc26-407c-43f4-a77c-ac69ef00458a/packages.zip
[INFO] 2018-10-25 23:51:55,309 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 23:51:55,340 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11670.
[INFO] 2018-10-25 23:51:55,341 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:11670
[INFO] 2018-10-25 23:51:55,343 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:51:55,384 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 11670, None)
[INFO] 2018-10-25 23:51:55,391 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:11670 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 11670, None)
[INFO] 2018-10-25 23:51:55,395 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 11670, None)
[INFO] 2018-10-25 23:51:55,397 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 11670, None)
[INFO] 2018-10-25 23:51:55,721 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:51:55,722 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:51:56,243 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:51:57,053 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:51:57,067 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 23:51:57,078 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:51:57,091 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:51:57,092 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:51:57,104 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:51:57,111 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:51:57,122 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:51:57,123 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:51:57,124 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-726c2d79-7c57-4513-a343-4ba7bc362bd9
[INFO] 2018-10-25 23:51:57,125 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-76eccbb3-98e2-4f3d-9b32-1a4c901687f7
[INFO] 2018-10-25 23:51:57,125 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-76eccbb3-98e2-4f3d-9b32-1a4c901687f7/pyspark-0cb4b9ec-ed9c-4a7a-a8ec-d699aea9ae9b
[WARN] 2018-10-25 23:52:17,161 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:52:17,968 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:52:17,995 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 23:52:18,123 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:52:18,123 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:52:18,123 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:52:18,124 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:52:18,125 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:52:18,362 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 15397.
[INFO] 2018-10-25 23:52:18,393 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:52:18,415 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:52:18,418 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:52:18,419 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:52:18,429 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-d2911333-fbb9-4866-93ba-e51f5ea41c83
[INFO] 2018-10-25 23:52:18,449 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:52:18,464 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:52:18,670 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 23:52:18,671 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-25 23:52:18,676 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-25 23:52:18,732 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 23:52:18,821 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540536738820
[INFO] 2018-10-25 23:52:18,822 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-71b5ed6c-4573-4d46-941b-a222cae64a0c/userFiles-b5e251ff-1603-46af-b5e8-0e1df921c8fe/etl_config.json
[INFO] 2018-10-25 23:52:18,836 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540536738836
[INFO] 2018-10-25 23:52:18,837 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-71b5ed6c-4573-4d46-941b-a222cae64a0c/userFiles-b5e251ff-1603-46af-b5e8-0e1df921c8fe/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 23:52:18,841 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540536738841
[INFO] 2018-10-25 23:52:18,841 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-71b5ed6c-4573-4d46-941b-a222cae64a0c/userFiles-b5e251ff-1603-46af-b5e8-0e1df921c8fe/packages.zip
[INFO] 2018-10-25 23:52:18,911 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 23:52:18,933 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18596.
[INFO] 2018-10-25 23:52:18,934 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:18596
[INFO] 2018-10-25 23:52:18,936 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:52:18,968 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 18596, None)
[INFO] 2018-10-25 23:52:18,973 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:18596 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 18596, None)
[INFO] 2018-10-25 23:52:18,978 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 18596, None)
[INFO] 2018-10-25 23:52:18,978 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 18596, None)
[INFO] 2018-10-25 23:52:19,268 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:52:19,269 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:52:19,759 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:52:20,179 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:52:20,191 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 23:52:20,203 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:52:20,212 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:52:20,213 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:52:20,224 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:52:20,229 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:52:20,238 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:52:20,238 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:52:20,239 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-71b5ed6c-4573-4d46-941b-a222cae64a0c/pyspark-ede58288-c72c-4b18-9a9c-0f8b8afff614
[INFO] 2018-10-25 23:52:20,239 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-a69629c7-186c-4e6b-833d-2d4fde90d8c0
[INFO] 2018-10-25 23:52:20,240 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-71b5ed6c-4573-4d46-941b-a222cae64a0c
[WARN] 2018-10-25 23:56:26,206 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:56:26,969 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:56:26,996 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 23:56:27,226 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:56:27,227 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:56:27,227 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:56:27,227 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:56:27,228 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:56:27,462 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 11943.
[INFO] 2018-10-25 23:56:27,489 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:56:27,508 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:56:27,511 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:56:27,512 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:56:27,520 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-0b4cc1e8-ec91-4867-a583-928294090bd6
[INFO] 2018-10-25 23:56:27,537 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:56:27,550 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:56:27,719 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN] 2018-10-25 23:56:27,720 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO] 2018-10-25 23:56:27,727 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4042.
[INFO] 2018-10-25 23:56:27,778 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 23:56:27,896 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540536987895
[INFO] 2018-10-25 23:56:27,898 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-f8443e1d-e580-4a4c-bd34-aaef92386541/userFiles-4ac0db41-19a3-4b75-96f4-de135d8e12c5/etl_config.json
[INFO] 2018-10-25 23:56:27,913 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540536987913
[INFO] 2018-10-25 23:56:27,914 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-f8443e1d-e580-4a4c-bd34-aaef92386541/userFiles-4ac0db41-19a3-4b75-96f4-de135d8e12c5/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 23:56:27,919 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540536987918
[INFO] 2018-10-25 23:56:27,919 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-f8443e1d-e580-4a4c-bd34-aaef92386541/userFiles-4ac0db41-19a3-4b75-96f4-de135d8e12c5/packages.zip
[INFO] 2018-10-25 23:56:27,979 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 23:56:28,004 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34768.
[INFO] 2018-10-25 23:56:28,005 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:34768
[INFO] 2018-10-25 23:56:28,006 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:56:28,036 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 34768, None)
[INFO] 2018-10-25 23:56:28,042 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:34768 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 34768, None)
[INFO] 2018-10-25 23:56:28,046 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 34768, None)
[INFO] 2018-10-25 23:56:28,047 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 34768, None)
[INFO] 2018-10-25 23:56:28,304 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:56:28,304 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:56:28,851 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:56:29,296 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:56:29,306 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4042
[INFO] 2018-10-25 23:56:29,318 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:56:29,329 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:56:29,330 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:56:29,342 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:56:29,348 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:56:29,357 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:56:29,357 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:56:29,358 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f8443e1d-e580-4a4c-bd34-aaef92386541/pyspark-a914ed3c-08a1-4bb2-9c5a-fb57de474f35
[INFO] 2018-10-25 23:56:29,359 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-f8443e1d-e580-4a4c-bd34-aaef92386541
[INFO] 2018-10-25 23:56:29,359 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-6ede6974-1175-4635-9fde-9fcbcc87a472
[INFO] 2018-10-25 23:56:54,696 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:56:54,707 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:56:54,723 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:56:54,734 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:56:54,734 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:56:54,735 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:56:54,738 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:56:54,744 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:56:54,744 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:56:54,745 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d97226d3-4339-4405-b015-03308ccf9cf1/pyspark-d9148862-0e33-404d-9732-039d0ef87edb
[INFO] 2018-10-25 23:56:54,746 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d35ebe68-c1b0-4fa8-837f-033053c9e629
[INFO] 2018-10-25 23:56:54,748 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-d97226d3-4339-4405-b015-03308ccf9cf1
[WARN] 2018-10-25 23:57:51,482 org.apache.hadoop.util.NativeCodeLoader <clinit> - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2018-10-25 23:57:52,294 org.apache.spark.SparkContext logInfo - Running Spark version 2.3.1
[INFO] 2018-10-25 23:57:52,320 org.apache.spark.SparkContext logInfo - Submitted application: JB_STG_BOOKINGS
[INFO] 2018-10-25 23:57:52,571 org.apache.spark.SecurityManager logInfo - Changing view acls to: pyspark
[INFO] 2018-10-25 23:57:52,571 org.apache.spark.SecurityManager logInfo - Changing modify acls to: pyspark
[INFO] 2018-10-25 23:57:52,571 org.apache.spark.SecurityManager logInfo - Changing view acls groups to: 
[INFO] 2018-10-25 23:57:52,572 org.apache.spark.SecurityManager logInfo - Changing modify acls groups to: 
[INFO] 2018-10-25 23:57:52,572 org.apache.spark.SecurityManager logInfo - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(pyspark); groups with view permissions: Set(); users  with modify permissions: Set(pyspark); groups with modify permissions: Set()
[INFO] 2018-10-25 23:57:52,798 org.apache.spark.util.Utils logInfo - Successfully started service 'sparkDriver' on port 17825.
[INFO] 2018-10-25 23:57:52,825 org.apache.spark.SparkEnv logInfo - Registering MapOutputTracker
[INFO] 2018-10-25 23:57:52,843 org.apache.spark.SparkEnv logInfo - Registering BlockManagerMaster
[INFO] 2018-10-25 23:57:52,846 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2018-10-25 23:57:52,846 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - BlockManagerMasterEndpoint up
[INFO] 2018-10-25 23:57:52,855 org.apache.spark.storage.DiskBlockManager logInfo - Created local directory at /tmp/blockmgr-e831a548-d887-4f8d-bbdb-9b2e5c93860d
[INFO] 2018-10-25 23:57:52,872 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore started with capacity 13.2 GB
[INFO] 2018-10-25 23:57:52,885 org.apache.spark.SparkEnv logInfo - Registering OutputCommitCoordinator
[WARN] 2018-10-25 23:57:53,050 org.apache.spark.util.Utils logWarning - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO] 2018-10-25 23:57:53,057 org.apache.spark.util.Utils logInfo - Successfully started service 'SparkUI' on port 4041.
[INFO] 2018-10-25 23:57:53,106 org.apache.spark.ui.SparkUI logInfo - Bound SparkUI to 0.0.0.0, and started at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:57:53,206 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/configs/etl_config.json at file:///home/spark/EBI_Project/configs/etl_config.json with timestamp 1540537073205
[INFO] 2018-10-25 23:57:53,208 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/configs/etl_config.json to /tmp/spark-937e3fdf-1327-4ec4-bfce-94dfb0d3c31b/userFiles-51e13018-6635-4f8a-8cf4-5634508c174f/etl_config.json
[INFO] 2018-10-25 23:57:53,219 org.apache.spark.SparkContext logInfo - Added file file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py at file:/home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py with timestamp 1540537073219
[INFO] 2018-10-25 23:57:53,220 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/jobs/SCH/JB_STG_BOOKINGS.py to /tmp/spark-937e3fdf-1327-4ec4-bfce-94dfb0d3c31b/userFiles-51e13018-6635-4f8a-8cf4-5634508c174f/JB_STG_BOOKINGS.py
[INFO] 2018-10-25 23:57:53,224 org.apache.spark.SparkContext logInfo - Added file file:///home/spark/EBI_Project/packages.zip at file:///home/spark/EBI_Project/packages.zip with timestamp 1540537073224
[INFO] 2018-10-25 23:57:53,224 org.apache.spark.util.Utils logInfo - Copying /home/spark/EBI_Project/packages.zip to /tmp/spark-937e3fdf-1327-4ec4-bfce-94dfb0d3c31b/userFiles-51e13018-6635-4f8a-8cf4-5634508c174f/packages.zip
[INFO] 2018-10-25 23:57:53,294 org.apache.spark.executor.Executor logInfo - Starting executor ID driver on host localhost
[INFO] 2018-10-25 23:57:53,317 org.apache.spark.util.Utils logInfo - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10405.
[INFO] 2018-10-25 23:57:53,318 org.apache.spark.network.netty.NettyBlockTransferService logInfo - Server created on vmwebietl02-dev:10405
[INFO] 2018-10-25 23:57:53,320 org.apache.spark.storage.BlockManager logInfo - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2018-10-25 23:57:53,352 org.apache.spark.storage.BlockManagerMaster logInfo - Registering BlockManager BlockManagerId(driver, vmwebietl02-dev, 10405, None)
[INFO] 2018-10-25 23:57:53,358 org.apache.spark.storage.BlockManagerMasterEndpoint logInfo - Registering block manager vmwebietl02-dev:10405 with 13.2 GB RAM, BlockManagerId(driver, vmwebietl02-dev, 10405, None)
[INFO] 2018-10-25 23:57:53,362 org.apache.spark.storage.BlockManagerMaster logInfo - Registered BlockManager BlockManagerId(driver, vmwebietl02-dev, 10405, None)
[INFO] 2018-10-25 23:57:53,363 org.apache.spark.storage.BlockManager logInfo - Initialized BlockManager: BlockManagerId(driver, vmwebietl02-dev, 10405, None)
[INFO] 2018-10-25 23:57:53,620 org.apache.spark.sql.internal.SharedState logInfo - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/EBI_Project/spark-warehouse/').
[INFO] 2018-10-25 23:57:53,620 org.apache.spark.sql.internal.SharedState logInfo - Warehouse path is 'file:/home/spark/EBI_Project/spark-warehouse/'.
[INFO] 2018-10-25 23:57:54,136 org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef logInfo - Registered StateStoreCoordinator endpoint
[INFO] 2018-10-25 23:59:15,455 org.apache.spark.SparkContext logInfo - Invoking stop() from shutdown hook
[INFO] 2018-10-25 23:59:15,469 org.apache.spark.ui.SparkUI logInfo - Stopped Spark web UI at http://vmwebietl02-dev:4041
[INFO] 2018-10-25 23:59:15,482 org.apache.spark.MapOutputTrackerMasterEndpoint logInfo - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2018-10-25 23:59:15,494 org.apache.spark.storage.memory.MemoryStore logInfo - MemoryStore cleared
[INFO] 2018-10-25 23:59:15,494 org.apache.spark.storage.BlockManager logInfo - BlockManager stopped
[INFO] 2018-10-25 23:59:15,496 org.apache.spark.storage.BlockManagerMaster logInfo - BlockManagerMaster stopped
[INFO] 2018-10-25 23:59:15,501 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint logInfo - OutputCommitCoordinator stopped!
[INFO] 2018-10-25 23:59:15,509 org.apache.spark.SparkContext logInfo - Successfully stopped SparkContext
[INFO] 2018-10-25 23:59:15,510 org.apache.spark.util.ShutdownHookManager logInfo - Shutdown hook called
[INFO] 2018-10-25 23:59:15,511 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-937e3fdf-1327-4ec4-bfce-94dfb0d3c31b/pyspark-ea37b8a4-d3c1-4c2f-ae3d-793eae1d5de1
[INFO] 2018-10-25 23:59:15,511 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-937e3fdf-1327-4ec4-bfce-94dfb0d3c31b
[INFO] 2018-10-25 23:59:15,512 org.apache.spark.util.ShutdownHookManager logInfo - Deleting directory /tmp/spark-1db4f38c-f87c-4d31-83ad-416a31a30e33
